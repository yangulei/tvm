Running pass: {} The meta data of the pass: pass name: sequentialopt_level: 2required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %8 = %7.0;
  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %12 = %11.0;
  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %18 = %17.0;
  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %27 = %26.0;
  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %31 = %30.0;
  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %37 = %36.0;
  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %41 = %40.0;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %46 = %45.0;
  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %50 = %49.0;
  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %60 = %59.0;
  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %65 = %64.0;
  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %69 = %68.0;
  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %79 = %78.0;
  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %84 = %83.0;
  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: MergeCompositeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %8 = %7.0;
  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %12 = %11.0;
  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %18 = %17.0;
  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %27 = %26.0;
  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %31 = %30.0;
  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %37 = %36.0;
  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %41 = %40.0;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %46 = %45.0;
  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %50 = %49.0;
  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %60 = %59.0;
  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %65 = %64.0;
  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %69 = %68.0;
  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %79 = %78.0;
  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %84 = %83.0;
  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %8 = %7.0;
  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %12 = %11.0;
  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %18 = %17.0;
  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %27 = %26.0;
  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %31 = %30.0;
  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %37 = %36.0;
  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %41 = %40.0;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %46 = %45.0;
  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %50 = %49.0;
  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %60 = %59.0;
  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %65 = %64.0;
  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %69 = %68.0;
  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %79 = %78.0;
  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %84 = %83.0;
  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %8 = %7.0;
  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %12 = %11.0;
  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %18 = %17.0;
  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %27 = %26.0;
  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %31 = %30.0;
  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %37 = %36.0;
  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %41 = %40.0;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %46 = %45.0;
  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %50 = %49.0;
  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %60 = %59.0;
  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %65 = %64.0;
  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %69 = %68.0;
  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %79 = %78.0;
  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %84 = %83.0;
  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %8 = %7.0;
  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %12 = %11.0;
  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %18 = %17.0;
  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %27 = %26.0;
  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %31 = %30.0;
  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %37 = %36.0;
  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %41 = %40.0;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %46 = %45.0;
  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %50 = %49.0;
  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %60 = %59.0;
  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %65 = %64.0;
  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %69 = %68.0;
  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %79 = %78.0;
  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %84 = %83.0;
  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: AnnotateTargetopt_level: 2required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %8 = %7.0;
  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %12 = %11.0;
  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %18 = %17.0;
  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %27 = %26.0;
  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %31 = %30.0;
  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %37 = %36.0;
  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %41 = %40.0;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %46 = %45.0;
  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %50 = %49.0;
  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %60 = %59.0;
  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %65 = %64.0;
  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %69 = %68.0;
  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %79 = %78.0;
  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %84 = %83.0;
  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %8 = %7.0;
  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %12 = %11.0;
  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %18 = %17.0;
  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %27 = %26.0;
  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %31 = %30.0;
  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %37 = %36.0;
  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %41 = %40.0;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %46 = %45.0;
  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %50 = %49.0;
  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %60 = %59.0;
  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %65 = %64.0;
  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %69 = %68.0;
  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %79 = %78.0;
  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %84 = %83.0;
  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: AnnotateTargetFuncopt_level: 0required passes: [
InferType, ]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = nn.batch_norm(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %conv0_weight, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  %5 = nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %6 = nn.max_pool2d(%5, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.batch_norm(%6, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %8 = %7.0;
  %9 = nn.relu(%8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.conv2d(%9, %stage1_unit1_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %11 = nn.batch_norm(%10, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %12 = %11.0;
  %13 = nn.relu(%12) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.conv2d(%13, %stage1_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.conv2d(%9, %stage1_unit1_sc_weight, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %16 = add(%14, %15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.batch_norm(%16, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %18 = %17.0;
  %19 = nn.relu(%18) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = nn.conv2d(%19, %stage1_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = nn.batch_norm(%20, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = nn.relu(%22) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.conv2d(%23, %stage1_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %25 = add(%24, %16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = nn.batch_norm(%25, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %27 = %26.0;
  %28 = nn.relu(%27) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = nn.conv2d(%28, %stage2_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %30 = nn.batch_norm(%29, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %31 = %30.0;
  %32 = nn.relu(%31) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.conv2d(%32, %stage2_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.conv2d(%28, %stage2_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %35 = add(%33, %34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.batch_norm(%35, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %37 = %36.0;
  %38 = nn.relu(%37) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %39 = nn.conv2d(%38, %stage2_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %40 = nn.batch_norm(%39, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %41 = %40.0;
  %42 = nn.relu(%41) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.conv2d(%42, %stage2_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %44 = add(%43, %35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %45 = nn.batch_norm(%44, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %46 = %45.0;
  %47 = nn.relu(%46) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %48 = nn.conv2d(%47, %stage3_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %49 = nn.batch_norm(%48, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %50 = %49.0;
  %51 = nn.relu(%50) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.conv2d(%51, %stage3_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.conv2d(%47, %stage3_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %54 = add(%52, %53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.batch_norm(%54, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %58 = nn.conv2d(%57, %stage3_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %59 = nn.batch_norm(%58, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %60 = %59.0;
  %61 = nn.relu(%60) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.conv2d(%61, %stage3_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %63 = add(%62, %54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %64 = nn.batch_norm(%63, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %65 = %64.0;
  %66 = nn.relu(%65) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %67 = nn.conv2d(%66, %stage4_unit1_conv1_weight, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %68 = nn.batch_norm(%67, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %69 = %68.0;
  %70 = nn.relu(%69) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.conv2d(%70, %stage4_unit1_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.conv2d(%66, %stage4_unit1_sc_weight, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %73 = add(%71, %72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.batch_norm(%73, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %77 = nn.conv2d(%76, %stage4_unit2_conv1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %78 = nn.batch_norm(%77, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %79 = %78.0;
  %80 = nn.relu(%79) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.conv2d(%80, %stage4_unit2_conv2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %82 = add(%81, %73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %83 = nn.batch_norm(%82, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %84 = %83.0;
  %85 = nn.relu(%84) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = nn.dense(%87, %fc1_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = annotation.compiler_end(%5, meta[relay.attrs.CompilerAttrs][5]) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %7 = annotation.compiler_begin(%6, meta[relay.attrs.CompilerAttrs][6]) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %8 = %7.0;
  %9 = annotation.compiler_end(%8, meta[relay.attrs.CompilerAttrs][7]);
  %10 = annotation.compiler_begin(%9, meta[relay.attrs.CompilerAttrs][8]);
  %11 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %12 = nn.conv2d(%10, %11, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %13 = annotation.compiler_end(%12, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %14 = annotation.compiler_begin(%13, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %15 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(64), float32] */;
  %16 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(64), float32] */;
  %17 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %18 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %19 = nn.batch_norm(%14, %15, %16, %17, %18, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = annotation.compiler_end(%19, meta[relay.attrs.CompilerAttrs][16]) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %21 = annotation.compiler_begin(%20, meta[relay.attrs.CompilerAttrs][17]) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = annotation.compiler_end(%22, meta[relay.attrs.CompilerAttrs][18]);
  %24 = annotation.compiler_begin(%23, meta[relay.attrs.CompilerAttrs][19]);
  %25 = nn.relu(%24) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %26 = annotation.compiler_end(%25, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %27 = annotation.compiler_begin(%26, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %28 = nn.max_pool2d(%27, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = annotation.compiler_end(%28, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %30 = annotation.compiler_begin(%29, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %34 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %35 = nn.batch_norm(%30, %31, %32, %33, %34, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %36 = annotation.compiler_end(%35, meta[relay.attrs.CompilerAttrs][28]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %37 = annotation.compiler_begin(%36, meta[relay.attrs.CompilerAttrs][29]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %38 = %37.0;
  %39 = annotation.compiler_end(%38, meta[relay.attrs.CompilerAttrs][30]);
  %40 = annotation.compiler_begin(%39, meta[relay.attrs.CompilerAttrs][31]);
  %41 = nn.relu(%40) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %42 = annotation.compiler_end(%41, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %43 = annotation.compiler_begin(%42, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %45 = nn.conv2d(%43, %44, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %46 = annotation.compiler_end(%45, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %47 = annotation.compiler_begin(%46, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %48 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %50 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(64), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(64), float32] */;
  %52 = nn.batch_norm(%47, %48, %49, %50, %51, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %53 = annotation.compiler_end(%52, meta[relay.attrs.CompilerAttrs][41]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %54 = annotation.compiler_begin(%53, meta[relay.attrs.CompilerAttrs][42]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %55 = %54.0;
  %56 = annotation.compiler_end(%55, meta[relay.attrs.CompilerAttrs][43]);
  %57 = annotation.compiler_begin(%56, meta[relay.attrs.CompilerAttrs][44]);
  %58 = nn.relu(%57) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %59 = annotation.compiler_end(%58, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %60 = annotation.compiler_begin(%59, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %61 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %62 = nn.conv2d(%60, %61, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %63 = annotation.compiler_end(%62, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %64 = annotation.compiler_end(%41, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %65 = annotation.compiler_begin(%64, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %66 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %67 = nn.conv2d(%65, %66, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %68 = annotation.compiler_end(%67, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %69 = annotation.compiler_begin(%63, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %70 = annotation.compiler_begin(%68, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %71 = add(%69, %70) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %72 = annotation.compiler_end(%71, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %73 = annotation.compiler_begin(%72, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %74 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(64), float32] */;
  %75 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(64), float32] */;
  %76 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(64), float32] */;
  %77 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(64), float32] */;
  %78 = nn.batch_norm(%73, %74, %75, %76, %77, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %79 = annotation.compiler_end(%78, meta[relay.attrs.CompilerAttrs][61]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %80 = annotation.compiler_begin(%79, meta[relay.attrs.CompilerAttrs][62]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %81 = %80.0;
  %82 = annotation.compiler_end(%81, meta[relay.attrs.CompilerAttrs][63]);
  %83 = annotation.compiler_begin(%82, meta[relay.attrs.CompilerAttrs][64]);
  %84 = nn.relu(%83) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %85 = annotation.compiler_end(%84, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %86 = annotation.compiler_begin(%85, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %87 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %89 = annotation.compiler_end(%88, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %90 = annotation.compiler_begin(%89, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %91 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(64), float32] */;
  %92 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(64), float32] */;
  %93 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(64), float32] */;
  %94 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(64), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %96 = annotation.compiler_end(%95, meta[relay.attrs.CompilerAttrs][74]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %97 = annotation.compiler_begin(%96, meta[relay.attrs.CompilerAttrs][75]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %98 = %97.0;
  %99 = annotation.compiler_end(%98, meta[relay.attrs.CompilerAttrs][76]);
  %100 = annotation.compiler_begin(%99, meta[relay.attrs.CompilerAttrs][77]);
  %101 = nn.relu(%100) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %102 = annotation.compiler_end(%101, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %103 = annotation.compiler_begin(%102, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %104 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %105 = nn.conv2d(%103, %104, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %106 = annotation.compiler_end(%105, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %107 = annotation.compiler_end(%71, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %108 = annotation.compiler_begin(%106, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %109 = annotation.compiler_begin(%107, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %110 = add(%108, %109) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %111 = annotation.compiler_end(%110, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %112 = annotation.compiler_begin(%111, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %113 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(64), float32] */;
  %114 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(64), float32] */;
  %115 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(64), float32] */;
  %116 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(64), float32] */;
  %117 = nn.batch_norm(%112, %113, %114, %115, %116, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %118 = annotation.compiler_end(%117, meta[relay.attrs.CompilerAttrs][91]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %119 = annotation.compiler_begin(%118, meta[relay.attrs.CompilerAttrs][92]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %120 = %119.0;
  %121 = annotation.compiler_end(%120, meta[relay.attrs.CompilerAttrs][93]);
  %122 = annotation.compiler_begin(%121, meta[relay.attrs.CompilerAttrs][94]);
  %123 = nn.relu(%122) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %124 = annotation.compiler_end(%123, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %125 = annotation.compiler_begin(%124, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %126 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %127 = nn.conv2d(%125, %126, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %128 = annotation.compiler_end(%127, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %129 = annotation.compiler_begin(%128, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %130 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(128), float32] */;
  %131 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(128), float32] */;
  %132 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(128), float32] */;
  %133 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(128), float32] */;
  %134 = nn.batch_norm(%129, %130, %131, %132, %133, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %135 = annotation.compiler_end(%134, meta[relay.attrs.CompilerAttrs][104]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %136 = annotation.compiler_begin(%135, meta[relay.attrs.CompilerAttrs][105]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %137 = %136.0;
  %138 = annotation.compiler_end(%137, meta[relay.attrs.CompilerAttrs][106]);
  %139 = annotation.compiler_begin(%138, meta[relay.attrs.CompilerAttrs][107]);
  %140 = nn.relu(%139) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %141 = annotation.compiler_end(%140, meta[relay.attrs.CompilerAttrs][108]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %142 = annotation.compiler_begin(%141, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %143 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][110]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %144 = nn.conv2d(%142, %143, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %145 = annotation.compiler_end(%144, meta[relay.attrs.CompilerAttrs][111]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %146 = annotation.compiler_end(%123, meta[relay.attrs.CompilerAttrs][113]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %147 = annotation.compiler_begin(%146, meta[relay.attrs.CompilerAttrs][114]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %148 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][115]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %150 = annotation.compiler_end(%149, meta[relay.attrs.CompilerAttrs][116]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %151 = annotation.compiler_begin(%145, meta[relay.attrs.CompilerAttrs][112]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %152 = annotation.compiler_begin(%150, meta[relay.attrs.CompilerAttrs][117]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %153 = add(%151, %152) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %154 = annotation.compiler_end(%153, meta[relay.attrs.CompilerAttrs][118]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %155 = annotation.compiler_begin(%154, meta[relay.attrs.CompilerAttrs][119]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %156 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][120]) /* ty=Tensor[(128), float32] */;
  %157 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][121]) /* ty=Tensor[(128), float32] */;
  %158 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][122]) /* ty=Tensor[(128), float32] */;
  %159 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][123]) /* ty=Tensor[(128), float32] */;
  %160 = nn.batch_norm(%155, %156, %157, %158, %159, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %161 = annotation.compiler_end(%160, meta[relay.attrs.CompilerAttrs][124]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %162 = annotation.compiler_begin(%161, meta[relay.attrs.CompilerAttrs][125]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %163 = %162.0;
  %164 = annotation.compiler_end(%163, meta[relay.attrs.CompilerAttrs][126]);
  %165 = annotation.compiler_begin(%164, meta[relay.attrs.CompilerAttrs][127]);
  %166 = nn.relu(%165) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %167 = annotation.compiler_end(%166, meta[relay.attrs.CompilerAttrs][128]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %168 = annotation.compiler_begin(%167, meta[relay.attrs.CompilerAttrs][129]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %169 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][130]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %171 = annotation.compiler_end(%170, meta[relay.attrs.CompilerAttrs][131]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %172 = annotation.compiler_begin(%171, meta[relay.attrs.CompilerAttrs][132]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %173 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][133]) /* ty=Tensor[(128), float32] */;
  %174 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][134]) /* ty=Tensor[(128), float32] */;
  %175 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][135]) /* ty=Tensor[(128), float32] */;
  %176 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][136]) /* ty=Tensor[(128), float32] */;
  %177 = nn.batch_norm(%172, %173, %174, %175, %176, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %178 = annotation.compiler_end(%177, meta[relay.attrs.CompilerAttrs][137]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %179 = annotation.compiler_begin(%178, meta[relay.attrs.CompilerAttrs][138]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %180 = %179.0;
  %181 = annotation.compiler_end(%180, meta[relay.attrs.CompilerAttrs][139]);
  %182 = annotation.compiler_begin(%181, meta[relay.attrs.CompilerAttrs][140]);
  %183 = nn.relu(%182) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %184 = annotation.compiler_end(%183, meta[relay.attrs.CompilerAttrs][141]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %185 = annotation.compiler_begin(%184, meta[relay.attrs.CompilerAttrs][142]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %186 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][143]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %187 = nn.conv2d(%185, %186, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][144]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %189 = annotation.compiler_end(%153, meta[relay.attrs.CompilerAttrs][146]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %190 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][145]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %191 = annotation.compiler_begin(%189, meta[relay.attrs.CompilerAttrs][147]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %192 = add(%190, %191) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %193 = annotation.compiler_end(%192, meta[relay.attrs.CompilerAttrs][148]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %194 = annotation.compiler_begin(%193, meta[relay.attrs.CompilerAttrs][149]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %195 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][150]) /* ty=Tensor[(128), float32] */;
  %196 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][151]) /* ty=Tensor[(128), float32] */;
  %197 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][152]) /* ty=Tensor[(128), float32] */;
  %198 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][153]) /* ty=Tensor[(128), float32] */;
  %199 = nn.batch_norm(%194, %195, %196, %197, %198, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %200 = annotation.compiler_end(%199, meta[relay.attrs.CompilerAttrs][154]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %201 = annotation.compiler_begin(%200, meta[relay.attrs.CompilerAttrs][155]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %202 = %201.0;
  %203 = annotation.compiler_end(%202, meta[relay.attrs.CompilerAttrs][156]);
  %204 = annotation.compiler_begin(%203, meta[relay.attrs.CompilerAttrs][157]);
  %205 = nn.relu(%204) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %206 = annotation.compiler_end(%205, meta[relay.attrs.CompilerAttrs][158]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %207 = annotation.compiler_begin(%206, meta[relay.attrs.CompilerAttrs][159]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %208 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][160]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %209 = nn.conv2d(%207, %208, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %210 = annotation.compiler_end(%209, meta[relay.attrs.CompilerAttrs][161]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %211 = annotation.compiler_begin(%210, meta[relay.attrs.CompilerAttrs][162]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %212 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][163]) /* ty=Tensor[(256), float32] */;
  %213 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][164]) /* ty=Tensor[(256), float32] */;
  %214 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][165]) /* ty=Tensor[(256), float32] */;
  %215 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][166]) /* ty=Tensor[(256), float32] */;
  %216 = nn.batch_norm(%211, %212, %213, %214, %215, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %217 = annotation.compiler_end(%216, meta[relay.attrs.CompilerAttrs][167]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %218 = annotation.compiler_begin(%217, meta[relay.attrs.CompilerAttrs][168]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %219 = %218.0;
  %220 = annotation.compiler_end(%219, meta[relay.attrs.CompilerAttrs][169]);
  %221 = annotation.compiler_begin(%220, meta[relay.attrs.CompilerAttrs][170]);
  %222 = nn.relu(%221) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %223 = annotation.compiler_end(%222, meta[relay.attrs.CompilerAttrs][171]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %224 = annotation.compiler_begin(%223, meta[relay.attrs.CompilerAttrs][172]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %225 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][173]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %226 = nn.conv2d(%224, %225, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %227 = annotation.compiler_end(%226, meta[relay.attrs.CompilerAttrs][174]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %228 = annotation.compiler_end(%205, meta[relay.attrs.CompilerAttrs][176]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %229 = annotation.compiler_begin(%228, meta[relay.attrs.CompilerAttrs][177]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %230 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][178]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %231 = nn.conv2d(%229, %230, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %232 = annotation.compiler_end(%231, meta[relay.attrs.CompilerAttrs][179]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %233 = annotation.compiler_begin(%227, meta[relay.attrs.CompilerAttrs][175]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %234 = annotation.compiler_begin(%232, meta[relay.attrs.CompilerAttrs][180]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %235 = add(%233, %234) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %236 = annotation.compiler_end(%235, meta[relay.attrs.CompilerAttrs][181]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %237 = annotation.compiler_begin(%236, meta[relay.attrs.CompilerAttrs][182]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %238 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][183]) /* ty=Tensor[(256), float32] */;
  %239 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][184]) /* ty=Tensor[(256), float32] */;
  %240 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][185]) /* ty=Tensor[(256), float32] */;
  %241 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][186]) /* ty=Tensor[(256), float32] */;
  %242 = nn.batch_norm(%237, %238, %239, %240, %241, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %243 = annotation.compiler_end(%242, meta[relay.attrs.CompilerAttrs][187]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %244 = annotation.compiler_begin(%243, meta[relay.attrs.CompilerAttrs][188]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %245 = %244.0;
  %246 = annotation.compiler_end(%245, meta[relay.attrs.CompilerAttrs][189]);
  %247 = annotation.compiler_begin(%246, meta[relay.attrs.CompilerAttrs][190]);
  %248 = nn.relu(%247) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %249 = annotation.compiler_end(%248, meta[relay.attrs.CompilerAttrs][191]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %250 = annotation.compiler_begin(%249, meta[relay.attrs.CompilerAttrs][192]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %251 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][193]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %252 = nn.conv2d(%250, %251, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %253 = annotation.compiler_end(%252, meta[relay.attrs.CompilerAttrs][194]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %254 = annotation.compiler_begin(%253, meta[relay.attrs.CompilerAttrs][195]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %255 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][196]) /* ty=Tensor[(256), float32] */;
  %256 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][197]) /* ty=Tensor[(256), float32] */;
  %257 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][198]) /* ty=Tensor[(256), float32] */;
  %258 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][199]) /* ty=Tensor[(256), float32] */;
  %259 = nn.batch_norm(%254, %255, %256, %257, %258, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %260 = annotation.compiler_end(%259, meta[relay.attrs.CompilerAttrs][200]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %261 = annotation.compiler_begin(%260, meta[relay.attrs.CompilerAttrs][201]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %262 = %261.0;
  %263 = annotation.compiler_end(%262, meta[relay.attrs.CompilerAttrs][202]);
  %264 = annotation.compiler_begin(%263, meta[relay.attrs.CompilerAttrs][203]);
  %265 = nn.relu(%264) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %266 = annotation.compiler_end(%265, meta[relay.attrs.CompilerAttrs][204]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %267 = annotation.compiler_begin(%266, meta[relay.attrs.CompilerAttrs][205]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %268 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][206]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %269 = nn.conv2d(%267, %268, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %270 = annotation.compiler_end(%269, meta[relay.attrs.CompilerAttrs][207]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %271 = annotation.compiler_end(%235, meta[relay.attrs.CompilerAttrs][209]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %272 = annotation.compiler_begin(%270, meta[relay.attrs.CompilerAttrs][208]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %273 = annotation.compiler_begin(%271, meta[relay.attrs.CompilerAttrs][210]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %274 = add(%272, %273) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %275 = annotation.compiler_end(%274, meta[relay.attrs.CompilerAttrs][211]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %276 = annotation.compiler_begin(%275, meta[relay.attrs.CompilerAttrs][212]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %277 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][213]) /* ty=Tensor[(256), float32] */;
  %278 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][214]) /* ty=Tensor[(256), float32] */;
  %279 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][215]) /* ty=Tensor[(256), float32] */;
  %280 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][216]) /* ty=Tensor[(256), float32] */;
  %281 = nn.batch_norm(%276, %277, %278, %279, %280, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %282 = annotation.compiler_end(%281, meta[relay.attrs.CompilerAttrs][217]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %283 = annotation.compiler_begin(%282, meta[relay.attrs.CompilerAttrs][218]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %284 = %283.0;
  %285 = annotation.compiler_end(%284, meta[relay.attrs.CompilerAttrs][219]);
  %286 = annotation.compiler_begin(%285, meta[relay.attrs.CompilerAttrs][220]);
  %287 = nn.relu(%286) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %288 = annotation.compiler_end(%287, meta[relay.attrs.CompilerAttrs][221]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %289 = annotation.compiler_begin(%288, meta[relay.attrs.CompilerAttrs][222]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %290 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][223]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %291 = nn.conv2d(%289, %290, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %292 = annotation.compiler_end(%291, meta[relay.attrs.CompilerAttrs][224]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %293 = annotation.compiler_begin(%292, meta[relay.attrs.CompilerAttrs][225]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %294 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][226]) /* ty=Tensor[(512), float32] */;
  %295 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][227]) /* ty=Tensor[(512), float32] */;
  %296 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][228]) /* ty=Tensor[(512), float32] */;
  %297 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][229]) /* ty=Tensor[(512), float32] */;
  %298 = nn.batch_norm(%293, %294, %295, %296, %297, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %299 = annotation.compiler_end(%298, meta[relay.attrs.CompilerAttrs][230]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %300 = annotation.compiler_begin(%299, meta[relay.attrs.CompilerAttrs][231]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %301 = %300.0;
  %302 = annotation.compiler_end(%301, meta[relay.attrs.CompilerAttrs][232]);
  %303 = annotation.compiler_begin(%302, meta[relay.attrs.CompilerAttrs][233]);
  %304 = nn.relu(%303) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %305 = annotation.compiler_end(%304, meta[relay.attrs.CompilerAttrs][234]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %306 = annotation.compiler_begin(%305, meta[relay.attrs.CompilerAttrs][235]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %307 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][236]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %308 = nn.conv2d(%306, %307, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %309 = annotation.compiler_end(%308, meta[relay.attrs.CompilerAttrs][237]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %310 = annotation.compiler_end(%287, meta[relay.attrs.CompilerAttrs][239]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %311 = annotation.compiler_begin(%310, meta[relay.attrs.CompilerAttrs][240]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %312 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][241]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %313 = nn.conv2d(%311, %312, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %314 = annotation.compiler_end(%313, meta[relay.attrs.CompilerAttrs][242]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %315 = annotation.compiler_begin(%309, meta[relay.attrs.CompilerAttrs][238]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %316 = annotation.compiler_begin(%314, meta[relay.attrs.CompilerAttrs][243]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %317 = add(%315, %316) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %318 = annotation.compiler_end(%317, meta[relay.attrs.CompilerAttrs][244]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %319 = annotation.compiler_begin(%318, meta[relay.attrs.CompilerAttrs][245]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %320 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][246]) /* ty=Tensor[(512), float32] */;
  %321 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][247]) /* ty=Tensor[(512), float32] */;
  %322 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][248]) /* ty=Tensor[(512), float32] */;
  %323 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][249]) /* ty=Tensor[(512), float32] */;
  %324 = nn.batch_norm(%319, %320, %321, %322, %323, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %325 = annotation.compiler_end(%324, meta[relay.attrs.CompilerAttrs][250]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %326 = annotation.compiler_begin(%325, meta[relay.attrs.CompilerAttrs][251]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %327 = %326.0;
  %328 = annotation.compiler_end(%327, meta[relay.attrs.CompilerAttrs][252]);
  %329 = annotation.compiler_begin(%328, meta[relay.attrs.CompilerAttrs][253]);
  %330 = nn.relu(%329) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %331 = annotation.compiler_end(%330, meta[relay.attrs.CompilerAttrs][254]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %332 = annotation.compiler_begin(%331, meta[relay.attrs.CompilerAttrs][255]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %333 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][256]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %334 = nn.conv2d(%332, %333, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %335 = annotation.compiler_end(%334, meta[relay.attrs.CompilerAttrs][257]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %336 = annotation.compiler_begin(%335, meta[relay.attrs.CompilerAttrs][258]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %337 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][259]) /* ty=Tensor[(512), float32] */;
  %338 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][260]) /* ty=Tensor[(512), float32] */;
  %339 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][261]) /* ty=Tensor[(512), float32] */;
  %340 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][262]) /* ty=Tensor[(512), float32] */;
  %341 = nn.batch_norm(%336, %337, %338, %339, %340, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %342 = annotation.compiler_end(%341, meta[relay.attrs.CompilerAttrs][263]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %343 = annotation.compiler_begin(%342, meta[relay.attrs.CompilerAttrs][264]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %344 = %343.0;
  %345 = annotation.compiler_end(%344, meta[relay.attrs.CompilerAttrs][265]);
  %346 = annotation.compiler_begin(%345, meta[relay.attrs.CompilerAttrs][266]);
  %347 = nn.relu(%346) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %348 = annotation.compiler_end(%347, meta[relay.attrs.CompilerAttrs][267]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %349 = annotation.compiler_begin(%348, meta[relay.attrs.CompilerAttrs][268]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %350 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][269]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %351 = nn.conv2d(%349, %350, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %352 = annotation.compiler_end(%351, meta[relay.attrs.CompilerAttrs][270]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %353 = annotation.compiler_end(%317, meta[relay.attrs.CompilerAttrs][272]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %354 = annotation.compiler_begin(%352, meta[relay.attrs.CompilerAttrs][271]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %355 = annotation.compiler_begin(%353, meta[relay.attrs.CompilerAttrs][273]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %356 = add(%354, %355) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %357 = annotation.compiler_end(%356, meta[relay.attrs.CompilerAttrs][274]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %358 = annotation.compiler_begin(%357, meta[relay.attrs.CompilerAttrs][275]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %359 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][276]) /* ty=Tensor[(512), float32] */;
  %360 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][277]) /* ty=Tensor[(512), float32] */;
  %361 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][278]) /* ty=Tensor[(512), float32] */;
  %362 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][279]) /* ty=Tensor[(512), float32] */;
  %363 = nn.batch_norm(%358, %359, %360, %361, %362, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %364 = annotation.compiler_end(%363, meta[relay.attrs.CompilerAttrs][280]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %365 = annotation.compiler_begin(%364, meta[relay.attrs.CompilerAttrs][281]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %366 = %365.0;
  %367 = annotation.compiler_end(%366, meta[relay.attrs.CompilerAttrs][282]);
  %368 = annotation.compiler_begin(%367, meta[relay.attrs.CompilerAttrs][283]);
  %369 = nn.relu(%368) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %370 = annotation.compiler_end(%369, meta[relay.attrs.CompilerAttrs][284]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %371 = annotation.compiler_begin(%370, meta[relay.attrs.CompilerAttrs][285]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %372 = nn.global_avg_pool2d(%371) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %373 = annotation.compiler_end(%372, meta[relay.attrs.CompilerAttrs][286]) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %374 = annotation.compiler_begin(%373, meta[relay.attrs.CompilerAttrs][287]) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %375 = nn.batch_flatten(%374) /* ty=Tensor[(1, 512), float32] */;
  %376 = annotation.compiler_end(%375, meta[relay.attrs.CompilerAttrs][288]) /* ty=Tensor[(1, 512), float32] */;
  %377 = annotation.compiler_begin(%376, meta[relay.attrs.CompilerAttrs][289]) /* ty=Tensor[(1, 512), float32] */;
  %378 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][290]) /* ty=Tensor[(1000, 512), float32] */;
  %379 = nn.dense(%377, %378, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %380 = annotation.compiler_end(%379, meta[relay.attrs.CompilerAttrs][291]) /* ty=Tensor[(1, 1000), float32] */;
  %381 = annotation.compiler_begin(%380, meta[relay.attrs.CompilerAttrs][292]) /* ty=Tensor[(1, 1000), float32] */;
  %382 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][293]) /* ty=Tensor[(1000), float32] */;
  %383 = nn.bias_add(%381, %382, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  %384 = annotation.compiler_end(%383, meta[relay.attrs.CompilerAttrs][294]) /* ty=Tensor[(1, 1000), float32] */;
  %385 = annotation.compiler_begin(%384, meta[relay.attrs.CompilerAttrs][295]) /* ty=Tensor[(1, 1000), float32] */;
  %386 = nn.softmax(%385) /* ty=Tensor[(1, 1000), float32] */;
  annotation.compiler_end(%386, meta[relay.attrs.CompilerAttrs][296]) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = annotation.compiler_end(%5, meta[relay.attrs.CompilerAttrs][5]) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %7 = annotation.compiler_begin(%6, meta[relay.attrs.CompilerAttrs][6]) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %8 = %7.0;
  %9 = annotation.compiler_end(%8, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %10 = annotation.compiler_begin(%9, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %11 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %12 = nn.conv2d(%10, %11, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %13 = annotation.compiler_end(%12, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %14 = annotation.compiler_begin(%13, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %15 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(64), float32] */;
  %16 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(64), float32] */;
  %17 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %18 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %19 = nn.batch_norm(%14, %15, %16, %17, %18, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = annotation.compiler_end(%19, meta[relay.attrs.CompilerAttrs][16]) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %21 = annotation.compiler_begin(%20, meta[relay.attrs.CompilerAttrs][17]) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = annotation.compiler_end(%22, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %24 = annotation.compiler_begin(%23, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %26 = annotation.compiler_end(%25, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %27 = annotation.compiler_begin(%26, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %28 = nn.max_pool2d(%27, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = annotation.compiler_end(%28, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %30 = annotation.compiler_begin(%29, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %34 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %35 = nn.batch_norm(%30, %31, %32, %33, %34, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %36 = annotation.compiler_end(%35, meta[relay.attrs.CompilerAttrs][28]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %37 = annotation.compiler_begin(%36, meta[relay.attrs.CompilerAttrs][29]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %38 = %37.0;
  %39 = annotation.compiler_end(%38, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = annotation.compiler_begin(%39, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %42 = annotation.compiler_end(%41, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %43 = annotation.compiler_begin(%42, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %45 = nn.conv2d(%43, %44, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %46 = annotation.compiler_end(%45, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %47 = annotation.compiler_begin(%46, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %48 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %50 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(64), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(64), float32] */;
  %52 = nn.batch_norm(%47, %48, %49, %50, %51, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %53 = annotation.compiler_end(%52, meta[relay.attrs.CompilerAttrs][41]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %54 = annotation.compiler_begin(%53, meta[relay.attrs.CompilerAttrs][42]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %55 = %54.0;
  %56 = annotation.compiler_end(%55, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %57 = annotation.compiler_begin(%56, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %59 = annotation.compiler_end(%58, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %60 = annotation.compiler_begin(%59, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %61 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %62 = nn.conv2d(%60, %61, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %63 = annotation.compiler_end(%62, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %64 = annotation.compiler_end(%41, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %65 = annotation.compiler_begin(%64, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %66 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %67 = nn.conv2d(%65, %66, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %68 = annotation.compiler_end(%67, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %69 = annotation.compiler_begin(%63, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %70 = annotation.compiler_begin(%68, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %71 = add(%69, %70) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %72 = annotation.compiler_end(%71, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %73 = annotation.compiler_begin(%72, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %74 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(64), float32] */;
  %75 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(64), float32] */;
  %76 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(64), float32] */;
  %77 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(64), float32] */;
  %78 = nn.batch_norm(%73, %74, %75, %76, %77, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %79 = annotation.compiler_end(%78, meta[relay.attrs.CompilerAttrs][61]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %80 = annotation.compiler_begin(%79, meta[relay.attrs.CompilerAttrs][62]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %81 = %80.0;
  %82 = annotation.compiler_end(%81, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %83 = annotation.compiler_begin(%82, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %84 = nn.relu(%83) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %85 = annotation.compiler_end(%84, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %86 = annotation.compiler_begin(%85, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %87 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %89 = annotation.compiler_end(%88, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %90 = annotation.compiler_begin(%89, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %91 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(64), float32] */;
  %92 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(64), float32] */;
  %93 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(64), float32] */;
  %94 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(64), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %96 = annotation.compiler_end(%95, meta[relay.attrs.CompilerAttrs][74]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %97 = annotation.compiler_begin(%96, meta[relay.attrs.CompilerAttrs][75]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %98 = %97.0;
  %99 = annotation.compiler_end(%98, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %100 = annotation.compiler_begin(%99, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %101 = nn.relu(%100) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %102 = annotation.compiler_end(%101, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %103 = annotation.compiler_begin(%102, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %104 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %105 = nn.conv2d(%103, %104, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %106 = annotation.compiler_end(%105, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %107 = annotation.compiler_end(%71, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %108 = annotation.compiler_begin(%106, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %109 = annotation.compiler_begin(%107, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %110 = add(%108, %109) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %111 = annotation.compiler_end(%110, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %112 = annotation.compiler_begin(%111, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %113 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(64), float32] */;
  %114 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(64), float32] */;
  %115 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(64), float32] */;
  %116 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(64), float32] */;
  %117 = nn.batch_norm(%112, %113, %114, %115, %116, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %118 = annotation.compiler_end(%117, meta[relay.attrs.CompilerAttrs][91]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %119 = annotation.compiler_begin(%118, meta[relay.attrs.CompilerAttrs][92]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %120 = %119.0;
  %121 = annotation.compiler_end(%120, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %122 = annotation.compiler_begin(%121, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %123 = nn.relu(%122) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %124 = annotation.compiler_end(%123, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %125 = annotation.compiler_begin(%124, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %126 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %127 = nn.conv2d(%125, %126, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %128 = annotation.compiler_end(%127, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %129 = annotation.compiler_begin(%128, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %130 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(128), float32] */;
  %131 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(128), float32] */;
  %132 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(128), float32] */;
  %133 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(128), float32] */;
  %134 = nn.batch_norm(%129, %130, %131, %132, %133, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %135 = annotation.compiler_end(%134, meta[relay.attrs.CompilerAttrs][104]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %136 = annotation.compiler_begin(%135, meta[relay.attrs.CompilerAttrs][105]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %137 = %136.0;
  %138 = annotation.compiler_end(%137, meta[relay.attrs.CompilerAttrs][106]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %139 = annotation.compiler_begin(%138, meta[relay.attrs.CompilerAttrs][107]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %140 = nn.relu(%139) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %141 = annotation.compiler_end(%140, meta[relay.attrs.CompilerAttrs][108]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %142 = annotation.compiler_begin(%141, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %143 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][110]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %144 = nn.conv2d(%142, %143, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %145 = annotation.compiler_end(%144, meta[relay.attrs.CompilerAttrs][111]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %146 = annotation.compiler_end(%123, meta[relay.attrs.CompilerAttrs][113]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %147 = annotation.compiler_begin(%146, meta[relay.attrs.CompilerAttrs][114]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %148 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][115]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %150 = annotation.compiler_end(%149, meta[relay.attrs.CompilerAttrs][116]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %151 = annotation.compiler_begin(%145, meta[relay.attrs.CompilerAttrs][112]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %152 = annotation.compiler_begin(%150, meta[relay.attrs.CompilerAttrs][117]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %153 = add(%151, %152) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %154 = annotation.compiler_end(%153, meta[relay.attrs.CompilerAttrs][118]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %155 = annotation.compiler_begin(%154, meta[relay.attrs.CompilerAttrs][119]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %156 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][120]) /* ty=Tensor[(128), float32] */;
  %157 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][121]) /* ty=Tensor[(128), float32] */;
  %158 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][122]) /* ty=Tensor[(128), float32] */;
  %159 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][123]) /* ty=Tensor[(128), float32] */;
  %160 = nn.batch_norm(%155, %156, %157, %158, %159, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %161 = annotation.compiler_end(%160, meta[relay.attrs.CompilerAttrs][124]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %162 = annotation.compiler_begin(%161, meta[relay.attrs.CompilerAttrs][125]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %163 = %162.0;
  %164 = annotation.compiler_end(%163, meta[relay.attrs.CompilerAttrs][126]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %165 = annotation.compiler_begin(%164, meta[relay.attrs.CompilerAttrs][127]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %166 = nn.relu(%165) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %167 = annotation.compiler_end(%166, meta[relay.attrs.CompilerAttrs][128]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %168 = annotation.compiler_begin(%167, meta[relay.attrs.CompilerAttrs][129]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %169 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][130]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %171 = annotation.compiler_end(%170, meta[relay.attrs.CompilerAttrs][131]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %172 = annotation.compiler_begin(%171, meta[relay.attrs.CompilerAttrs][132]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %173 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][133]) /* ty=Tensor[(128), float32] */;
  %174 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][134]) /* ty=Tensor[(128), float32] */;
  %175 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][135]) /* ty=Tensor[(128), float32] */;
  %176 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][136]) /* ty=Tensor[(128), float32] */;
  %177 = nn.batch_norm(%172, %173, %174, %175, %176, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %178 = annotation.compiler_end(%177, meta[relay.attrs.CompilerAttrs][137]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %179 = annotation.compiler_begin(%178, meta[relay.attrs.CompilerAttrs][138]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %180 = %179.0;
  %181 = annotation.compiler_end(%180, meta[relay.attrs.CompilerAttrs][139]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %182 = annotation.compiler_begin(%181, meta[relay.attrs.CompilerAttrs][140]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %183 = nn.relu(%182) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %184 = annotation.compiler_end(%183, meta[relay.attrs.CompilerAttrs][141]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %185 = annotation.compiler_begin(%184, meta[relay.attrs.CompilerAttrs][142]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %186 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][143]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %187 = nn.conv2d(%185, %186, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][144]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %189 = annotation.compiler_end(%153, meta[relay.attrs.CompilerAttrs][146]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %190 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][145]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %191 = annotation.compiler_begin(%189, meta[relay.attrs.CompilerAttrs][147]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %192 = add(%190, %191) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %193 = annotation.compiler_end(%192, meta[relay.attrs.CompilerAttrs][148]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %194 = annotation.compiler_begin(%193, meta[relay.attrs.CompilerAttrs][149]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %195 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][150]) /* ty=Tensor[(128), float32] */;
  %196 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][151]) /* ty=Tensor[(128), float32] */;
  %197 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][152]) /* ty=Tensor[(128), float32] */;
  %198 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][153]) /* ty=Tensor[(128), float32] */;
  %199 = nn.batch_norm(%194, %195, %196, %197, %198, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %200 = annotation.compiler_end(%199, meta[relay.attrs.CompilerAttrs][154]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %201 = annotation.compiler_begin(%200, meta[relay.attrs.CompilerAttrs][155]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %202 = %201.0;
  %203 = annotation.compiler_end(%202, meta[relay.attrs.CompilerAttrs][156]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %204 = annotation.compiler_begin(%203, meta[relay.attrs.CompilerAttrs][157]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %206 = annotation.compiler_end(%205, meta[relay.attrs.CompilerAttrs][158]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %207 = annotation.compiler_begin(%206, meta[relay.attrs.CompilerAttrs][159]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %208 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][160]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %209 = nn.conv2d(%207, %208, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %210 = annotation.compiler_end(%209, meta[relay.attrs.CompilerAttrs][161]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %211 = annotation.compiler_begin(%210, meta[relay.attrs.CompilerAttrs][162]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %212 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][163]) /* ty=Tensor[(256), float32] */;
  %213 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][164]) /* ty=Tensor[(256), float32] */;
  %214 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][165]) /* ty=Tensor[(256), float32] */;
  %215 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][166]) /* ty=Tensor[(256), float32] */;
  %216 = nn.batch_norm(%211, %212, %213, %214, %215, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %217 = annotation.compiler_end(%216, meta[relay.attrs.CompilerAttrs][167]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %218 = annotation.compiler_begin(%217, meta[relay.attrs.CompilerAttrs][168]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %219 = %218.0;
  %220 = annotation.compiler_end(%219, meta[relay.attrs.CompilerAttrs][169]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %221 = annotation.compiler_begin(%220, meta[relay.attrs.CompilerAttrs][170]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %222 = nn.relu(%221) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %223 = annotation.compiler_end(%222, meta[relay.attrs.CompilerAttrs][171]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %224 = annotation.compiler_begin(%223, meta[relay.attrs.CompilerAttrs][172]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %225 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][173]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %226 = nn.conv2d(%224, %225, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %227 = annotation.compiler_end(%226, meta[relay.attrs.CompilerAttrs][174]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %228 = annotation.compiler_end(%205, meta[relay.attrs.CompilerAttrs][176]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %229 = annotation.compiler_begin(%228, meta[relay.attrs.CompilerAttrs][177]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %230 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][178]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %231 = nn.conv2d(%229, %230, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %232 = annotation.compiler_end(%231, meta[relay.attrs.CompilerAttrs][179]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %233 = annotation.compiler_begin(%227, meta[relay.attrs.CompilerAttrs][175]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %234 = annotation.compiler_begin(%232, meta[relay.attrs.CompilerAttrs][180]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %235 = add(%233, %234) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %236 = annotation.compiler_end(%235, meta[relay.attrs.CompilerAttrs][181]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %237 = annotation.compiler_begin(%236, meta[relay.attrs.CompilerAttrs][182]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %238 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][183]) /* ty=Tensor[(256), float32] */;
  %239 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][184]) /* ty=Tensor[(256), float32] */;
  %240 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][185]) /* ty=Tensor[(256), float32] */;
  %241 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][186]) /* ty=Tensor[(256), float32] */;
  %242 = nn.batch_norm(%237, %238, %239, %240, %241, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %243 = annotation.compiler_end(%242, meta[relay.attrs.CompilerAttrs][187]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %244 = annotation.compiler_begin(%243, meta[relay.attrs.CompilerAttrs][188]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %245 = %244.0;
  %246 = annotation.compiler_end(%245, meta[relay.attrs.CompilerAttrs][189]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %247 = annotation.compiler_begin(%246, meta[relay.attrs.CompilerAttrs][190]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %248 = nn.relu(%247) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %249 = annotation.compiler_end(%248, meta[relay.attrs.CompilerAttrs][191]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %250 = annotation.compiler_begin(%249, meta[relay.attrs.CompilerAttrs][192]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %251 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][193]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %252 = nn.conv2d(%250, %251, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %253 = annotation.compiler_end(%252, meta[relay.attrs.CompilerAttrs][194]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %254 = annotation.compiler_begin(%253, meta[relay.attrs.CompilerAttrs][195]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %255 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][196]) /* ty=Tensor[(256), float32] */;
  %256 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][197]) /* ty=Tensor[(256), float32] */;
  %257 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][198]) /* ty=Tensor[(256), float32] */;
  %258 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][199]) /* ty=Tensor[(256), float32] */;
  %259 = nn.batch_norm(%254, %255, %256, %257, %258, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %260 = annotation.compiler_end(%259, meta[relay.attrs.CompilerAttrs][200]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %261 = annotation.compiler_begin(%260, meta[relay.attrs.CompilerAttrs][201]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %262 = %261.0;
  %263 = annotation.compiler_end(%262, meta[relay.attrs.CompilerAttrs][202]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %264 = annotation.compiler_begin(%263, meta[relay.attrs.CompilerAttrs][203]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %265 = nn.relu(%264) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %266 = annotation.compiler_end(%265, meta[relay.attrs.CompilerAttrs][204]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %267 = annotation.compiler_begin(%266, meta[relay.attrs.CompilerAttrs][205]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %268 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][206]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %269 = nn.conv2d(%267, %268, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %270 = annotation.compiler_end(%269, meta[relay.attrs.CompilerAttrs][207]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %271 = annotation.compiler_end(%235, meta[relay.attrs.CompilerAttrs][209]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %272 = annotation.compiler_begin(%270, meta[relay.attrs.CompilerAttrs][208]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %273 = annotation.compiler_begin(%271, meta[relay.attrs.CompilerAttrs][210]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %274 = add(%272, %273) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %275 = annotation.compiler_end(%274, meta[relay.attrs.CompilerAttrs][211]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %276 = annotation.compiler_begin(%275, meta[relay.attrs.CompilerAttrs][212]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %277 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][213]) /* ty=Tensor[(256), float32] */;
  %278 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][214]) /* ty=Tensor[(256), float32] */;
  %279 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][215]) /* ty=Tensor[(256), float32] */;
  %280 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][216]) /* ty=Tensor[(256), float32] */;
  %281 = nn.batch_norm(%276, %277, %278, %279, %280, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %282 = annotation.compiler_end(%281, meta[relay.attrs.CompilerAttrs][217]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %283 = annotation.compiler_begin(%282, meta[relay.attrs.CompilerAttrs][218]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %284 = %283.0;
  %285 = annotation.compiler_end(%284, meta[relay.attrs.CompilerAttrs][219]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %286 = annotation.compiler_begin(%285, meta[relay.attrs.CompilerAttrs][220]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %287 = nn.relu(%286) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %288 = annotation.compiler_end(%287, meta[relay.attrs.CompilerAttrs][221]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %289 = annotation.compiler_begin(%288, meta[relay.attrs.CompilerAttrs][222]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %290 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][223]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %291 = nn.conv2d(%289, %290, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %292 = annotation.compiler_end(%291, meta[relay.attrs.CompilerAttrs][224]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %293 = annotation.compiler_begin(%292, meta[relay.attrs.CompilerAttrs][225]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %294 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][226]) /* ty=Tensor[(512), float32] */;
  %295 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][227]) /* ty=Tensor[(512), float32] */;
  %296 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][228]) /* ty=Tensor[(512), float32] */;
  %297 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][229]) /* ty=Tensor[(512), float32] */;
  %298 = nn.batch_norm(%293, %294, %295, %296, %297, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %299 = annotation.compiler_end(%298, meta[relay.attrs.CompilerAttrs][230]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %300 = annotation.compiler_begin(%299, meta[relay.attrs.CompilerAttrs][231]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %301 = %300.0;
  %302 = annotation.compiler_end(%301, meta[relay.attrs.CompilerAttrs][232]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %303 = annotation.compiler_begin(%302, meta[relay.attrs.CompilerAttrs][233]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %304 = nn.relu(%303) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %305 = annotation.compiler_end(%304, meta[relay.attrs.CompilerAttrs][234]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %306 = annotation.compiler_begin(%305, meta[relay.attrs.CompilerAttrs][235]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %307 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][236]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %308 = nn.conv2d(%306, %307, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %309 = annotation.compiler_end(%308, meta[relay.attrs.CompilerAttrs][237]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %310 = annotation.compiler_end(%287, meta[relay.attrs.CompilerAttrs][239]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %311 = annotation.compiler_begin(%310, meta[relay.attrs.CompilerAttrs][240]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %312 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][241]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %313 = nn.conv2d(%311, %312, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %314 = annotation.compiler_end(%313, meta[relay.attrs.CompilerAttrs][242]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %315 = annotation.compiler_begin(%309, meta[relay.attrs.CompilerAttrs][238]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %316 = annotation.compiler_begin(%314, meta[relay.attrs.CompilerAttrs][243]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %317 = add(%315, %316) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %318 = annotation.compiler_end(%317, meta[relay.attrs.CompilerAttrs][244]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %319 = annotation.compiler_begin(%318, meta[relay.attrs.CompilerAttrs][245]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %320 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][246]) /* ty=Tensor[(512), float32] */;
  %321 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][247]) /* ty=Tensor[(512), float32] */;
  %322 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][248]) /* ty=Tensor[(512), float32] */;
  %323 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][249]) /* ty=Tensor[(512), float32] */;
  %324 = nn.batch_norm(%319, %320, %321, %322, %323, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %325 = annotation.compiler_end(%324, meta[relay.attrs.CompilerAttrs][250]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %326 = annotation.compiler_begin(%325, meta[relay.attrs.CompilerAttrs][251]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %327 = %326.0;
  %328 = annotation.compiler_end(%327, meta[relay.attrs.CompilerAttrs][252]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %329 = annotation.compiler_begin(%328, meta[relay.attrs.CompilerAttrs][253]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %330 = nn.relu(%329) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %331 = annotation.compiler_end(%330, meta[relay.attrs.CompilerAttrs][254]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %332 = annotation.compiler_begin(%331, meta[relay.attrs.CompilerAttrs][255]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %333 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][256]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %334 = nn.conv2d(%332, %333, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %335 = annotation.compiler_end(%334, meta[relay.attrs.CompilerAttrs][257]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %336 = annotation.compiler_begin(%335, meta[relay.attrs.CompilerAttrs][258]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %337 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][259]) /* ty=Tensor[(512), float32] */;
  %338 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][260]) /* ty=Tensor[(512), float32] */;
  %339 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][261]) /* ty=Tensor[(512), float32] */;
  %340 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][262]) /* ty=Tensor[(512), float32] */;
  %341 = nn.batch_norm(%336, %337, %338, %339, %340, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %342 = annotation.compiler_end(%341, meta[relay.attrs.CompilerAttrs][263]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %343 = annotation.compiler_begin(%342, meta[relay.attrs.CompilerAttrs][264]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %344 = %343.0;
  %345 = annotation.compiler_end(%344, meta[relay.attrs.CompilerAttrs][265]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %346 = annotation.compiler_begin(%345, meta[relay.attrs.CompilerAttrs][266]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %347 = nn.relu(%346) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %348 = annotation.compiler_end(%347, meta[relay.attrs.CompilerAttrs][267]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %349 = annotation.compiler_begin(%348, meta[relay.attrs.CompilerAttrs][268]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %350 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][269]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %351 = nn.conv2d(%349, %350, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %352 = annotation.compiler_end(%351, meta[relay.attrs.CompilerAttrs][270]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %353 = annotation.compiler_end(%317, meta[relay.attrs.CompilerAttrs][272]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %354 = annotation.compiler_begin(%352, meta[relay.attrs.CompilerAttrs][271]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %355 = annotation.compiler_begin(%353, meta[relay.attrs.CompilerAttrs][273]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %356 = add(%354, %355) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %357 = annotation.compiler_end(%356, meta[relay.attrs.CompilerAttrs][274]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %358 = annotation.compiler_begin(%357, meta[relay.attrs.CompilerAttrs][275]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %359 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][276]) /* ty=Tensor[(512), float32] */;
  %360 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][277]) /* ty=Tensor[(512), float32] */;
  %361 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][278]) /* ty=Tensor[(512), float32] */;
  %362 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][279]) /* ty=Tensor[(512), float32] */;
  %363 = nn.batch_norm(%358, %359, %360, %361, %362, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %364 = annotation.compiler_end(%363, meta[relay.attrs.CompilerAttrs][280]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %365 = annotation.compiler_begin(%364, meta[relay.attrs.CompilerAttrs][281]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %366 = %365.0;
  %367 = annotation.compiler_end(%366, meta[relay.attrs.CompilerAttrs][282]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %368 = annotation.compiler_begin(%367, meta[relay.attrs.CompilerAttrs][283]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %369 = nn.relu(%368) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %370 = annotation.compiler_end(%369, meta[relay.attrs.CompilerAttrs][284]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %371 = annotation.compiler_begin(%370, meta[relay.attrs.CompilerAttrs][285]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %372 = nn.global_avg_pool2d(%371) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %373 = annotation.compiler_end(%372, meta[relay.attrs.CompilerAttrs][286]) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %374 = annotation.compiler_begin(%373, meta[relay.attrs.CompilerAttrs][287]) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %375 = nn.batch_flatten(%374) /* ty=Tensor[(1, 512), float32] */;
  %376 = annotation.compiler_end(%375, meta[relay.attrs.CompilerAttrs][288]) /* ty=Tensor[(1, 512), float32] */;
  %377 = annotation.compiler_begin(%376, meta[relay.attrs.CompilerAttrs][289]) /* ty=Tensor[(1, 512), float32] */;
  %378 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][290]) /* ty=Tensor[(1000, 512), float32] */;
  %379 = nn.dense(%377, %378, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %380 = annotation.compiler_end(%379, meta[relay.attrs.CompilerAttrs][291]) /* ty=Tensor[(1, 1000), float32] */;
  %381 = annotation.compiler_begin(%380, meta[relay.attrs.CompilerAttrs][292]) /* ty=Tensor[(1, 1000), float32] */;
  %382 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][293]) /* ty=Tensor[(1000), float32] */;
  %383 = nn.bias_add(%381, %382, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  %384 = annotation.compiler_end(%383, meta[relay.attrs.CompilerAttrs][294]) /* ty=Tensor[(1, 1000), float32] */;
  %385 = annotation.compiler_begin(%384, meta[relay.attrs.CompilerAttrs][295]) /* ty=Tensor[(1, 1000), float32] */;
  %386 = nn.softmax(%385) /* ty=Tensor[(1, 1000), float32] */;
  annotation.compiler_end(%386, meta[relay.attrs.CompilerAttrs][296]) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: sequentialopt_level: 2required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = annotation.compiler_end(%5, meta[relay.attrs.CompilerAttrs][5]) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %7 = annotation.compiler_begin(%6, meta[relay.attrs.CompilerAttrs][6]) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %8 = %7.0;
  %9 = annotation.compiler_end(%8, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %10 = annotation.compiler_begin(%9, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %11 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %12 = nn.conv2d(%10, %11, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %13 = annotation.compiler_end(%12, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %14 = annotation.compiler_begin(%13, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %15 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(64), float32] */;
  %16 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(64), float32] */;
  %17 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %18 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %19 = nn.batch_norm(%14, %15, %16, %17, %18, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = annotation.compiler_end(%19, meta[relay.attrs.CompilerAttrs][16]) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %21 = annotation.compiler_begin(%20, meta[relay.attrs.CompilerAttrs][17]) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = annotation.compiler_end(%22, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %24 = annotation.compiler_begin(%23, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %26 = annotation.compiler_end(%25, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %27 = annotation.compiler_begin(%26, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %28 = nn.max_pool2d(%27, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = annotation.compiler_end(%28, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %30 = annotation.compiler_begin(%29, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %34 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %35 = nn.batch_norm(%30, %31, %32, %33, %34, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %36 = annotation.compiler_end(%35, meta[relay.attrs.CompilerAttrs][28]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %37 = annotation.compiler_begin(%36, meta[relay.attrs.CompilerAttrs][29]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %38 = %37.0;
  %39 = annotation.compiler_end(%38, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = annotation.compiler_begin(%39, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %42 = annotation.compiler_end(%41, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %43 = annotation.compiler_begin(%42, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %45 = nn.conv2d(%43, %44, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %46 = annotation.compiler_end(%45, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %47 = annotation.compiler_begin(%46, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %48 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %50 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(64), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(64), float32] */;
  %52 = nn.batch_norm(%47, %48, %49, %50, %51, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %53 = annotation.compiler_end(%52, meta[relay.attrs.CompilerAttrs][41]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %54 = annotation.compiler_begin(%53, meta[relay.attrs.CompilerAttrs][42]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %55 = %54.0;
  %56 = annotation.compiler_end(%55, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %57 = annotation.compiler_begin(%56, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %59 = annotation.compiler_end(%58, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %60 = annotation.compiler_begin(%59, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %61 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %62 = nn.conv2d(%60, %61, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %63 = annotation.compiler_end(%62, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %64 = annotation.compiler_end(%41, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %65 = annotation.compiler_begin(%64, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %66 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %67 = nn.conv2d(%65, %66, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %68 = annotation.compiler_end(%67, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %69 = annotation.compiler_begin(%63, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %70 = annotation.compiler_begin(%68, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %71 = add(%69, %70) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %72 = annotation.compiler_end(%71, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %73 = annotation.compiler_begin(%72, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %74 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(64), float32] */;
  %75 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(64), float32] */;
  %76 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(64), float32] */;
  %77 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(64), float32] */;
  %78 = nn.batch_norm(%73, %74, %75, %76, %77, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %79 = annotation.compiler_end(%78, meta[relay.attrs.CompilerAttrs][61]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %80 = annotation.compiler_begin(%79, meta[relay.attrs.CompilerAttrs][62]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %81 = %80.0;
  %82 = annotation.compiler_end(%81, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %83 = annotation.compiler_begin(%82, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %84 = nn.relu(%83) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %85 = annotation.compiler_end(%84, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %86 = annotation.compiler_begin(%85, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %87 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %89 = annotation.compiler_end(%88, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %90 = annotation.compiler_begin(%89, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %91 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(64), float32] */;
  %92 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(64), float32] */;
  %93 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(64), float32] */;
  %94 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(64), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %96 = annotation.compiler_end(%95, meta[relay.attrs.CompilerAttrs][74]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %97 = annotation.compiler_begin(%96, meta[relay.attrs.CompilerAttrs][75]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %98 = %97.0;
  %99 = annotation.compiler_end(%98, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %100 = annotation.compiler_begin(%99, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %101 = nn.relu(%100) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %102 = annotation.compiler_end(%101, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %103 = annotation.compiler_begin(%102, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %104 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %105 = nn.conv2d(%103, %104, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %106 = annotation.compiler_end(%105, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %107 = annotation.compiler_end(%71, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %108 = annotation.compiler_begin(%106, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %109 = annotation.compiler_begin(%107, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %110 = add(%108, %109) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %111 = annotation.compiler_end(%110, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %112 = annotation.compiler_begin(%111, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %113 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(64), float32] */;
  %114 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(64), float32] */;
  %115 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(64), float32] */;
  %116 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(64), float32] */;
  %117 = nn.batch_norm(%112, %113, %114, %115, %116, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %118 = annotation.compiler_end(%117, meta[relay.attrs.CompilerAttrs][91]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %119 = annotation.compiler_begin(%118, meta[relay.attrs.CompilerAttrs][92]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %120 = %119.0;
  %121 = annotation.compiler_end(%120, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %122 = annotation.compiler_begin(%121, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %123 = nn.relu(%122) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %124 = annotation.compiler_end(%123, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %125 = annotation.compiler_begin(%124, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %126 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %127 = nn.conv2d(%125, %126, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %128 = annotation.compiler_end(%127, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %129 = annotation.compiler_begin(%128, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %130 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(128), float32] */;
  %131 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(128), float32] */;
  %132 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(128), float32] */;
  %133 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(128), float32] */;
  %134 = nn.batch_norm(%129, %130, %131, %132, %133, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %135 = annotation.compiler_end(%134, meta[relay.attrs.CompilerAttrs][104]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %136 = annotation.compiler_begin(%135, meta[relay.attrs.CompilerAttrs][105]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %137 = %136.0;
  %138 = annotation.compiler_end(%137, meta[relay.attrs.CompilerAttrs][106]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %139 = annotation.compiler_begin(%138, meta[relay.attrs.CompilerAttrs][107]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %140 = nn.relu(%139) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %141 = annotation.compiler_end(%140, meta[relay.attrs.CompilerAttrs][108]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %142 = annotation.compiler_begin(%141, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %143 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][110]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %144 = nn.conv2d(%142, %143, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %145 = annotation.compiler_end(%144, meta[relay.attrs.CompilerAttrs][111]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %146 = annotation.compiler_end(%123, meta[relay.attrs.CompilerAttrs][113]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %147 = annotation.compiler_begin(%146, meta[relay.attrs.CompilerAttrs][114]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %148 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][115]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %150 = annotation.compiler_end(%149, meta[relay.attrs.CompilerAttrs][116]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %151 = annotation.compiler_begin(%145, meta[relay.attrs.CompilerAttrs][112]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %152 = annotation.compiler_begin(%150, meta[relay.attrs.CompilerAttrs][117]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %153 = add(%151, %152) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %154 = annotation.compiler_end(%153, meta[relay.attrs.CompilerAttrs][118]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %155 = annotation.compiler_begin(%154, meta[relay.attrs.CompilerAttrs][119]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %156 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][120]) /* ty=Tensor[(128), float32] */;
  %157 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][121]) /* ty=Tensor[(128), float32] */;
  %158 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][122]) /* ty=Tensor[(128), float32] */;
  %159 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][123]) /* ty=Tensor[(128), float32] */;
  %160 = nn.batch_norm(%155, %156, %157, %158, %159, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %161 = annotation.compiler_end(%160, meta[relay.attrs.CompilerAttrs][124]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %162 = annotation.compiler_begin(%161, meta[relay.attrs.CompilerAttrs][125]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %163 = %162.0;
  %164 = annotation.compiler_end(%163, meta[relay.attrs.CompilerAttrs][126]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %165 = annotation.compiler_begin(%164, meta[relay.attrs.CompilerAttrs][127]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %166 = nn.relu(%165) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %167 = annotation.compiler_end(%166, meta[relay.attrs.CompilerAttrs][128]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %168 = annotation.compiler_begin(%167, meta[relay.attrs.CompilerAttrs][129]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %169 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][130]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %171 = annotation.compiler_end(%170, meta[relay.attrs.CompilerAttrs][131]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %172 = annotation.compiler_begin(%171, meta[relay.attrs.CompilerAttrs][132]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %173 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][133]) /* ty=Tensor[(128), float32] */;
  %174 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][134]) /* ty=Tensor[(128), float32] */;
  %175 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][135]) /* ty=Tensor[(128), float32] */;
  %176 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][136]) /* ty=Tensor[(128), float32] */;
  %177 = nn.batch_norm(%172, %173, %174, %175, %176, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %178 = annotation.compiler_end(%177, meta[relay.attrs.CompilerAttrs][137]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %179 = annotation.compiler_begin(%178, meta[relay.attrs.CompilerAttrs][138]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %180 = %179.0;
  %181 = annotation.compiler_end(%180, meta[relay.attrs.CompilerAttrs][139]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %182 = annotation.compiler_begin(%181, meta[relay.attrs.CompilerAttrs][140]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %183 = nn.relu(%182) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %184 = annotation.compiler_end(%183, meta[relay.attrs.CompilerAttrs][141]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %185 = annotation.compiler_begin(%184, meta[relay.attrs.CompilerAttrs][142]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %186 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][143]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %187 = nn.conv2d(%185, %186, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][144]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %189 = annotation.compiler_end(%153, meta[relay.attrs.CompilerAttrs][146]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %190 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][145]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %191 = annotation.compiler_begin(%189, meta[relay.attrs.CompilerAttrs][147]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %192 = add(%190, %191) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %193 = annotation.compiler_end(%192, meta[relay.attrs.CompilerAttrs][148]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %194 = annotation.compiler_begin(%193, meta[relay.attrs.CompilerAttrs][149]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %195 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][150]) /* ty=Tensor[(128), float32] */;
  %196 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][151]) /* ty=Tensor[(128), float32] */;
  %197 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][152]) /* ty=Tensor[(128), float32] */;
  %198 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][153]) /* ty=Tensor[(128), float32] */;
  %199 = nn.batch_norm(%194, %195, %196, %197, %198, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %200 = annotation.compiler_end(%199, meta[relay.attrs.CompilerAttrs][154]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %201 = annotation.compiler_begin(%200, meta[relay.attrs.CompilerAttrs][155]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %202 = %201.0;
  %203 = annotation.compiler_end(%202, meta[relay.attrs.CompilerAttrs][156]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %204 = annotation.compiler_begin(%203, meta[relay.attrs.CompilerAttrs][157]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %206 = annotation.compiler_end(%205, meta[relay.attrs.CompilerAttrs][158]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %207 = annotation.compiler_begin(%206, meta[relay.attrs.CompilerAttrs][159]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %208 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][160]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %209 = nn.conv2d(%207, %208, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %210 = annotation.compiler_end(%209, meta[relay.attrs.CompilerAttrs][161]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %211 = annotation.compiler_begin(%210, meta[relay.attrs.CompilerAttrs][162]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %212 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][163]) /* ty=Tensor[(256), float32] */;
  %213 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][164]) /* ty=Tensor[(256), float32] */;
  %214 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][165]) /* ty=Tensor[(256), float32] */;
  %215 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][166]) /* ty=Tensor[(256), float32] */;
  %216 = nn.batch_norm(%211, %212, %213, %214, %215, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %217 = annotation.compiler_end(%216, meta[relay.attrs.CompilerAttrs][167]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %218 = annotation.compiler_begin(%217, meta[relay.attrs.CompilerAttrs][168]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %219 = %218.0;
  %220 = annotation.compiler_end(%219, meta[relay.attrs.CompilerAttrs][169]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %221 = annotation.compiler_begin(%220, meta[relay.attrs.CompilerAttrs][170]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %222 = nn.relu(%221) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %223 = annotation.compiler_end(%222, meta[relay.attrs.CompilerAttrs][171]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %224 = annotation.compiler_begin(%223, meta[relay.attrs.CompilerAttrs][172]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %225 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][173]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %226 = nn.conv2d(%224, %225, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %227 = annotation.compiler_end(%226, meta[relay.attrs.CompilerAttrs][174]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %228 = annotation.compiler_end(%205, meta[relay.attrs.CompilerAttrs][176]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %229 = annotation.compiler_begin(%228, meta[relay.attrs.CompilerAttrs][177]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %230 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][178]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %231 = nn.conv2d(%229, %230, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %232 = annotation.compiler_end(%231, meta[relay.attrs.CompilerAttrs][179]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %233 = annotation.compiler_begin(%227, meta[relay.attrs.CompilerAttrs][175]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %234 = annotation.compiler_begin(%232, meta[relay.attrs.CompilerAttrs][180]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %235 = add(%233, %234) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %236 = annotation.compiler_end(%235, meta[relay.attrs.CompilerAttrs][181]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %237 = annotation.compiler_begin(%236, meta[relay.attrs.CompilerAttrs][182]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %238 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][183]) /* ty=Tensor[(256), float32] */;
  %239 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][184]) /* ty=Tensor[(256), float32] */;
  %240 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][185]) /* ty=Tensor[(256), float32] */;
  %241 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][186]) /* ty=Tensor[(256), float32] */;
  %242 = nn.batch_norm(%237, %238, %239, %240, %241, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %243 = annotation.compiler_end(%242, meta[relay.attrs.CompilerAttrs][187]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %244 = annotation.compiler_begin(%243, meta[relay.attrs.CompilerAttrs][188]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %245 = %244.0;
  %246 = annotation.compiler_end(%245, meta[relay.attrs.CompilerAttrs][189]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %247 = annotation.compiler_begin(%246, meta[relay.attrs.CompilerAttrs][190]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %248 = nn.relu(%247) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %249 = annotation.compiler_end(%248, meta[relay.attrs.CompilerAttrs][191]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %250 = annotation.compiler_begin(%249, meta[relay.attrs.CompilerAttrs][192]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %251 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][193]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %252 = nn.conv2d(%250, %251, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %253 = annotation.compiler_end(%252, meta[relay.attrs.CompilerAttrs][194]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %254 = annotation.compiler_begin(%253, meta[relay.attrs.CompilerAttrs][195]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %255 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][196]) /* ty=Tensor[(256), float32] */;
  %256 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][197]) /* ty=Tensor[(256), float32] */;
  %257 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][198]) /* ty=Tensor[(256), float32] */;
  %258 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][199]) /* ty=Tensor[(256), float32] */;
  %259 = nn.batch_norm(%254, %255, %256, %257, %258, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %260 = annotation.compiler_end(%259, meta[relay.attrs.CompilerAttrs][200]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %261 = annotation.compiler_begin(%260, meta[relay.attrs.CompilerAttrs][201]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %262 = %261.0;
  %263 = annotation.compiler_end(%262, meta[relay.attrs.CompilerAttrs][202]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %264 = annotation.compiler_begin(%263, meta[relay.attrs.CompilerAttrs][203]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %265 = nn.relu(%264) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %266 = annotation.compiler_end(%265, meta[relay.attrs.CompilerAttrs][204]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %267 = annotation.compiler_begin(%266, meta[relay.attrs.CompilerAttrs][205]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %268 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][206]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %269 = nn.conv2d(%267, %268, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %270 = annotation.compiler_end(%269, meta[relay.attrs.CompilerAttrs][207]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %271 = annotation.compiler_end(%235, meta[relay.attrs.CompilerAttrs][209]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %272 = annotation.compiler_begin(%270, meta[relay.attrs.CompilerAttrs][208]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %273 = annotation.compiler_begin(%271, meta[relay.attrs.CompilerAttrs][210]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %274 = add(%272, %273) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %275 = annotation.compiler_end(%274, meta[relay.attrs.CompilerAttrs][211]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %276 = annotation.compiler_begin(%275, meta[relay.attrs.CompilerAttrs][212]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %277 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][213]) /* ty=Tensor[(256), float32] */;
  %278 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][214]) /* ty=Tensor[(256), float32] */;
  %279 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][215]) /* ty=Tensor[(256), float32] */;
  %280 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][216]) /* ty=Tensor[(256), float32] */;
  %281 = nn.batch_norm(%276, %277, %278, %279, %280, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %282 = annotation.compiler_end(%281, meta[relay.attrs.CompilerAttrs][217]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %283 = annotation.compiler_begin(%282, meta[relay.attrs.CompilerAttrs][218]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %284 = %283.0;
  %285 = annotation.compiler_end(%284, meta[relay.attrs.CompilerAttrs][219]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %286 = annotation.compiler_begin(%285, meta[relay.attrs.CompilerAttrs][220]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %287 = nn.relu(%286) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %288 = annotation.compiler_end(%287, meta[relay.attrs.CompilerAttrs][221]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %289 = annotation.compiler_begin(%288, meta[relay.attrs.CompilerAttrs][222]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %290 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][223]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %291 = nn.conv2d(%289, %290, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %292 = annotation.compiler_end(%291, meta[relay.attrs.CompilerAttrs][224]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %293 = annotation.compiler_begin(%292, meta[relay.attrs.CompilerAttrs][225]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %294 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][226]) /* ty=Tensor[(512), float32] */;
  %295 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][227]) /* ty=Tensor[(512), float32] */;
  %296 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][228]) /* ty=Tensor[(512), float32] */;
  %297 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][229]) /* ty=Tensor[(512), float32] */;
  %298 = nn.batch_norm(%293, %294, %295, %296, %297, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %299 = annotation.compiler_end(%298, meta[relay.attrs.CompilerAttrs][230]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %300 = annotation.compiler_begin(%299, meta[relay.attrs.CompilerAttrs][231]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %301 = %300.0;
  %302 = annotation.compiler_end(%301, meta[relay.attrs.CompilerAttrs][232]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %303 = annotation.compiler_begin(%302, meta[relay.attrs.CompilerAttrs][233]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %304 = nn.relu(%303) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %305 = annotation.compiler_end(%304, meta[relay.attrs.CompilerAttrs][234]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %306 = annotation.compiler_begin(%305, meta[relay.attrs.CompilerAttrs][235]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %307 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][236]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %308 = nn.conv2d(%306, %307, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %309 = annotation.compiler_end(%308, meta[relay.attrs.CompilerAttrs][237]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %310 = annotation.compiler_end(%287, meta[relay.attrs.CompilerAttrs][239]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %311 = annotation.compiler_begin(%310, meta[relay.attrs.CompilerAttrs][240]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %312 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][241]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %313 = nn.conv2d(%311, %312, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %314 = annotation.compiler_end(%313, meta[relay.attrs.CompilerAttrs][242]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %315 = annotation.compiler_begin(%309, meta[relay.attrs.CompilerAttrs][238]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %316 = annotation.compiler_begin(%314, meta[relay.attrs.CompilerAttrs][243]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %317 = add(%315, %316) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %318 = annotation.compiler_end(%317, meta[relay.attrs.CompilerAttrs][244]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %319 = annotation.compiler_begin(%318, meta[relay.attrs.CompilerAttrs][245]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %320 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][246]) /* ty=Tensor[(512), float32] */;
  %321 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][247]) /* ty=Tensor[(512), float32] */;
  %322 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][248]) /* ty=Tensor[(512), float32] */;
  %323 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][249]) /* ty=Tensor[(512), float32] */;
  %324 = nn.batch_norm(%319, %320, %321, %322, %323, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %325 = annotation.compiler_end(%324, meta[relay.attrs.CompilerAttrs][250]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %326 = annotation.compiler_begin(%325, meta[relay.attrs.CompilerAttrs][251]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %327 = %326.0;
  %328 = annotation.compiler_end(%327, meta[relay.attrs.CompilerAttrs][252]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %329 = annotation.compiler_begin(%328, meta[relay.attrs.CompilerAttrs][253]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %330 = nn.relu(%329) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %331 = annotation.compiler_end(%330, meta[relay.attrs.CompilerAttrs][254]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %332 = annotation.compiler_begin(%331, meta[relay.attrs.CompilerAttrs][255]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %333 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][256]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %334 = nn.conv2d(%332, %333, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %335 = annotation.compiler_end(%334, meta[relay.attrs.CompilerAttrs][257]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %336 = annotation.compiler_begin(%335, meta[relay.attrs.CompilerAttrs][258]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %337 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][259]) /* ty=Tensor[(512), float32] */;
  %338 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][260]) /* ty=Tensor[(512), float32] */;
  %339 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][261]) /* ty=Tensor[(512), float32] */;
  %340 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][262]) /* ty=Tensor[(512), float32] */;
  %341 = nn.batch_norm(%336, %337, %338, %339, %340, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %342 = annotation.compiler_end(%341, meta[relay.attrs.CompilerAttrs][263]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %343 = annotation.compiler_begin(%342, meta[relay.attrs.CompilerAttrs][264]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %344 = %343.0;
  %345 = annotation.compiler_end(%344, meta[relay.attrs.CompilerAttrs][265]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %346 = annotation.compiler_begin(%345, meta[relay.attrs.CompilerAttrs][266]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %347 = nn.relu(%346) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %348 = annotation.compiler_end(%347, meta[relay.attrs.CompilerAttrs][267]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %349 = annotation.compiler_begin(%348, meta[relay.attrs.CompilerAttrs][268]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %350 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][269]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %351 = nn.conv2d(%349, %350, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %352 = annotation.compiler_end(%351, meta[relay.attrs.CompilerAttrs][270]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %353 = annotation.compiler_end(%317, meta[relay.attrs.CompilerAttrs][272]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %354 = annotation.compiler_begin(%352, meta[relay.attrs.CompilerAttrs][271]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %355 = annotation.compiler_begin(%353, meta[relay.attrs.CompilerAttrs][273]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %356 = add(%354, %355) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %357 = annotation.compiler_end(%356, meta[relay.attrs.CompilerAttrs][274]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %358 = annotation.compiler_begin(%357, meta[relay.attrs.CompilerAttrs][275]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %359 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][276]) /* ty=Tensor[(512), float32] */;
  %360 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][277]) /* ty=Tensor[(512), float32] */;
  %361 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][278]) /* ty=Tensor[(512), float32] */;
  %362 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][279]) /* ty=Tensor[(512), float32] */;
  %363 = nn.batch_norm(%358, %359, %360, %361, %362, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %364 = annotation.compiler_end(%363, meta[relay.attrs.CompilerAttrs][280]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %365 = annotation.compiler_begin(%364, meta[relay.attrs.CompilerAttrs][281]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %366 = %365.0;
  %367 = annotation.compiler_end(%366, meta[relay.attrs.CompilerAttrs][282]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %368 = annotation.compiler_begin(%367, meta[relay.attrs.CompilerAttrs][283]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %369 = nn.relu(%368) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %370 = annotation.compiler_end(%369, meta[relay.attrs.CompilerAttrs][284]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %371 = annotation.compiler_begin(%370, meta[relay.attrs.CompilerAttrs][285]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %372 = nn.global_avg_pool2d(%371) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %373 = annotation.compiler_end(%372, meta[relay.attrs.CompilerAttrs][286]) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %374 = annotation.compiler_begin(%373, meta[relay.attrs.CompilerAttrs][287]) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %375 = nn.batch_flatten(%374) /* ty=Tensor[(1, 512), float32] */;
  %376 = annotation.compiler_end(%375, meta[relay.attrs.CompilerAttrs][288]) /* ty=Tensor[(1, 512), float32] */;
  %377 = annotation.compiler_begin(%376, meta[relay.attrs.CompilerAttrs][289]) /* ty=Tensor[(1, 512), float32] */;
  %378 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][290]) /* ty=Tensor[(1000, 512), float32] */;
  %379 = nn.dense(%377, %378, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %380 = annotation.compiler_end(%379, meta[relay.attrs.CompilerAttrs][291]) /* ty=Tensor[(1, 1000), float32] */;
  %381 = annotation.compiler_begin(%380, meta[relay.attrs.CompilerAttrs][292]) /* ty=Tensor[(1, 1000), float32] */;
  %382 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][293]) /* ty=Tensor[(1000), float32] */;
  %383 = nn.bias_add(%381, %382, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  %384 = annotation.compiler_end(%383, meta[relay.attrs.CompilerAttrs][294]) /* ty=Tensor[(1, 1000), float32] */;
  %385 = annotation.compiler_begin(%384, meta[relay.attrs.CompilerAttrs][295]) /* ty=Tensor[(1, 1000), float32] */;
  %386 = nn.softmax(%385) /* ty=Tensor[(1, 1000), float32] */;
  annotation.compiler_end(%386, meta[relay.attrs.CompilerAttrs][296]) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: MergeCompilerRegionsopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = annotation.compiler_end(%5, meta[relay.attrs.CompilerAttrs][5]) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %7 = annotation.compiler_begin(%6, meta[relay.attrs.CompilerAttrs][6]) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %8 = %7.0;
  %9 = annotation.compiler_end(%8, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %10 = annotation.compiler_begin(%9, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %11 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %12 = nn.conv2d(%10, %11, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %13 = annotation.compiler_end(%12, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %14 = annotation.compiler_begin(%13, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %15 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(64), float32] */;
  %16 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(64), float32] */;
  %17 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %18 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %19 = nn.batch_norm(%14, %15, %16, %17, %18, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = annotation.compiler_end(%19, meta[relay.attrs.CompilerAttrs][16]) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %21 = annotation.compiler_begin(%20, meta[relay.attrs.CompilerAttrs][17]) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %22 = %21.0;
  %23 = annotation.compiler_end(%22, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %24 = annotation.compiler_begin(%23, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %26 = annotation.compiler_end(%25, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %27 = annotation.compiler_begin(%26, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %28 = nn.max_pool2d(%27, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %29 = annotation.compiler_end(%28, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %30 = annotation.compiler_begin(%29, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %34 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %35 = nn.batch_norm(%30, %31, %32, %33, %34, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %36 = annotation.compiler_end(%35, meta[relay.attrs.CompilerAttrs][28]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %37 = annotation.compiler_begin(%36, meta[relay.attrs.CompilerAttrs][29]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %38 = %37.0;
  %39 = annotation.compiler_end(%38, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = annotation.compiler_begin(%39, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %41 = nn.relu(%40) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %42 = annotation.compiler_end(%41, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %43 = annotation.compiler_begin(%42, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %45 = nn.conv2d(%43, %44, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %46 = annotation.compiler_end(%45, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %47 = annotation.compiler_begin(%46, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %48 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %50 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(64), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(64), float32] */;
  %52 = nn.batch_norm(%47, %48, %49, %50, %51, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %53 = annotation.compiler_end(%52, meta[relay.attrs.CompilerAttrs][41]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %54 = annotation.compiler_begin(%53, meta[relay.attrs.CompilerAttrs][42]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %55 = %54.0;
  %56 = annotation.compiler_end(%55, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %57 = annotation.compiler_begin(%56, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = nn.relu(%57) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %59 = annotation.compiler_end(%58, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %60 = annotation.compiler_begin(%59, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %61 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %62 = nn.conv2d(%60, %61, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %63 = annotation.compiler_end(%62, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %64 = annotation.compiler_end(%41, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %65 = annotation.compiler_begin(%64, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %66 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %67 = nn.conv2d(%65, %66, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %68 = annotation.compiler_end(%67, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %69 = annotation.compiler_begin(%63, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %70 = annotation.compiler_begin(%68, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %71 = add(%69, %70) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %72 = annotation.compiler_end(%71, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %73 = annotation.compiler_begin(%72, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %74 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(64), float32] */;
  %75 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(64), float32] */;
  %76 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(64), float32] */;
  %77 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(64), float32] */;
  %78 = nn.batch_norm(%73, %74, %75, %76, %77, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %79 = annotation.compiler_end(%78, meta[relay.attrs.CompilerAttrs][61]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %80 = annotation.compiler_begin(%79, meta[relay.attrs.CompilerAttrs][62]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %81 = %80.0;
  %82 = annotation.compiler_end(%81, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %83 = annotation.compiler_begin(%82, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %84 = nn.relu(%83) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %85 = annotation.compiler_end(%84, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %86 = annotation.compiler_begin(%85, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %87 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %89 = annotation.compiler_end(%88, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %90 = annotation.compiler_begin(%89, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %91 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(64), float32] */;
  %92 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(64), float32] */;
  %93 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(64), float32] */;
  %94 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(64), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %96 = annotation.compiler_end(%95, meta[relay.attrs.CompilerAttrs][74]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %97 = annotation.compiler_begin(%96, meta[relay.attrs.CompilerAttrs][75]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %98 = %97.0;
  %99 = annotation.compiler_end(%98, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %100 = annotation.compiler_begin(%99, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %101 = nn.relu(%100) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %102 = annotation.compiler_end(%101, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %103 = annotation.compiler_begin(%102, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %104 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %105 = nn.conv2d(%103, %104, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %106 = annotation.compiler_end(%105, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %107 = annotation.compiler_end(%71, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %108 = annotation.compiler_begin(%106, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %109 = annotation.compiler_begin(%107, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %110 = add(%108, %109) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %111 = annotation.compiler_end(%110, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %112 = annotation.compiler_begin(%111, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %113 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(64), float32] */;
  %114 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(64), float32] */;
  %115 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(64), float32] */;
  %116 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(64), float32] */;
  %117 = nn.batch_norm(%112, %113, %114, %115, %116, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %118 = annotation.compiler_end(%117, meta[relay.attrs.CompilerAttrs][91]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %119 = annotation.compiler_begin(%118, meta[relay.attrs.CompilerAttrs][92]) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %120 = %119.0;
  %121 = annotation.compiler_end(%120, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %122 = annotation.compiler_begin(%121, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %123 = nn.relu(%122) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %124 = annotation.compiler_end(%123, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %125 = annotation.compiler_begin(%124, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %126 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %127 = nn.conv2d(%125, %126, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %128 = annotation.compiler_end(%127, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %129 = annotation.compiler_begin(%128, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %130 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(128), float32] */;
  %131 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(128), float32] */;
  %132 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(128), float32] */;
  %133 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(128), float32] */;
  %134 = nn.batch_norm(%129, %130, %131, %132, %133, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %135 = annotation.compiler_end(%134, meta[relay.attrs.CompilerAttrs][104]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %136 = annotation.compiler_begin(%135, meta[relay.attrs.CompilerAttrs][105]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %137 = %136.0;
  %138 = annotation.compiler_end(%137, meta[relay.attrs.CompilerAttrs][106]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %139 = annotation.compiler_begin(%138, meta[relay.attrs.CompilerAttrs][107]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %140 = nn.relu(%139) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %141 = annotation.compiler_end(%140, meta[relay.attrs.CompilerAttrs][108]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %142 = annotation.compiler_begin(%141, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %143 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][110]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %144 = nn.conv2d(%142, %143, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %145 = annotation.compiler_end(%144, meta[relay.attrs.CompilerAttrs][111]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %146 = annotation.compiler_end(%123, meta[relay.attrs.CompilerAttrs][113]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %147 = annotation.compiler_begin(%146, meta[relay.attrs.CompilerAttrs][114]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %148 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][115]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %150 = annotation.compiler_end(%149, meta[relay.attrs.CompilerAttrs][116]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %151 = annotation.compiler_begin(%145, meta[relay.attrs.CompilerAttrs][112]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %152 = annotation.compiler_begin(%150, meta[relay.attrs.CompilerAttrs][117]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %153 = add(%151, %152) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %154 = annotation.compiler_end(%153, meta[relay.attrs.CompilerAttrs][118]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %155 = annotation.compiler_begin(%154, meta[relay.attrs.CompilerAttrs][119]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %156 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][120]) /* ty=Tensor[(128), float32] */;
  %157 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][121]) /* ty=Tensor[(128), float32] */;
  %158 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][122]) /* ty=Tensor[(128), float32] */;
  %159 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][123]) /* ty=Tensor[(128), float32] */;
  %160 = nn.batch_norm(%155, %156, %157, %158, %159, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %161 = annotation.compiler_end(%160, meta[relay.attrs.CompilerAttrs][124]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %162 = annotation.compiler_begin(%161, meta[relay.attrs.CompilerAttrs][125]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %163 = %162.0;
  %164 = annotation.compiler_end(%163, meta[relay.attrs.CompilerAttrs][126]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %165 = annotation.compiler_begin(%164, meta[relay.attrs.CompilerAttrs][127]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %166 = nn.relu(%165) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %167 = annotation.compiler_end(%166, meta[relay.attrs.CompilerAttrs][128]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %168 = annotation.compiler_begin(%167, meta[relay.attrs.CompilerAttrs][129]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %169 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][130]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %171 = annotation.compiler_end(%170, meta[relay.attrs.CompilerAttrs][131]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %172 = annotation.compiler_begin(%171, meta[relay.attrs.CompilerAttrs][132]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %173 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][133]) /* ty=Tensor[(128), float32] */;
  %174 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][134]) /* ty=Tensor[(128), float32] */;
  %175 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][135]) /* ty=Tensor[(128), float32] */;
  %176 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][136]) /* ty=Tensor[(128), float32] */;
  %177 = nn.batch_norm(%172, %173, %174, %175, %176, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %178 = annotation.compiler_end(%177, meta[relay.attrs.CompilerAttrs][137]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %179 = annotation.compiler_begin(%178, meta[relay.attrs.CompilerAttrs][138]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %180 = %179.0;
  %181 = annotation.compiler_end(%180, meta[relay.attrs.CompilerAttrs][139]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %182 = annotation.compiler_begin(%181, meta[relay.attrs.CompilerAttrs][140]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %183 = nn.relu(%182) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %184 = annotation.compiler_end(%183, meta[relay.attrs.CompilerAttrs][141]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %185 = annotation.compiler_begin(%184, meta[relay.attrs.CompilerAttrs][142]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %186 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][143]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %187 = nn.conv2d(%185, %186, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][144]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %189 = annotation.compiler_end(%153, meta[relay.attrs.CompilerAttrs][146]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %190 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][145]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %191 = annotation.compiler_begin(%189, meta[relay.attrs.CompilerAttrs][147]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %192 = add(%190, %191) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %193 = annotation.compiler_end(%192, meta[relay.attrs.CompilerAttrs][148]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %194 = annotation.compiler_begin(%193, meta[relay.attrs.CompilerAttrs][149]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %195 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][150]) /* ty=Tensor[(128), float32] */;
  %196 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][151]) /* ty=Tensor[(128), float32] */;
  %197 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][152]) /* ty=Tensor[(128), float32] */;
  %198 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][153]) /* ty=Tensor[(128), float32] */;
  %199 = nn.batch_norm(%194, %195, %196, %197, %198, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %200 = annotation.compiler_end(%199, meta[relay.attrs.CompilerAttrs][154]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %201 = annotation.compiler_begin(%200, meta[relay.attrs.CompilerAttrs][155]) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %202 = %201.0;
  %203 = annotation.compiler_end(%202, meta[relay.attrs.CompilerAttrs][156]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %204 = annotation.compiler_begin(%203, meta[relay.attrs.CompilerAttrs][157]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %205 = nn.relu(%204) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %206 = annotation.compiler_end(%205, meta[relay.attrs.CompilerAttrs][158]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %207 = annotation.compiler_begin(%206, meta[relay.attrs.CompilerAttrs][159]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %208 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][160]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %209 = nn.conv2d(%207, %208, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %210 = annotation.compiler_end(%209, meta[relay.attrs.CompilerAttrs][161]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %211 = annotation.compiler_begin(%210, meta[relay.attrs.CompilerAttrs][162]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %212 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][163]) /* ty=Tensor[(256), float32] */;
  %213 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][164]) /* ty=Tensor[(256), float32] */;
  %214 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][165]) /* ty=Tensor[(256), float32] */;
  %215 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][166]) /* ty=Tensor[(256), float32] */;
  %216 = nn.batch_norm(%211, %212, %213, %214, %215, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %217 = annotation.compiler_end(%216, meta[relay.attrs.CompilerAttrs][167]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %218 = annotation.compiler_begin(%217, meta[relay.attrs.CompilerAttrs][168]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %219 = %218.0;
  %220 = annotation.compiler_end(%219, meta[relay.attrs.CompilerAttrs][169]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %221 = annotation.compiler_begin(%220, meta[relay.attrs.CompilerAttrs][170]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %222 = nn.relu(%221) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %223 = annotation.compiler_end(%222, meta[relay.attrs.CompilerAttrs][171]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %224 = annotation.compiler_begin(%223, meta[relay.attrs.CompilerAttrs][172]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %225 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][173]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %226 = nn.conv2d(%224, %225, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %227 = annotation.compiler_end(%226, meta[relay.attrs.CompilerAttrs][174]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %228 = annotation.compiler_end(%205, meta[relay.attrs.CompilerAttrs][176]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %229 = annotation.compiler_begin(%228, meta[relay.attrs.CompilerAttrs][177]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %230 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][178]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %231 = nn.conv2d(%229, %230, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %232 = annotation.compiler_end(%231, meta[relay.attrs.CompilerAttrs][179]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %233 = annotation.compiler_begin(%227, meta[relay.attrs.CompilerAttrs][175]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %234 = annotation.compiler_begin(%232, meta[relay.attrs.CompilerAttrs][180]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %235 = add(%233, %234) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %236 = annotation.compiler_end(%235, meta[relay.attrs.CompilerAttrs][181]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %237 = annotation.compiler_begin(%236, meta[relay.attrs.CompilerAttrs][182]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %238 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][183]) /* ty=Tensor[(256), float32] */;
  %239 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][184]) /* ty=Tensor[(256), float32] */;
  %240 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][185]) /* ty=Tensor[(256), float32] */;
  %241 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][186]) /* ty=Tensor[(256), float32] */;
  %242 = nn.batch_norm(%237, %238, %239, %240, %241, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %243 = annotation.compiler_end(%242, meta[relay.attrs.CompilerAttrs][187]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %244 = annotation.compiler_begin(%243, meta[relay.attrs.CompilerAttrs][188]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %245 = %244.0;
  %246 = annotation.compiler_end(%245, meta[relay.attrs.CompilerAttrs][189]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %247 = annotation.compiler_begin(%246, meta[relay.attrs.CompilerAttrs][190]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %248 = nn.relu(%247) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %249 = annotation.compiler_end(%248, meta[relay.attrs.CompilerAttrs][191]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %250 = annotation.compiler_begin(%249, meta[relay.attrs.CompilerAttrs][192]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %251 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][193]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %252 = nn.conv2d(%250, %251, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %253 = annotation.compiler_end(%252, meta[relay.attrs.CompilerAttrs][194]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %254 = annotation.compiler_begin(%253, meta[relay.attrs.CompilerAttrs][195]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %255 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][196]) /* ty=Tensor[(256), float32] */;
  %256 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][197]) /* ty=Tensor[(256), float32] */;
  %257 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][198]) /* ty=Tensor[(256), float32] */;
  %258 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][199]) /* ty=Tensor[(256), float32] */;
  %259 = nn.batch_norm(%254, %255, %256, %257, %258, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %260 = annotation.compiler_end(%259, meta[relay.attrs.CompilerAttrs][200]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %261 = annotation.compiler_begin(%260, meta[relay.attrs.CompilerAttrs][201]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %262 = %261.0;
  %263 = annotation.compiler_end(%262, meta[relay.attrs.CompilerAttrs][202]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %264 = annotation.compiler_begin(%263, meta[relay.attrs.CompilerAttrs][203]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %265 = nn.relu(%264) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %266 = annotation.compiler_end(%265, meta[relay.attrs.CompilerAttrs][204]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %267 = annotation.compiler_begin(%266, meta[relay.attrs.CompilerAttrs][205]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %268 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][206]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %269 = nn.conv2d(%267, %268, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %270 = annotation.compiler_end(%269, meta[relay.attrs.CompilerAttrs][207]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %271 = annotation.compiler_end(%235, meta[relay.attrs.CompilerAttrs][209]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %272 = annotation.compiler_begin(%270, meta[relay.attrs.CompilerAttrs][208]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %273 = annotation.compiler_begin(%271, meta[relay.attrs.CompilerAttrs][210]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %274 = add(%272, %273) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %275 = annotation.compiler_end(%274, meta[relay.attrs.CompilerAttrs][211]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %276 = annotation.compiler_begin(%275, meta[relay.attrs.CompilerAttrs][212]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %277 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][213]) /* ty=Tensor[(256), float32] */;
  %278 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][214]) /* ty=Tensor[(256), float32] */;
  %279 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][215]) /* ty=Tensor[(256), float32] */;
  %280 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][216]) /* ty=Tensor[(256), float32] */;
  %281 = nn.batch_norm(%276, %277, %278, %279, %280, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %282 = annotation.compiler_end(%281, meta[relay.attrs.CompilerAttrs][217]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %283 = annotation.compiler_begin(%282, meta[relay.attrs.CompilerAttrs][218]) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %284 = %283.0;
  %285 = annotation.compiler_end(%284, meta[relay.attrs.CompilerAttrs][219]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %286 = annotation.compiler_begin(%285, meta[relay.attrs.CompilerAttrs][220]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %287 = nn.relu(%286) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %288 = annotation.compiler_end(%287, meta[relay.attrs.CompilerAttrs][221]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %289 = annotation.compiler_begin(%288, meta[relay.attrs.CompilerAttrs][222]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %290 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][223]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %291 = nn.conv2d(%289, %290, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %292 = annotation.compiler_end(%291, meta[relay.attrs.CompilerAttrs][224]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %293 = annotation.compiler_begin(%292, meta[relay.attrs.CompilerAttrs][225]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %294 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][226]) /* ty=Tensor[(512), float32] */;
  %295 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][227]) /* ty=Tensor[(512), float32] */;
  %296 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][228]) /* ty=Tensor[(512), float32] */;
  %297 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][229]) /* ty=Tensor[(512), float32] */;
  %298 = nn.batch_norm(%293, %294, %295, %296, %297, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %299 = annotation.compiler_end(%298, meta[relay.attrs.CompilerAttrs][230]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %300 = annotation.compiler_begin(%299, meta[relay.attrs.CompilerAttrs][231]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %301 = %300.0;
  %302 = annotation.compiler_end(%301, meta[relay.attrs.CompilerAttrs][232]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %303 = annotation.compiler_begin(%302, meta[relay.attrs.CompilerAttrs][233]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %304 = nn.relu(%303) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %305 = annotation.compiler_end(%304, meta[relay.attrs.CompilerAttrs][234]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %306 = annotation.compiler_begin(%305, meta[relay.attrs.CompilerAttrs][235]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %307 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][236]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %308 = nn.conv2d(%306, %307, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %309 = annotation.compiler_end(%308, meta[relay.attrs.CompilerAttrs][237]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %310 = annotation.compiler_end(%287, meta[relay.attrs.CompilerAttrs][239]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %311 = annotation.compiler_begin(%310, meta[relay.attrs.CompilerAttrs][240]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %312 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][241]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %313 = nn.conv2d(%311, %312, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %314 = annotation.compiler_end(%313, meta[relay.attrs.CompilerAttrs][242]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %315 = annotation.compiler_begin(%309, meta[relay.attrs.CompilerAttrs][238]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %316 = annotation.compiler_begin(%314, meta[relay.attrs.CompilerAttrs][243]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %317 = add(%315, %316) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %318 = annotation.compiler_end(%317, meta[relay.attrs.CompilerAttrs][244]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %319 = annotation.compiler_begin(%318, meta[relay.attrs.CompilerAttrs][245]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %320 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][246]) /* ty=Tensor[(512), float32] */;
  %321 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][247]) /* ty=Tensor[(512), float32] */;
  %322 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][248]) /* ty=Tensor[(512), float32] */;
  %323 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][249]) /* ty=Tensor[(512), float32] */;
  %324 = nn.batch_norm(%319, %320, %321, %322, %323, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %325 = annotation.compiler_end(%324, meta[relay.attrs.CompilerAttrs][250]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %326 = annotation.compiler_begin(%325, meta[relay.attrs.CompilerAttrs][251]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %327 = %326.0;
  %328 = annotation.compiler_end(%327, meta[relay.attrs.CompilerAttrs][252]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %329 = annotation.compiler_begin(%328, meta[relay.attrs.CompilerAttrs][253]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %330 = nn.relu(%329) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %331 = annotation.compiler_end(%330, meta[relay.attrs.CompilerAttrs][254]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %332 = annotation.compiler_begin(%331, meta[relay.attrs.CompilerAttrs][255]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %333 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][256]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %334 = nn.conv2d(%332, %333, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %335 = annotation.compiler_end(%334, meta[relay.attrs.CompilerAttrs][257]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %336 = annotation.compiler_begin(%335, meta[relay.attrs.CompilerAttrs][258]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %337 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][259]) /* ty=Tensor[(512), float32] */;
  %338 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][260]) /* ty=Tensor[(512), float32] */;
  %339 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][261]) /* ty=Tensor[(512), float32] */;
  %340 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][262]) /* ty=Tensor[(512), float32] */;
  %341 = nn.batch_norm(%336, %337, %338, %339, %340, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %342 = annotation.compiler_end(%341, meta[relay.attrs.CompilerAttrs][263]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %343 = annotation.compiler_begin(%342, meta[relay.attrs.CompilerAttrs][264]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %344 = %343.0;
  %345 = annotation.compiler_end(%344, meta[relay.attrs.CompilerAttrs][265]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %346 = annotation.compiler_begin(%345, meta[relay.attrs.CompilerAttrs][266]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %347 = nn.relu(%346) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %348 = annotation.compiler_end(%347, meta[relay.attrs.CompilerAttrs][267]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %349 = annotation.compiler_begin(%348, meta[relay.attrs.CompilerAttrs][268]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %350 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][269]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %351 = nn.conv2d(%349, %350, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %352 = annotation.compiler_end(%351, meta[relay.attrs.CompilerAttrs][270]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %353 = annotation.compiler_end(%317, meta[relay.attrs.CompilerAttrs][272]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %354 = annotation.compiler_begin(%352, meta[relay.attrs.CompilerAttrs][271]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %355 = annotation.compiler_begin(%353, meta[relay.attrs.CompilerAttrs][273]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %356 = add(%354, %355) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %357 = annotation.compiler_end(%356, meta[relay.attrs.CompilerAttrs][274]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %358 = annotation.compiler_begin(%357, meta[relay.attrs.CompilerAttrs][275]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %359 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][276]) /* ty=Tensor[(512), float32] */;
  %360 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][277]) /* ty=Tensor[(512), float32] */;
  %361 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][278]) /* ty=Tensor[(512), float32] */;
  %362 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][279]) /* ty=Tensor[(512), float32] */;
  %363 = nn.batch_norm(%358, %359, %360, %361, %362, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %364 = annotation.compiler_end(%363, meta[relay.attrs.CompilerAttrs][280]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %365 = annotation.compiler_begin(%364, meta[relay.attrs.CompilerAttrs][281]) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %366 = %365.0;
  %367 = annotation.compiler_end(%366, meta[relay.attrs.CompilerAttrs][282]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %368 = annotation.compiler_begin(%367, meta[relay.attrs.CompilerAttrs][283]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %369 = nn.relu(%368) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %370 = annotation.compiler_end(%369, meta[relay.attrs.CompilerAttrs][284]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %371 = annotation.compiler_begin(%370, meta[relay.attrs.CompilerAttrs][285]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %372 = nn.global_avg_pool2d(%371) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %373 = annotation.compiler_end(%372, meta[relay.attrs.CompilerAttrs][286]) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %374 = annotation.compiler_begin(%373, meta[relay.attrs.CompilerAttrs][287]) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %375 = nn.batch_flatten(%374) /* ty=Tensor[(1, 512), float32] */;
  %376 = annotation.compiler_end(%375, meta[relay.attrs.CompilerAttrs][288]) /* ty=Tensor[(1, 512), float32] */;
  %377 = annotation.compiler_begin(%376, meta[relay.attrs.CompilerAttrs][289]) /* ty=Tensor[(1, 512), float32] */;
  %378 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][290]) /* ty=Tensor[(1000, 512), float32] */;
  %379 = nn.dense(%377, %378, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %380 = annotation.compiler_end(%379, meta[relay.attrs.CompilerAttrs][291]) /* ty=Tensor[(1, 1000), float32] */;
  %381 = annotation.compiler_begin(%380, meta[relay.attrs.CompilerAttrs][292]) /* ty=Tensor[(1, 1000), float32] */;
  %382 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][293]) /* ty=Tensor[(1000), float32] */;
  %383 = nn.bias_add(%381, %382, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  %384 = annotation.compiler_end(%383, meta[relay.attrs.CompilerAttrs][294]) /* ty=Tensor[(1, 1000), float32] */;
  %385 = annotation.compiler_begin(%384, meta[relay.attrs.CompilerAttrs][295]) /* ty=Tensor[(1, 1000), float32] */;
  %386 = nn.softmax(%385) /* ty=Tensor[(1, 1000), float32] */;
  annotation.compiler_end(%386, meta[relay.attrs.CompilerAttrs][296]) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]);
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f);
  %14 = %13.0;
  %15 = nn.relu(%14);
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]);
  %17 = annotation.compiler_begin(%16, meta[relay.attrs.CompilerAttrs][11]);
  %18 = nn.max_pool2d(%17, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]);
  %19 = annotation.compiler_end(%18, meta[relay.attrs.CompilerAttrs][12]);
  %20 = annotation.compiler_begin(%19, meta[relay.attrs.CompilerAttrs][13]);
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64), float32] */;
  %24 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %25 = nn.batch_norm(%20, %21, %22, %23, %24, epsilon=2e-05f);
  %26 = %25.0;
  %27 = nn.relu(%26);
  %28 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %29 = nn.conv2d(%27, %28, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64), float32] */;
  %34 = nn.batch_norm(%29, %30, %31, %32, %33, epsilon=2e-05f);
  %35 = %34.0;
  %36 = nn.relu(%35);
  %37 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %38 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %39 = nn.conv2d(%36, %37, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %40 = nn.conv2d(%27, %38, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]);
  %41 = add(%39, %40);
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %45 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %46 = nn.batch_norm(%41, %42, %43, %44, %45, epsilon=2e-05f);
  %47 = %46.0;
  %48 = nn.relu(%47);
  %49 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %50 = nn.conv2d(%48, %49, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64), float32] */;
  %54 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %55 = nn.batch_norm(%50, %51, %52, %53, %54, epsilon=2e-05f);
  %56 = %55.0;
  %57 = nn.relu(%56);
  %58 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %59 = nn.conv2d(%57, %58, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %60 = add(%59, %41);
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %64 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %65 = nn.batch_norm(%60, %61, %62, %63, %64, epsilon=2e-05f);
  %66 = %65.0;
  %67 = nn.relu(%66);
  %68 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %69 = nn.conv2d(%67, %68, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128), float32] */;
  %73 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128), float32] */;
  %74 = nn.batch_norm(%69, %70, %71, %72, %73, epsilon=2e-05f);
  %75 = %74.0;
  %76 = nn.relu(%75);
  %77 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %78 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %79 = nn.conv2d(%76, %77, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %80 = nn.conv2d(%67, %78, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]);
  %81 = add(%79, %80);
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128), float32] */;
  %85 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %86 = nn.batch_norm(%81, %82, %83, %84, %85, epsilon=2e-05f);
  %87 = %86.0;
  %88 = nn.relu(%87);
  %89 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %90 = nn.conv2d(%88, %89, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128), float32] */;
  %94 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f);
  %96 = %95.0;
  %97 = nn.relu(%96);
  %98 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %99 = nn.conv2d(%97, %98, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %100 = add(%99, %81);
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(128), float32] */;
  %104 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(128), float32] */;
  %105 = nn.batch_norm(%100, %101, %102, %103, %104, epsilon=2e-05f);
  %106 = %105.0;
  %107 = nn.relu(%106);
  %108 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %109 = nn.conv2d(%107, %108, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256), float32] */;
  %113 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256), float32] */;
  %114 = nn.batch_norm(%109, %110, %111, %112, %113, epsilon=2e-05f);
  %115 = %114.0;
  %116 = nn.relu(%115);
  %117 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %118 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %119 = nn.conv2d(%116, %117, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %120 = nn.conv2d(%107, %118, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]);
  %121 = add(%119, %120);
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256), float32] */;
  %125 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %126 = nn.batch_norm(%121, %122, %123, %124, %125, epsilon=2e-05f);
  %127 = %126.0;
  %128 = nn.relu(%127);
  %129 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %130 = nn.conv2d(%128, %129, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256), float32] */;
  %134 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %135 = nn.batch_norm(%130, %131, %132, %133, %134, epsilon=2e-05f);
  %136 = %135.0;
  %137 = nn.relu(%136);
  %138 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %139 = nn.conv2d(%137, %138, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %140 = add(%139, %121);
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(256), float32] */;
  %144 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(256), float32] */;
  %145 = nn.batch_norm(%140, %141, %142, %143, %144, epsilon=2e-05f);
  %146 = %145.0;
  %147 = nn.relu(%146);
  %148 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512), float32] */;
  %153 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512), float32] */;
  %154 = nn.batch_norm(%149, %150, %151, %152, %153, epsilon=2e-05f);
  %155 = %154.0;
  %156 = nn.relu(%155);
  %157 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %158 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %159 = nn.conv2d(%156, %157, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %160 = nn.conv2d(%147, %158, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]);
  %161 = add(%159, %160);
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512), float32] */;
  %165 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %166 = nn.batch_norm(%161, %162, %163, %164, %165, epsilon=2e-05f);
  %167 = %166.0;
  %168 = nn.relu(%167);
  %169 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512), float32] */;
  %174 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %175 = nn.batch_norm(%170, %171, %172, %173, %174, epsilon=2e-05f);
  %176 = %175.0;
  %177 = nn.relu(%176);
  %178 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %179 = nn.conv2d(%177, %178, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %180 = add(%179, %161);
  %181 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(512), float32] */;
  %184 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(512), float32] */;
  %185 = nn.batch_norm(%180, %181, %182, %183, %184, epsilon=2e-05f);
  %186 = %185.0;
  %187 = nn.relu(%186);
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][102]);
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][103]);
  %190 = nn.global_avg_pool2d(%189);
  %191 = nn.batch_flatten(%190);
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][104]);
  %193 = annotation.compiler_begin(%192, meta[relay.attrs.CompilerAttrs][105]);
  %194 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][106]) /* ty=Tensor[(1000, 512), float32] */;
  %195 = nn.dense(%193, %194, units=1000);
  %196 = annotation.compiler_end(%195, meta[relay.attrs.CompilerAttrs][107]);
  %197 = annotation.compiler_begin(%196, meta[relay.attrs.CompilerAttrs][108]);
  %198 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1000), float32] */;
  %199 = nn.bias_add(%197, %198, axis=-1);
  %200 = nn.softmax(%199);
  annotation.compiler_end(%200, meta[relay.attrs.CompilerAttrs][110])
}


Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = annotation.compiler_begin(%16, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %18 = nn.max_pool2d(%17, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = annotation.compiler_end(%18, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = annotation.compiler_begin(%19, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64), float32] */;
  %24 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %25 = nn.batch_norm(%20, %21, %22, %23, %24, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %26 = %25.0;
  %27 = nn.relu(%26) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %28 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %29 = nn.conv2d(%27, %28, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64), float32] */;
  %34 = nn.batch_norm(%29, %30, %31, %32, %33, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %35 = %34.0;
  %36 = nn.relu(%35) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %37 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %38 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %39 = nn.conv2d(%36, %37, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = nn.conv2d(%27, %38, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %41 = add(%39, %40) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %45 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %46 = nn.batch_norm(%41, %42, %43, %44, %45, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %47 = %46.0;
  %48 = nn.relu(%47) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %50 = nn.conv2d(%48, %49, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64), float32] */;
  %54 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %55 = nn.batch_norm(%50, %51, %52, %53, %54, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %59 = nn.conv2d(%57, %58, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %60 = add(%59, %41) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %64 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %65 = nn.batch_norm(%60, %61, %62, %63, %64, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %66 = %65.0;
  %67 = nn.relu(%66) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %68 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %69 = nn.conv2d(%67, %68, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128), float32] */;
  %73 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128), float32] */;
  %74 = nn.batch_norm(%69, %70, %71, %72, %73, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %77 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %78 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %79 = nn.conv2d(%76, %77, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %80 = nn.conv2d(%67, %78, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %81 = add(%79, %80) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128), float32] */;
  %85 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %86 = nn.batch_norm(%81, %82, %83, %84, %85, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %87 = %86.0;
  %88 = nn.relu(%87) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %89 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %90 = nn.conv2d(%88, %89, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128), float32] */;
  %94 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %96 = %95.0;
  %97 = nn.relu(%96) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %98 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %99 = nn.conv2d(%97, %98, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %100 = add(%99, %81) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(128), float32] */;
  %104 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(128), float32] */;
  %105 = nn.batch_norm(%100, %101, %102, %103, %104, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %106 = %105.0;
  %107 = nn.relu(%106) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %108 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %109 = nn.conv2d(%107, %108, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256), float32] */;
  %113 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256), float32] */;
  %114 = nn.batch_norm(%109, %110, %111, %112, %113, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %115 = %114.0;
  %116 = nn.relu(%115) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %117 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %118 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %119 = nn.conv2d(%116, %117, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %120 = nn.conv2d(%107, %118, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %121 = add(%119, %120) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256), float32] */;
  %125 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %126 = nn.batch_norm(%121, %122, %123, %124, %125, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %127 = %126.0;
  %128 = nn.relu(%127) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %129 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %130 = nn.conv2d(%128, %129, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256), float32] */;
  %134 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %135 = nn.batch_norm(%130, %131, %132, %133, %134, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %136 = %135.0;
  %137 = nn.relu(%136) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %138 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %139 = nn.conv2d(%137, %138, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %140 = add(%139, %121) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(256), float32] */;
  %144 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(256), float32] */;
  %145 = nn.batch_norm(%140, %141, %142, %143, %144, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %146 = %145.0;
  %147 = nn.relu(%146) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %148 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512), float32] */;
  %153 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512), float32] */;
  %154 = nn.batch_norm(%149, %150, %151, %152, %153, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %155 = %154.0;
  %156 = nn.relu(%155) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %157 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %158 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %159 = nn.conv2d(%156, %157, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %160 = nn.conv2d(%147, %158, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %161 = add(%159, %160) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512), float32] */;
  %165 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %166 = nn.batch_norm(%161, %162, %163, %164, %165, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %167 = %166.0;
  %168 = nn.relu(%167) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %169 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512), float32] */;
  %174 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %175 = nn.batch_norm(%170, %171, %172, %173, %174, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %176 = %175.0;
  %177 = nn.relu(%176) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %178 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %179 = nn.conv2d(%177, %178, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %180 = add(%179, %161) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %181 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(512), float32] */;
  %184 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(512), float32] */;
  %185 = nn.batch_norm(%180, %181, %182, %183, %184, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %186 = %185.0;
  %187 = nn.relu(%186) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %190 = nn.global_avg_pool2d(%189) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %191 = nn.batch_flatten(%190) /* ty=Tensor[(1, 512), float32] */;
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][104]) /* ty=Tensor[(1, 512), float32] */;
  %193 = annotation.compiler_begin(%192, meta[relay.attrs.CompilerAttrs][105]) /* ty=Tensor[(1, 512), float32] */;
  %194 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][106]) /* ty=Tensor[(1000, 512), float32] */;
  %195 = nn.dense(%193, %194, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %196 = annotation.compiler_end(%195, meta[relay.attrs.CompilerAttrs][107]) /* ty=Tensor[(1, 1000), float32] */;
  %197 = annotation.compiler_begin(%196, meta[relay.attrs.CompilerAttrs][108]) /* ty=Tensor[(1, 1000), float32] */;
  %198 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1000), float32] */;
  %199 = nn.bias_add(%197, %198, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  %200 = nn.softmax(%199) /* ty=Tensor[(1, 1000), float32] */;
  annotation.compiler_end(%200, meta[relay.attrs.CompilerAttrs][110]) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: sequentialopt_level: 2required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = annotation.compiler_begin(%16, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %18 = nn.max_pool2d(%17, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = annotation.compiler_end(%18, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = annotation.compiler_begin(%19, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64), float32] */;
  %24 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %25 = nn.batch_norm(%20, %21, %22, %23, %24, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %26 = %25.0;
  %27 = nn.relu(%26) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %28 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %29 = nn.conv2d(%27, %28, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64), float32] */;
  %34 = nn.batch_norm(%29, %30, %31, %32, %33, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %35 = %34.0;
  %36 = nn.relu(%35) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %37 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %38 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %39 = nn.conv2d(%36, %37, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = nn.conv2d(%27, %38, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %41 = add(%39, %40) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %45 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %46 = nn.batch_norm(%41, %42, %43, %44, %45, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %47 = %46.0;
  %48 = nn.relu(%47) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %50 = nn.conv2d(%48, %49, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64), float32] */;
  %54 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %55 = nn.batch_norm(%50, %51, %52, %53, %54, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %59 = nn.conv2d(%57, %58, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %60 = add(%59, %41) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %64 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %65 = nn.batch_norm(%60, %61, %62, %63, %64, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %66 = %65.0;
  %67 = nn.relu(%66) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %68 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %69 = nn.conv2d(%67, %68, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128), float32] */;
  %73 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128), float32] */;
  %74 = nn.batch_norm(%69, %70, %71, %72, %73, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %77 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %78 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %79 = nn.conv2d(%76, %77, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %80 = nn.conv2d(%67, %78, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %81 = add(%79, %80) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128), float32] */;
  %85 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %86 = nn.batch_norm(%81, %82, %83, %84, %85, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %87 = %86.0;
  %88 = nn.relu(%87) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %89 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %90 = nn.conv2d(%88, %89, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128), float32] */;
  %94 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %96 = %95.0;
  %97 = nn.relu(%96) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %98 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %99 = nn.conv2d(%97, %98, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %100 = add(%99, %81) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(128), float32] */;
  %104 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(128), float32] */;
  %105 = nn.batch_norm(%100, %101, %102, %103, %104, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %106 = %105.0;
  %107 = nn.relu(%106) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %108 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %109 = nn.conv2d(%107, %108, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256), float32] */;
  %113 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256), float32] */;
  %114 = nn.batch_norm(%109, %110, %111, %112, %113, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %115 = %114.0;
  %116 = nn.relu(%115) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %117 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %118 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %119 = nn.conv2d(%116, %117, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %120 = nn.conv2d(%107, %118, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %121 = add(%119, %120) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256), float32] */;
  %125 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %126 = nn.batch_norm(%121, %122, %123, %124, %125, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %127 = %126.0;
  %128 = nn.relu(%127) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %129 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %130 = nn.conv2d(%128, %129, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256), float32] */;
  %134 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %135 = nn.batch_norm(%130, %131, %132, %133, %134, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %136 = %135.0;
  %137 = nn.relu(%136) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %138 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %139 = nn.conv2d(%137, %138, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %140 = add(%139, %121) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(256), float32] */;
  %144 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(256), float32] */;
  %145 = nn.batch_norm(%140, %141, %142, %143, %144, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %146 = %145.0;
  %147 = nn.relu(%146) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %148 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512), float32] */;
  %153 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512), float32] */;
  %154 = nn.batch_norm(%149, %150, %151, %152, %153, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %155 = %154.0;
  %156 = nn.relu(%155) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %157 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %158 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %159 = nn.conv2d(%156, %157, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %160 = nn.conv2d(%147, %158, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %161 = add(%159, %160) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512), float32] */;
  %165 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %166 = nn.batch_norm(%161, %162, %163, %164, %165, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %167 = %166.0;
  %168 = nn.relu(%167) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %169 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512), float32] */;
  %174 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %175 = nn.batch_norm(%170, %171, %172, %173, %174, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %176 = %175.0;
  %177 = nn.relu(%176) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %178 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %179 = nn.conv2d(%177, %178, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %180 = add(%179, %161) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %181 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(512), float32] */;
  %184 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(512), float32] */;
  %185 = nn.batch_norm(%180, %181, %182, %183, %184, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %186 = %185.0;
  %187 = nn.relu(%186) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %190 = nn.global_avg_pool2d(%189) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %191 = nn.batch_flatten(%190) /* ty=Tensor[(1, 512), float32] */;
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][104]) /* ty=Tensor[(1, 512), float32] */;
  %193 = annotation.compiler_begin(%192, meta[relay.attrs.CompilerAttrs][105]) /* ty=Tensor[(1, 512), float32] */;
  %194 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][106]) /* ty=Tensor[(1000, 512), float32] */;
  %195 = nn.dense(%193, %194, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %196 = annotation.compiler_end(%195, meta[relay.attrs.CompilerAttrs][107]) /* ty=Tensor[(1, 1000), float32] */;
  %197 = annotation.compiler_begin(%196, meta[relay.attrs.CompilerAttrs][108]) /* ty=Tensor[(1, 1000), float32] */;
  %198 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1000), float32] */;
  %199 = nn.bias_add(%197, %198, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  %200 = nn.softmax(%199) /* ty=Tensor[(1, 1000), float32] */;
  annotation.compiler_end(%200, meta[relay.attrs.CompilerAttrs][110]) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: FlattenNestedTuplesopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = annotation.compiler_begin(%16, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %18 = nn.max_pool2d(%17, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = annotation.compiler_end(%18, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = annotation.compiler_begin(%19, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64), float32] */;
  %24 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %25 = nn.batch_norm(%20, %21, %22, %23, %24, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %26 = %25.0;
  %27 = nn.relu(%26) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %28 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %29 = nn.conv2d(%27, %28, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64), float32] */;
  %34 = nn.batch_norm(%29, %30, %31, %32, %33, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %35 = %34.0;
  %36 = nn.relu(%35) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %37 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %38 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %39 = nn.conv2d(%36, %37, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = nn.conv2d(%27, %38, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %41 = add(%39, %40) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %45 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %46 = nn.batch_norm(%41, %42, %43, %44, %45, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %47 = %46.0;
  %48 = nn.relu(%47) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %50 = nn.conv2d(%48, %49, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64), float32] */;
  %54 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %55 = nn.batch_norm(%50, %51, %52, %53, %54, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %59 = nn.conv2d(%57, %58, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %60 = add(%59, %41) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %64 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %65 = nn.batch_norm(%60, %61, %62, %63, %64, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %66 = %65.0;
  %67 = nn.relu(%66) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %68 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %69 = nn.conv2d(%67, %68, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128), float32] */;
  %73 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128), float32] */;
  %74 = nn.batch_norm(%69, %70, %71, %72, %73, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %77 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %78 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %79 = nn.conv2d(%76, %77, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %80 = nn.conv2d(%67, %78, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %81 = add(%79, %80) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128), float32] */;
  %85 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %86 = nn.batch_norm(%81, %82, %83, %84, %85, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %87 = %86.0;
  %88 = nn.relu(%87) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %89 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %90 = nn.conv2d(%88, %89, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128), float32] */;
  %94 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %96 = %95.0;
  %97 = nn.relu(%96) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %98 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %99 = nn.conv2d(%97, %98, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %100 = add(%99, %81) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(128), float32] */;
  %104 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(128), float32] */;
  %105 = nn.batch_norm(%100, %101, %102, %103, %104, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %106 = %105.0;
  %107 = nn.relu(%106) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %108 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %109 = nn.conv2d(%107, %108, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256), float32] */;
  %113 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256), float32] */;
  %114 = nn.batch_norm(%109, %110, %111, %112, %113, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %115 = %114.0;
  %116 = nn.relu(%115) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %117 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %118 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %119 = nn.conv2d(%116, %117, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %120 = nn.conv2d(%107, %118, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %121 = add(%119, %120) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256), float32] */;
  %125 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %126 = nn.batch_norm(%121, %122, %123, %124, %125, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %127 = %126.0;
  %128 = nn.relu(%127) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %129 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %130 = nn.conv2d(%128, %129, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256), float32] */;
  %134 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %135 = nn.batch_norm(%130, %131, %132, %133, %134, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %136 = %135.0;
  %137 = nn.relu(%136) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %138 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %139 = nn.conv2d(%137, %138, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %140 = add(%139, %121) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(256), float32] */;
  %144 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(256), float32] */;
  %145 = nn.batch_norm(%140, %141, %142, %143, %144, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %146 = %145.0;
  %147 = nn.relu(%146) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %148 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512), float32] */;
  %153 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512), float32] */;
  %154 = nn.batch_norm(%149, %150, %151, %152, %153, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %155 = %154.0;
  %156 = nn.relu(%155) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %157 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %158 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %159 = nn.conv2d(%156, %157, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %160 = nn.conv2d(%147, %158, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %161 = add(%159, %160) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512), float32] */;
  %165 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %166 = nn.batch_norm(%161, %162, %163, %164, %165, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %167 = %166.0;
  %168 = nn.relu(%167) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %169 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512), float32] */;
  %174 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %175 = nn.batch_norm(%170, %171, %172, %173, %174, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %176 = %175.0;
  %177 = nn.relu(%176) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %178 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %179 = nn.conv2d(%177, %178, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %180 = add(%179, %161) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %181 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(512), float32] */;
  %184 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(512), float32] */;
  %185 = nn.batch_norm(%180, %181, %182, %183, %184, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %186 = %185.0;
  %187 = nn.relu(%186) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %190 = nn.global_avg_pool2d(%189) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %191 = nn.batch_flatten(%190) /* ty=Tensor[(1, 512), float32] */;
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][104]) /* ty=Tensor[(1, 512), float32] */;
  %193 = annotation.compiler_begin(%192, meta[relay.attrs.CompilerAttrs][105]) /* ty=Tensor[(1, 512), float32] */;
  %194 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][106]) /* ty=Tensor[(1000, 512), float32] */;
  %195 = nn.dense(%193, %194, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %196 = annotation.compiler_end(%195, meta[relay.attrs.CompilerAttrs][107]) /* ty=Tensor[(1, 1000), float32] */;
  %197 = annotation.compiler_begin(%196, meta[relay.attrs.CompilerAttrs][108]) /* ty=Tensor[(1, 1000), float32] */;
  %198 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1000), float32] */;
  %199 = nn.bias_add(%197, %198, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  %200 = nn.softmax(%199) /* ty=Tensor[(1, 1000), float32] */;
  annotation.compiler_end(%200, meta[relay.attrs.CompilerAttrs][110]) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = annotation.compiler_begin(%16, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %18 = nn.max_pool2d(%17, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = annotation.compiler_end(%18, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = annotation.compiler_begin(%19, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64), float32] */;
  %24 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %25 = nn.batch_norm(%20, %21, %22, %23, %24, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %26 = %25.0;
  %27 = nn.relu(%26) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %28 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %29 = nn.conv2d(%27, %28, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64), float32] */;
  %34 = nn.batch_norm(%29, %30, %31, %32, %33, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %35 = %34.0;
  %36 = nn.relu(%35) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %37 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %38 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %39 = nn.conv2d(%36, %37, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = nn.conv2d(%27, %38, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %41 = add(%39, %40) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %45 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %46 = nn.batch_norm(%41, %42, %43, %44, %45, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %47 = %46.0;
  %48 = nn.relu(%47) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %50 = nn.conv2d(%48, %49, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64), float32] */;
  %54 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %55 = nn.batch_norm(%50, %51, %52, %53, %54, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %59 = nn.conv2d(%57, %58, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %60 = add(%59, %41) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %64 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %65 = nn.batch_norm(%60, %61, %62, %63, %64, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %66 = %65.0;
  %67 = nn.relu(%66) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %68 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %69 = nn.conv2d(%67, %68, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128), float32] */;
  %73 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128), float32] */;
  %74 = nn.batch_norm(%69, %70, %71, %72, %73, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %77 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %78 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %79 = nn.conv2d(%76, %77, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %80 = nn.conv2d(%67, %78, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %81 = add(%79, %80) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128), float32] */;
  %85 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %86 = nn.batch_norm(%81, %82, %83, %84, %85, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %87 = %86.0;
  %88 = nn.relu(%87) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %89 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %90 = nn.conv2d(%88, %89, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128), float32] */;
  %94 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %96 = %95.0;
  %97 = nn.relu(%96) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %98 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %99 = nn.conv2d(%97, %98, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %100 = add(%99, %81) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(128), float32] */;
  %104 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(128), float32] */;
  %105 = nn.batch_norm(%100, %101, %102, %103, %104, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %106 = %105.0;
  %107 = nn.relu(%106) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %108 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %109 = nn.conv2d(%107, %108, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256), float32] */;
  %113 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256), float32] */;
  %114 = nn.batch_norm(%109, %110, %111, %112, %113, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %115 = %114.0;
  %116 = nn.relu(%115) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %117 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %118 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %119 = nn.conv2d(%116, %117, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %120 = nn.conv2d(%107, %118, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %121 = add(%119, %120) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256), float32] */;
  %125 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %126 = nn.batch_norm(%121, %122, %123, %124, %125, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %127 = %126.0;
  %128 = nn.relu(%127) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %129 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %130 = nn.conv2d(%128, %129, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256), float32] */;
  %134 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %135 = nn.batch_norm(%130, %131, %132, %133, %134, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %136 = %135.0;
  %137 = nn.relu(%136) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %138 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %139 = nn.conv2d(%137, %138, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %140 = add(%139, %121) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(256), float32] */;
  %144 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(256), float32] */;
  %145 = nn.batch_norm(%140, %141, %142, %143, %144, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %146 = %145.0;
  %147 = nn.relu(%146) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %148 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512), float32] */;
  %153 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512), float32] */;
  %154 = nn.batch_norm(%149, %150, %151, %152, %153, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %155 = %154.0;
  %156 = nn.relu(%155) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %157 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %158 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %159 = nn.conv2d(%156, %157, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %160 = nn.conv2d(%147, %158, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %161 = add(%159, %160) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512), float32] */;
  %165 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %166 = nn.batch_norm(%161, %162, %163, %164, %165, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %167 = %166.0;
  %168 = nn.relu(%167) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %169 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512), float32] */;
  %174 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %175 = nn.batch_norm(%170, %171, %172, %173, %174, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %176 = %175.0;
  %177 = nn.relu(%176) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %178 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %179 = nn.conv2d(%177, %178, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %180 = add(%179, %161) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %181 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(512), float32] */;
  %184 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(512), float32] */;
  %185 = nn.batch_norm(%180, %181, %182, %183, %184, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %186 = %185.0;
  %187 = nn.relu(%186) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %190 = nn.global_avg_pool2d(%189) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %191 = nn.batch_flatten(%190) /* ty=Tensor[(1, 512), float32] */;
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][104]) /* ty=Tensor[(1, 512), float32] */;
  %193 = annotation.compiler_begin(%192, meta[relay.attrs.CompilerAttrs][105]) /* ty=Tensor[(1, 512), float32] */;
  %194 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][106]) /* ty=Tensor[(1000, 512), float32] */;
  %195 = nn.dense(%193, %194, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %196 = annotation.compiler_end(%195, meta[relay.attrs.CompilerAttrs][107]) /* ty=Tensor[(1, 1000), float32] */;
  %197 = annotation.compiler_begin(%196, meta[relay.attrs.CompilerAttrs][108]) /* ty=Tensor[(1, 1000), float32] */;
  %198 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1000), float32] */;
  %199 = nn.bias_add(%197, %198, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  %200 = nn.softmax(%199) /* ty=Tensor[(1, 1000), float32] */;
  annotation.compiler_end(%200, meta[relay.attrs.CompilerAttrs][110]) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: RemoveDefaultAnnotationsopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = annotation.compiler_begin(%16, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %18 = nn.max_pool2d(%17, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = annotation.compiler_end(%18, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %20 = annotation.compiler_begin(%19, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64), float32] */;
  %24 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %25 = nn.batch_norm(%20, %21, %22, %23, %24, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %26 = %25.0;
  %27 = nn.relu(%26) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %28 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %29 = nn.conv2d(%27, %28, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64), float32] */;
  %33 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64), float32] */;
  %34 = nn.batch_norm(%29, %30, %31, %32, %33, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %35 = %34.0;
  %36 = nn.relu(%35) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %37 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %38 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %39 = nn.conv2d(%36, %37, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = nn.conv2d(%27, %38, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %41 = add(%39, %40) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64), float32] */;
  %45 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %46 = nn.batch_norm(%41, %42, %43, %44, %45, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %47 = %46.0;
  %48 = nn.relu(%47) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %50 = nn.conv2d(%48, %49, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64), float32] */;
  %54 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %55 = nn.batch_norm(%50, %51, %52, %53, %54, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %56 = %55.0;
  %57 = nn.relu(%56) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %59 = nn.conv2d(%57, %58, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %60 = add(%59, %41) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(64), float32] */;
  %64 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(64), float32] */;
  %65 = nn.batch_norm(%60, %61, %62, %63, %64, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %66 = %65.0;
  %67 = nn.relu(%66) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %68 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %69 = nn.conv2d(%67, %68, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128), float32] */;
  %73 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128), float32] */;
  %74 = nn.batch_norm(%69, %70, %71, %72, %73, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %75 = %74.0;
  %76 = nn.relu(%75) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %77 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %78 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %79 = nn.conv2d(%76, %77, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %80 = nn.conv2d(%67, %78, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %81 = add(%79, %80) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128), float32] */;
  %85 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %86 = nn.batch_norm(%81, %82, %83, %84, %85, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %87 = %86.0;
  %88 = nn.relu(%87) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %89 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %90 = nn.conv2d(%88, %89, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128), float32] */;
  %94 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %95 = nn.batch_norm(%90, %91, %92, %93, %94, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %96 = %95.0;
  %97 = nn.relu(%96) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %98 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %99 = nn.conv2d(%97, %98, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %100 = add(%99, %81) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(128), float32] */;
  %104 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(128), float32] */;
  %105 = nn.batch_norm(%100, %101, %102, %103, %104, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %106 = %105.0;
  %107 = nn.relu(%106) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %108 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %109 = nn.conv2d(%107, %108, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256), float32] */;
  %113 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256), float32] */;
  %114 = nn.batch_norm(%109, %110, %111, %112, %113, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %115 = %114.0;
  %116 = nn.relu(%115) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %117 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %118 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %119 = nn.conv2d(%116, %117, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %120 = nn.conv2d(%107, %118, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %121 = add(%119, %120) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256), float32] */;
  %125 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %126 = nn.batch_norm(%121, %122, %123, %124, %125, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %127 = %126.0;
  %128 = nn.relu(%127) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %129 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %130 = nn.conv2d(%128, %129, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256), float32] */;
  %134 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %135 = nn.batch_norm(%130, %131, %132, %133, %134, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %136 = %135.0;
  %137 = nn.relu(%136) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %138 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %139 = nn.conv2d(%137, %138, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %140 = add(%139, %121) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(256), float32] */;
  %144 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(256), float32] */;
  %145 = nn.batch_norm(%140, %141, %142, %143, %144, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %146 = %145.0;
  %147 = nn.relu(%146) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %148 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %149 = nn.conv2d(%147, %148, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512), float32] */;
  %153 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512), float32] */;
  %154 = nn.batch_norm(%149, %150, %151, %152, %153, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %155 = %154.0;
  %156 = nn.relu(%155) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %157 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %158 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %159 = nn.conv2d(%156, %157, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %160 = nn.conv2d(%147, %158, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %161 = add(%159, %160) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512), float32] */;
  %165 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %166 = nn.batch_norm(%161, %162, %163, %164, %165, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %167 = %166.0;
  %168 = nn.relu(%167) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %169 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %170 = nn.conv2d(%168, %169, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512), float32] */;
  %174 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %175 = nn.batch_norm(%170, %171, %172, %173, %174, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %176 = %175.0;
  %177 = nn.relu(%176) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %178 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %179 = nn.conv2d(%177, %178, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %180 = add(%179, %161) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %181 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(512), float32] */;
  %184 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(512), float32] */;
  %185 = nn.batch_norm(%180, %181, %182, %183, %184, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %186 = %185.0;
  %187 = nn.relu(%186) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %188 = annotation.compiler_end(%187, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %190 = nn.global_avg_pool2d(%189) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %191 = nn.batch_flatten(%190) /* ty=Tensor[(1, 512), float32] */;
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][104]) /* ty=Tensor[(1, 512), float32] */;
  %193 = annotation.compiler_begin(%192, meta[relay.attrs.CompilerAttrs][105]) /* ty=Tensor[(1, 512), float32] */;
  %194 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][106]) /* ty=Tensor[(1000, 512), float32] */;
  %195 = nn.dense(%193, %194, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %196 = annotation.compiler_end(%195, meta[relay.attrs.CompilerAttrs][107]) /* ty=Tensor[(1, 1000), float32] */;
  %197 = annotation.compiler_begin(%196, meta[relay.attrs.CompilerAttrs][108]) /* ty=Tensor[(1, 1000), float32] */;
  %198 = annotation.compiler_begin(%fc1_bias, meta[relay.attrs.CompilerAttrs][109]) /* ty=Tensor[(1000), float32] */;
  %199 = nn.bias_add(%197, %198, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  %200 = nn.softmax(%199) /* ty=Tensor[(1, 1000), float32] */;
  annotation.compiler_end(%200, meta[relay.attrs.CompilerAttrs][110]) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = nn.max_pool2d(%16, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]);
  %18 = annotation.compiler_begin(%17, meta[relay.attrs.CompilerAttrs][11]);
  %19 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(64), float32] */;
  %20 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(64), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = nn.batch_norm(%18, %19, %20, %21, %22, epsilon=2e-05f);
  %24 = %23.0;
  %25 = nn.relu(%24);
  %26 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %27 = nn.conv2d(%25, %26, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %28 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %29 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = nn.batch_norm(%27, %28, %29, %30, %31, epsilon=2e-05f);
  %33 = %32.0;
  %34 = nn.relu(%33);
  %35 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %36 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %37 = nn.conv2d(%34, %35, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %38 = nn.conv2d(%25, %36, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]);
  %39 = add(%37, %38);
  %40 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64), float32] */;
  %41 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = nn.batch_norm(%39, %40, %41, %42, %43, epsilon=2e-05f);
  %45 = %44.0;
  %46 = nn.relu(%45);
  %47 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %48 = nn.conv2d(%46, %47, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %49 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %50 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = nn.batch_norm(%48, %49, %50, %51, %52, epsilon=2e-05f);
  %54 = %53.0;
  %55 = nn.relu(%54);
  %56 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %57 = nn.conv2d(%55, %56, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %58 = add(%57, %39);
  %59 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %60 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = nn.batch_norm(%58, %59, %60, %61, %62, epsilon=2e-05f);
  %64 = %63.0;
  %65 = nn.relu(%64);
  %66 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %67 = nn.conv2d(%65, %66, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %68 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(128), float32] */;
  %69 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = nn.batch_norm(%67, %68, %69, %70, %71, epsilon=2e-05f);
  %73 = %72.0;
  %74 = nn.relu(%73);
  %75 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %76 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %77 = nn.conv2d(%74, %75, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %78 = nn.conv2d(%65, %76, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]);
  %79 = add(%77, %78);
  %80 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128), float32] */;
  %81 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = nn.batch_norm(%79, %80, %81, %82, %83, epsilon=2e-05f);
  %85 = %84.0;
  %86 = nn.relu(%85);
  %87 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %89 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %90 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = nn.batch_norm(%88, %89, %90, %91, %92, epsilon=2e-05f);
  %94 = %93.0;
  %95 = nn.relu(%94);
  %96 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %97 = nn.conv2d(%95, %96, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %98 = add(%97, %79);
  %99 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %100 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = nn.batch_norm(%98, %99, %100, %101, %102, epsilon=2e-05f);
  %104 = %103.0;
  %105 = nn.relu(%104);
  %106 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %107 = nn.conv2d(%105, %106, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %108 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(256), float32] */;
  %109 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = nn.batch_norm(%107, %108, %109, %110, %111, epsilon=2e-05f);
  %113 = %112.0;
  %114 = nn.relu(%113);
  %115 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %116 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %117 = nn.conv2d(%114, %115, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %118 = nn.conv2d(%105, %116, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]);
  %119 = add(%117, %118);
  %120 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256), float32] */;
  %121 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = nn.batch_norm(%119, %120, %121, %122, %123, epsilon=2e-05f);
  %125 = %124.0;
  %126 = nn.relu(%125);
  %127 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %129 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %130 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = nn.batch_norm(%128, %129, %130, %131, %132, epsilon=2e-05f);
  %134 = %133.0;
  %135 = nn.relu(%134);
  %136 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %137 = nn.conv2d(%135, %136, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %138 = add(%137, %119);
  %139 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %140 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = nn.batch_norm(%138, %139, %140, %141, %142, epsilon=2e-05f);
  %144 = %143.0;
  %145 = nn.relu(%144);
  %146 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %147 = nn.conv2d(%145, %146, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %148 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(512), float32] */;
  %149 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = nn.batch_norm(%147, %148, %149, %150, %151, epsilon=2e-05f);
  %153 = %152.0;
  %154 = nn.relu(%153);
  %155 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %156 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %157 = nn.conv2d(%154, %155, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %158 = nn.conv2d(%145, %156, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]);
  %159 = add(%157, %158);
  %160 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512), float32] */;
  %161 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = nn.batch_norm(%159, %160, %161, %162, %163, epsilon=2e-05f);
  %165 = %164.0;
  %166 = nn.relu(%165);
  %167 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %168 = nn.conv2d(%166, %167, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %169 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %170 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = nn.batch_norm(%168, %169, %170, %171, %172, epsilon=2e-05f);
  %174 = %173.0;
  %175 = nn.relu(%174);
  %176 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %177 = nn.conv2d(%175, %176, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %178 = add(%177, %159);
  %179 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %180 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512), float32] */;
  %181 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = nn.batch_norm(%178, %179, %180, %181, %182, epsilon=2e-05f);
  %184 = %183.0;
  %185 = nn.relu(%184);
  %186 = annotation.compiler_end(%185, meta[relay.attrs.CompilerAttrs][100]);
  %187 = nn.global_avg_pool2d(%186);
  %188 = nn.batch_flatten(%187);
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][101]);
  %190 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1000, 512), float32] */;
  %191 = nn.dense(%189, %190, units=1000);
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][103]);
  %193 = nn.bias_add(%192, %fc1_bias, axis=-1);
  nn.softmax(%193)
}


Running pass: {} The meta data of the pass: pass name: PartitionGraphopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = nn.max_pool2d(%16, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = annotation.compiler_begin(%17, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(64), float32] */;
  %20 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(64), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = nn.batch_norm(%18, %19, %20, %21, %22, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %24 = %23.0;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %27 = nn.conv2d(%25, %26, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %28 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %29 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = nn.batch_norm(%27, %28, %29, %30, %31, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %33 = %32.0;
  %34 = nn.relu(%33) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %35 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %36 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %37 = nn.conv2d(%34, %35, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %38 = nn.conv2d(%25, %36, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %39 = add(%37, %38) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64), float32] */;
  %41 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = nn.batch_norm(%39, %40, %41, %42, %43, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %45 = %44.0;
  %46 = nn.relu(%45) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %47 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %48 = nn.conv2d(%46, %47, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %50 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = nn.batch_norm(%48, %49, %50, %51, %52, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %54 = %53.0;
  %55 = nn.relu(%54) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %56 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %57 = nn.conv2d(%55, %56, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = add(%57, %39) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %59 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %60 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = nn.batch_norm(%58, %59, %60, %61, %62, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %64 = %63.0;
  %65 = nn.relu(%64) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %66 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %67 = nn.conv2d(%65, %66, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %68 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(128), float32] */;
  %69 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = nn.batch_norm(%67, %68, %69, %70, %71, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %73 = %72.0;
  %74 = nn.relu(%73) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %75 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %76 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %77 = nn.conv2d(%74, %75, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %78 = nn.conv2d(%65, %76, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %79 = add(%77, %78) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %80 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128), float32] */;
  %81 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = nn.batch_norm(%79, %80, %81, %82, %83, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %85 = %84.0;
  %86 = nn.relu(%85) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %87 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %89 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %90 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = nn.batch_norm(%88, %89, %90, %91, %92, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %94 = %93.0;
  %95 = nn.relu(%94) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %96 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %97 = nn.conv2d(%95, %96, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %98 = add(%97, %79) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %99 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %100 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = nn.batch_norm(%98, %99, %100, %101, %102, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %104 = %103.0;
  %105 = nn.relu(%104) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %106 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %107 = nn.conv2d(%105, %106, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %108 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(256), float32] */;
  %109 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = nn.batch_norm(%107, %108, %109, %110, %111, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %113 = %112.0;
  %114 = nn.relu(%113) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %115 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %116 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %117 = nn.conv2d(%114, %115, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %118 = nn.conv2d(%105, %116, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %119 = add(%117, %118) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %120 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256), float32] */;
  %121 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = nn.batch_norm(%119, %120, %121, %122, %123, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %125 = %124.0;
  %126 = nn.relu(%125) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %127 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %129 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %130 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = nn.batch_norm(%128, %129, %130, %131, %132, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %134 = %133.0;
  %135 = nn.relu(%134) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %136 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %137 = nn.conv2d(%135, %136, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %138 = add(%137, %119) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %139 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %140 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = nn.batch_norm(%138, %139, %140, %141, %142, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %144 = %143.0;
  %145 = nn.relu(%144) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %146 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %147 = nn.conv2d(%145, %146, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %148 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(512), float32] */;
  %149 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = nn.batch_norm(%147, %148, %149, %150, %151, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %153 = %152.0;
  %154 = nn.relu(%153) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %155 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %156 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %157 = nn.conv2d(%154, %155, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %158 = nn.conv2d(%145, %156, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %159 = add(%157, %158) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %160 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512), float32] */;
  %161 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = nn.batch_norm(%159, %160, %161, %162, %163, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %165 = %164.0;
  %166 = nn.relu(%165) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %167 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %168 = nn.conv2d(%166, %167, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %169 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %170 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = nn.batch_norm(%168, %169, %170, %171, %172, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %174 = %173.0;
  %175 = nn.relu(%174) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %176 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %177 = nn.conv2d(%175, %176, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %178 = add(%177, %159) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %179 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %180 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512), float32] */;
  %181 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = nn.batch_norm(%178, %179, %180, %181, %182, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %184 = %183.0;
  %185 = nn.relu(%184) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %186 = annotation.compiler_end(%185, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %187 = nn.global_avg_pool2d(%186) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %188 = nn.batch_flatten(%187) /* ty=Tensor[(1, 512), float32] */;
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(1, 512), float32] */;
  %190 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1000, 512), float32] */;
  %191 = nn.dense(%189, %190, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(1, 1000), float32] */;
  %193 = nn.bias_add(%192, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%193) /* ty=Tensor[(1, 1000), float32] */
}


Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = nn.max_pool2d(%16, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = annotation.compiler_begin(%17, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(64), float32] */;
  %20 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(64), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = nn.batch_norm(%18, %19, %20, %21, %22, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %24 = %23.0;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %27 = nn.conv2d(%25, %26, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %28 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %29 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = nn.batch_norm(%27, %28, %29, %30, %31, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %33 = %32.0;
  %34 = nn.relu(%33) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %35 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %36 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %37 = nn.conv2d(%34, %35, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %38 = nn.conv2d(%25, %36, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %39 = add(%37, %38) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64), float32] */;
  %41 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = nn.batch_norm(%39, %40, %41, %42, %43, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %45 = %44.0;
  %46 = nn.relu(%45) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %47 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %48 = nn.conv2d(%46, %47, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %50 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = nn.batch_norm(%48, %49, %50, %51, %52, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %54 = %53.0;
  %55 = nn.relu(%54) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %56 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %57 = nn.conv2d(%55, %56, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = add(%57, %39) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %59 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %60 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = nn.batch_norm(%58, %59, %60, %61, %62, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %64 = %63.0;
  %65 = nn.relu(%64) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %66 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %67 = nn.conv2d(%65, %66, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %68 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(128), float32] */;
  %69 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = nn.batch_norm(%67, %68, %69, %70, %71, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %73 = %72.0;
  %74 = nn.relu(%73) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %75 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %76 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %77 = nn.conv2d(%74, %75, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %78 = nn.conv2d(%65, %76, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %79 = add(%77, %78) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %80 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128), float32] */;
  %81 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = nn.batch_norm(%79, %80, %81, %82, %83, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %85 = %84.0;
  %86 = nn.relu(%85) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %87 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %89 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %90 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = nn.batch_norm(%88, %89, %90, %91, %92, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %94 = %93.0;
  %95 = nn.relu(%94) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %96 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %97 = nn.conv2d(%95, %96, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %98 = add(%97, %79) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %99 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %100 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = nn.batch_norm(%98, %99, %100, %101, %102, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %104 = %103.0;
  %105 = nn.relu(%104) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %106 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %107 = nn.conv2d(%105, %106, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %108 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(256), float32] */;
  %109 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = nn.batch_norm(%107, %108, %109, %110, %111, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %113 = %112.0;
  %114 = nn.relu(%113) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %115 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %116 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %117 = nn.conv2d(%114, %115, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %118 = nn.conv2d(%105, %116, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %119 = add(%117, %118) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %120 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256), float32] */;
  %121 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = nn.batch_norm(%119, %120, %121, %122, %123, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %125 = %124.0;
  %126 = nn.relu(%125) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %127 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %129 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %130 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = nn.batch_norm(%128, %129, %130, %131, %132, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %134 = %133.0;
  %135 = nn.relu(%134) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %136 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %137 = nn.conv2d(%135, %136, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %138 = add(%137, %119) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %139 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %140 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = nn.batch_norm(%138, %139, %140, %141, %142, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %144 = %143.0;
  %145 = nn.relu(%144) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %146 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %147 = nn.conv2d(%145, %146, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %148 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(512), float32] */;
  %149 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = nn.batch_norm(%147, %148, %149, %150, %151, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %153 = %152.0;
  %154 = nn.relu(%153) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %155 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %156 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %157 = nn.conv2d(%154, %155, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %158 = nn.conv2d(%145, %156, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %159 = add(%157, %158) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %160 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512), float32] */;
  %161 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = nn.batch_norm(%159, %160, %161, %162, %163, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %165 = %164.0;
  %166 = nn.relu(%165) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %167 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %168 = nn.conv2d(%166, %167, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %169 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %170 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = nn.batch_norm(%168, %169, %170, %171, %172, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %174 = %173.0;
  %175 = nn.relu(%174) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %176 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %177 = nn.conv2d(%175, %176, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %178 = add(%177, %159) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %179 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %180 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512), float32] */;
  %181 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = nn.batch_norm(%178, %179, %180, %181, %182, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %184 = %183.0;
  %185 = nn.relu(%184) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %186 = annotation.compiler_end(%185, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %187 = nn.global_avg_pool2d(%186) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %188 = nn.batch_flatten(%187) /* ty=Tensor[(1, 512), float32] */;
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(1, 512), float32] */;
  %190 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1000, 512), float32] */;
  %191 = nn.dense(%189, %190, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(1, 1000), float32] */;
  %193 = nn.bias_add(%192, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%193) /* ty=Tensor[(1, 1000), float32] */
}

def @dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], global_symbol="dnnl_0", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 64, 112, 112), float32] {
  %194 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False);
  %195 = %194.0;
  %196 = nn.conv2d(%195, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]);
  %197 = nn.batch_norm(%196, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f);
  %198 = %197.0;
  nn.relu(%198)
}


Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = nn.max_pool2d(%16, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = annotation.compiler_begin(%17, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(64), float32] */;
  %20 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(64), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = nn.batch_norm(%18, %19, %20, %21, %22, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %24 = %23.0;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %27 = nn.conv2d(%25, %26, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %28 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %29 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = nn.batch_norm(%27, %28, %29, %30, %31, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %33 = %32.0;
  %34 = nn.relu(%33) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %35 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %36 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %37 = nn.conv2d(%34, %35, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %38 = nn.conv2d(%25, %36, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %39 = add(%37, %38) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64), float32] */;
  %41 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = nn.batch_norm(%39, %40, %41, %42, %43, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %45 = %44.0;
  %46 = nn.relu(%45) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %47 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %48 = nn.conv2d(%46, %47, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %50 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = nn.batch_norm(%48, %49, %50, %51, %52, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %54 = %53.0;
  %55 = nn.relu(%54) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %56 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %57 = nn.conv2d(%55, %56, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = add(%57, %39) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %59 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %60 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = nn.batch_norm(%58, %59, %60, %61, %62, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %64 = %63.0;
  %65 = nn.relu(%64) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %66 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %67 = nn.conv2d(%65, %66, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %68 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(128), float32] */;
  %69 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = nn.batch_norm(%67, %68, %69, %70, %71, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %73 = %72.0;
  %74 = nn.relu(%73) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %75 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %76 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %77 = nn.conv2d(%74, %75, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %78 = nn.conv2d(%65, %76, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %79 = add(%77, %78) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %80 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128), float32] */;
  %81 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = nn.batch_norm(%79, %80, %81, %82, %83, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %85 = %84.0;
  %86 = nn.relu(%85) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %87 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %89 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %90 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = nn.batch_norm(%88, %89, %90, %91, %92, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %94 = %93.0;
  %95 = nn.relu(%94) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %96 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %97 = nn.conv2d(%95, %96, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %98 = add(%97, %79) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %99 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %100 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = nn.batch_norm(%98, %99, %100, %101, %102, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %104 = %103.0;
  %105 = nn.relu(%104) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %106 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %107 = nn.conv2d(%105, %106, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %108 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(256), float32] */;
  %109 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = nn.batch_norm(%107, %108, %109, %110, %111, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %113 = %112.0;
  %114 = nn.relu(%113) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %115 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %116 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %117 = nn.conv2d(%114, %115, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %118 = nn.conv2d(%105, %116, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %119 = add(%117, %118) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %120 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256), float32] */;
  %121 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = nn.batch_norm(%119, %120, %121, %122, %123, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %125 = %124.0;
  %126 = nn.relu(%125) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %127 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %129 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %130 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = nn.batch_norm(%128, %129, %130, %131, %132, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %134 = %133.0;
  %135 = nn.relu(%134) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %136 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %137 = nn.conv2d(%135, %136, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %138 = add(%137, %119) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %139 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %140 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = nn.batch_norm(%138, %139, %140, %141, %142, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %144 = %143.0;
  %145 = nn.relu(%144) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %146 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %147 = nn.conv2d(%145, %146, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %148 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(512), float32] */;
  %149 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = nn.batch_norm(%147, %148, %149, %150, %151, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %153 = %152.0;
  %154 = nn.relu(%153) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %155 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %156 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %157 = nn.conv2d(%154, %155, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %158 = nn.conv2d(%145, %156, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %159 = add(%157, %158) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %160 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512), float32] */;
  %161 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = nn.batch_norm(%159, %160, %161, %162, %163, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %165 = %164.0;
  %166 = nn.relu(%165) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %167 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %168 = nn.conv2d(%166, %167, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %169 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %170 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = nn.batch_norm(%168, %169, %170, %171, %172, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %174 = %173.0;
  %175 = nn.relu(%174) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %176 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %177 = nn.conv2d(%175, %176, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %178 = add(%177, %159) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %179 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %180 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512), float32] */;
  %181 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = nn.batch_norm(%178, %179, %180, %181, %182, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %184 = %183.0;
  %185 = nn.relu(%184) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %186 = annotation.compiler_end(%185, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %187 = nn.global_avg_pool2d(%186) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %188 = nn.batch_flatten(%187) /* ty=Tensor[(1, 512), float32] */;
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(1, 512), float32] */;
  %190 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1000, 512), float32] */;
  %191 = nn.dense(%189, %190, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(1, 1000), float32] */;
  %193 = nn.bias_add(%192, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%193) /* ty=Tensor[(1, 1000), float32] */
}

def @dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], global_symbol="dnnl_0", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 64, 112, 112), float32] {
  %194 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %195 = %194.0;
  %196 = nn.conv2d(%195, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %197 = nn.batch_norm(%196, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %198 = %197.0;
  nn.relu(%198) /* ty=Tensor[(1, 64, 112, 112), float32] */
}

def @dnnl_10(%dnnl_10_i0: Tensor[(1, 64, 56, 56), float32], %dnnl_10_i1: Tensor[(64), float32], %dnnl_10_i2: Tensor[(64), float32], %dnnl_10_i3: Tensor[(64), float32], %dnnl_10_i4: Tensor[(64), float32], %dnnl_10_i5: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i6: Tensor[(64), float32], %dnnl_10_i7: Tensor[(64), float32], %dnnl_10_i8: Tensor[(64), float32], %dnnl_10_i9: Tensor[(64), float32], %dnnl_10_i10: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i11: Tensor[(64, 64, 1, 1), float32], %dnnl_10_i12: Tensor[(64), float32], %dnnl_10_i13: Tensor[(64), float32], %dnnl_10_i14: Tensor[(64), float32], %dnnl_10_i15: Tensor[(64), float32], %dnnl_10_i16: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i17: Tensor[(64), float32], %dnnl_10_i18: Tensor[(64), float32], %dnnl_10_i19: Tensor[(64), float32], %dnnl_10_i20: Tensor[(64), float32], %dnnl_10_i21: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i22: Tensor[(64), float32], %dnnl_10_i23: Tensor[(64), float32], %dnnl_10_i24: Tensor[(64), float32], %dnnl_10_i25: Tensor[(64), float32], %dnnl_10_i26: Tensor[(128, 64, 3, 3), float32], %dnnl_10_i27: Tensor[(128), float32], %dnnl_10_i28: Tensor[(128), float32], %dnnl_10_i29: Tensor[(128), float32], %dnnl_10_i30: Tensor[(128), float32], %dnnl_10_i31: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i32: Tensor[(128, 64, 1, 1), float32], %dnnl_10_i33: Tensor[(128), float32], %dnnl_10_i34: Tensor[(128), float32], %dnnl_10_i35: Tensor[(128), float32], %dnnl_10_i36: Tensor[(128), float32], %dnnl_10_i37: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i38: Tensor[(128), float32], %dnnl_10_i39: Tensor[(128), float32], %dnnl_10_i40: Tensor[(128), float32], %dnnl_10_i41: Tensor[(128), float32], %dnnl_10_i42: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i43: Tensor[(128), float32], %dnnl_10_i44: Tensor[(128), float32], %dnnl_10_i45: Tensor[(128), float32], %dnnl_10_i46: Tensor[(128), float32], %dnnl_10_i47: Tensor[(256, 128, 3, 3), float32], %dnnl_10_i48: Tensor[(256), float32], %dnnl_10_i49: Tensor[(256), float32], %dnnl_10_i50: Tensor[(256), float32], %dnnl_10_i51: Tensor[(256), float32], %dnnl_10_i52: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i53: Tensor[(256, 128, 1, 1), float32], %dnnl_10_i54: Tensor[(256), float32], %dnnl_10_i55: Tensor[(256), float32], %dnnl_10_i56: Tensor[(256), float32], %dnnl_10_i57: Tensor[(256), float32], %dnnl_10_i58: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i59: Tensor[(256), float32], %dnnl_10_i60: Tensor[(256), float32], %dnnl_10_i61: Tensor[(256), float32], %dnnl_10_i62: Tensor[(256), float32], %dnnl_10_i63: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i64: Tensor[(256), float32], %dnnl_10_i65: Tensor[(256), float32], %dnnl_10_i66: Tensor[(256), float32], %dnnl_10_i67: Tensor[(256), float32], %dnnl_10_i68: Tensor[(512, 256, 3, 3), float32], %dnnl_10_i69: Tensor[(512), float32], %dnnl_10_i70: Tensor[(512), float32], %dnnl_10_i71: Tensor[(512), float32], %dnnl_10_i72: Tensor[(512), float32], %dnnl_10_i73: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i74: Tensor[(512, 256, 1, 1), float32], %dnnl_10_i75: Tensor[(512), float32], %dnnl_10_i76: Tensor[(512), float32], %dnnl_10_i77: Tensor[(512), float32], %dnnl_10_i78: Tensor[(512), float32], %dnnl_10_i79: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i80: Tensor[(512), float32], %dnnl_10_i81: Tensor[(512), float32], %dnnl_10_i82: Tensor[(512), float32], %dnnl_10_i83: Tensor[(512), float32], %dnnl_10_i84: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i85: Tensor[(512), float32], %dnnl_10_i86: Tensor[(512), float32], %dnnl_10_i87: Tensor[(512), float32], %dnnl_10_i88: Tensor[(512), float32], global_symbol="dnnl_10", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 512, 7, 7), float32] {
  %199 = nn.batch_norm(%dnnl_10_i0, %dnnl_10_i1, %dnnl_10_i2, %dnnl_10_i3, %dnnl_10_i4, epsilon=2e-05f);
  %200 = %199.0;
  %201 = nn.relu(%200);
  %202 = nn.conv2d(%201, %dnnl_10_i5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %203 = nn.batch_norm(%202, %dnnl_10_i6, %dnnl_10_i7, %dnnl_10_i8, %dnnl_10_i9, epsilon=2e-05f);
  %204 = %203.0;
  %205 = nn.relu(%204);
  %206 = nn.conv2d(%205, %dnnl_10_i10, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %207 = nn.conv2d(%201, %dnnl_10_i11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]);
  %208 = add(%206, %207);
  %209 = nn.batch_norm(%208, %dnnl_10_i12, %dnnl_10_i13, %dnnl_10_i14, %dnnl_10_i15, epsilon=2e-05f);
  %210 = %209.0;
  %211 = nn.relu(%210);
  %212 = nn.conv2d(%211, %dnnl_10_i16, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %213 = nn.batch_norm(%212, %dnnl_10_i17, %dnnl_10_i18, %dnnl_10_i19, %dnnl_10_i20, epsilon=2e-05f);
  %214 = %213.0;
  %215 = nn.relu(%214);
  %216 = nn.conv2d(%215, %dnnl_10_i21, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);
  %217 = add(%216, %208);
  %218 = nn.batch_norm(%217, %dnnl_10_i22, %dnnl_10_i23, %dnnl_10_i24, %dnnl_10_i25, epsilon=2e-05f);
  %219 = %218.0;
  %220 = nn.relu(%219);
  %221 = nn.conv2d(%220, %dnnl_10_i26, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %222 = nn.batch_norm(%221, %dnnl_10_i27, %dnnl_10_i28, %dnnl_10_i29, %dnnl_10_i30, epsilon=2e-05f);
  %223 = %222.0;
  %224 = nn.relu(%223);
  %225 = nn.conv2d(%224, %dnnl_10_i31, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %226 = nn.conv2d(%220, %dnnl_10_i32, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]);
  %227 = add(%225, %226);
  %228 = nn.batch_norm(%227, %dnnl_10_i33, %dnnl_10_i34, %dnnl_10_i35, %dnnl_10_i36, epsilon=2e-05f);
  %229 = %228.0;
  %230 = nn.relu(%229);
  %231 = nn.conv2d(%230, %dnnl_10_i37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %232 = nn.batch_norm(%231, %dnnl_10_i38, %dnnl_10_i39, %dnnl_10_i40, %dnnl_10_i41, epsilon=2e-05f);
  %233 = %232.0;
  %234 = nn.relu(%233);
  %235 = nn.conv2d(%234, %dnnl_10_i42, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);
  %236 = add(%235, %227);
  %237 = nn.batch_norm(%236, %dnnl_10_i43, %dnnl_10_i44, %dnnl_10_i45, %dnnl_10_i46, epsilon=2e-05f);
  %238 = %237.0;
  %239 = nn.relu(%238);
  %240 = nn.conv2d(%239, %dnnl_10_i47, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %241 = nn.batch_norm(%240, %dnnl_10_i48, %dnnl_10_i49, %dnnl_10_i50, %dnnl_10_i51, epsilon=2e-05f);
  %242 = %241.0;
  %243 = nn.relu(%242);
  %244 = nn.conv2d(%243, %dnnl_10_i52, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %245 = nn.conv2d(%239, %dnnl_10_i53, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]);
  %246 = add(%244, %245);
  %247 = nn.batch_norm(%246, %dnnl_10_i54, %dnnl_10_i55, %dnnl_10_i56, %dnnl_10_i57, epsilon=2e-05f);
  %248 = %247.0;
  %249 = nn.relu(%248);
  %250 = nn.conv2d(%249, %dnnl_10_i58, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %251 = nn.batch_norm(%250, %dnnl_10_i59, %dnnl_10_i60, %dnnl_10_i61, %dnnl_10_i62, epsilon=2e-05f);
  %252 = %251.0;
  %253 = nn.relu(%252);
  %254 = nn.conv2d(%253, %dnnl_10_i63, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);
  %255 = add(%254, %246);
  %256 = nn.batch_norm(%255, %dnnl_10_i64, %dnnl_10_i65, %dnnl_10_i66, %dnnl_10_i67, epsilon=2e-05f);
  %257 = %256.0;
  %258 = nn.relu(%257);
  %259 = nn.conv2d(%258, %dnnl_10_i68, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %260 = nn.batch_norm(%259, %dnnl_10_i69, %dnnl_10_i70, %dnnl_10_i71, %dnnl_10_i72, epsilon=2e-05f);
  %261 = %260.0;
  %262 = nn.relu(%261);
  %263 = nn.conv2d(%262, %dnnl_10_i73, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %264 = nn.conv2d(%258, %dnnl_10_i74, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]);
  %265 = add(%263, %264);
  %266 = nn.batch_norm(%265, %dnnl_10_i75, %dnnl_10_i76, %dnnl_10_i77, %dnnl_10_i78, epsilon=2e-05f);
  %267 = %266.0;
  %268 = nn.relu(%267);
  %269 = nn.conv2d(%268, %dnnl_10_i79, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %270 = nn.batch_norm(%269, %dnnl_10_i80, %dnnl_10_i81, %dnnl_10_i82, %dnnl_10_i83, epsilon=2e-05f);
  %271 = %270.0;
  %272 = nn.relu(%271);
  %273 = nn.conv2d(%272, %dnnl_10_i84, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);
  %274 = add(%273, %265);
  %275 = nn.batch_norm(%274, %dnnl_10_i85, %dnnl_10_i86, %dnnl_10_i87, %dnnl_10_i88, epsilon=2e-05f);
  %276 = %275.0;
  nn.relu(%276)
}


Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %0 = annotation.compiler_begin(%data, meta[relay.attrs.CompilerAttrs][0]) /* ty=Tensor[(1, 3, 224, 224), float32] */;
  %1 = annotation.compiler_begin(%bn_data_gamma, meta[relay.attrs.CompilerAttrs][1]) /* ty=Tensor[(3), float32] */;
  %2 = annotation.compiler_begin(%bn_data_beta, meta[relay.attrs.CompilerAttrs][2]) /* ty=Tensor[(3), float32] */;
  %3 = annotation.compiler_begin(%bn_data_moving_mean, meta[relay.attrs.CompilerAttrs][3]) /* ty=Tensor[(3), float32] */;
  %4 = annotation.compiler_begin(%bn_data_moving_var, meta[relay.attrs.CompilerAttrs][4]) /* ty=Tensor[(3), float32] */;
  %5 = nn.batch_norm(%0, %1, %2, %3, %4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %6 = %5.0;
  %7 = annotation.compiler_begin(%conv0_weight, meta[relay.attrs.CompilerAttrs][5]) /* ty=Tensor[(64, 3, 7, 7), float32] */;
  %8 = nn.conv2d(%6, %7, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %9 = annotation.compiler_begin(%bn0_gamma, meta[relay.attrs.CompilerAttrs][6]) /* ty=Tensor[(64), float32] */;
  %10 = annotation.compiler_begin(%bn0_beta, meta[relay.attrs.CompilerAttrs][7]) /* ty=Tensor[(64), float32] */;
  %11 = annotation.compiler_begin(%bn0_moving_mean, meta[relay.attrs.CompilerAttrs][8]) /* ty=Tensor[(64), float32] */;
  %12 = annotation.compiler_begin(%bn0_moving_var, meta[relay.attrs.CompilerAttrs][9]) /* ty=Tensor[(64), float32] */;
  %13 = nn.batch_norm(%8, %9, %10, %11, %12, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %14 = %13.0;
  %15 = nn.relu(%14) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %16 = annotation.compiler_end(%15, meta[relay.attrs.CompilerAttrs][10]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %17 = nn.max_pool2d(%16, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = annotation.compiler_begin(%17, meta[relay.attrs.CompilerAttrs][11]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = annotation.compiler_begin(%stage1_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][12]) /* ty=Tensor[(64), float32] */;
  %20 = annotation.compiler_begin(%stage1_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][13]) /* ty=Tensor[(64), float32] */;
  %21 = annotation.compiler_begin(%stage1_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][14]) /* ty=Tensor[(64), float32] */;
  %22 = annotation.compiler_begin(%stage1_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][15]) /* ty=Tensor[(64), float32] */;
  %23 = nn.batch_norm(%18, %19, %20, %21, %22, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %24 = %23.0;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %26 = annotation.compiler_begin(%stage1_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][16]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %27 = nn.conv2d(%25, %26, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %28 = annotation.compiler_begin(%stage1_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][17]) /* ty=Tensor[(64), float32] */;
  %29 = annotation.compiler_begin(%stage1_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][18]) /* ty=Tensor[(64), float32] */;
  %30 = annotation.compiler_begin(%stage1_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][19]) /* ty=Tensor[(64), float32] */;
  %31 = annotation.compiler_begin(%stage1_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][20]) /* ty=Tensor[(64), float32] */;
  %32 = nn.batch_norm(%27, %28, %29, %30, %31, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %33 = %32.0;
  %34 = nn.relu(%33) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %35 = annotation.compiler_begin(%stage1_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][21]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %36 = annotation.compiler_begin(%stage1_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][22]) /* ty=Tensor[(64, 64, 1, 1), float32] */;
  %37 = nn.conv2d(%34, %35, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %38 = nn.conv2d(%25, %36, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %39 = add(%37, %38) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %40 = annotation.compiler_begin(%stage1_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][23]) /* ty=Tensor[(64), float32] */;
  %41 = annotation.compiler_begin(%stage1_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][24]) /* ty=Tensor[(64), float32] */;
  %42 = annotation.compiler_begin(%stage1_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][25]) /* ty=Tensor[(64), float32] */;
  %43 = annotation.compiler_begin(%stage1_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][26]) /* ty=Tensor[(64), float32] */;
  %44 = nn.batch_norm(%39, %40, %41, %42, %43, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %45 = %44.0;
  %46 = nn.relu(%45) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %47 = annotation.compiler_begin(%stage1_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][27]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %48 = nn.conv2d(%46, %47, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %49 = annotation.compiler_begin(%stage1_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][28]) /* ty=Tensor[(64), float32] */;
  %50 = annotation.compiler_begin(%stage1_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][29]) /* ty=Tensor[(64), float32] */;
  %51 = annotation.compiler_begin(%stage1_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][30]) /* ty=Tensor[(64), float32] */;
  %52 = annotation.compiler_begin(%stage1_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][31]) /* ty=Tensor[(64), float32] */;
  %53 = nn.batch_norm(%48, %49, %50, %51, %52, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %54 = %53.0;
  %55 = nn.relu(%54) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %56 = annotation.compiler_begin(%stage1_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][32]) /* ty=Tensor[(64, 64, 3, 3), float32] */;
  %57 = nn.conv2d(%55, %56, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %58 = add(%57, %39) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %59 = annotation.compiler_begin(%stage2_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][33]) /* ty=Tensor[(64), float32] */;
  %60 = annotation.compiler_begin(%stage2_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][34]) /* ty=Tensor[(64), float32] */;
  %61 = annotation.compiler_begin(%stage2_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][35]) /* ty=Tensor[(64), float32] */;
  %62 = annotation.compiler_begin(%stage2_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][36]) /* ty=Tensor[(64), float32] */;
  %63 = nn.batch_norm(%58, %59, %60, %61, %62, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %64 = %63.0;
  %65 = nn.relu(%64) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %66 = annotation.compiler_begin(%stage2_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][37]) /* ty=Tensor[(128, 64, 3, 3), float32] */;
  %67 = nn.conv2d(%65, %66, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %68 = annotation.compiler_begin(%stage2_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][38]) /* ty=Tensor[(128), float32] */;
  %69 = annotation.compiler_begin(%stage2_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][39]) /* ty=Tensor[(128), float32] */;
  %70 = annotation.compiler_begin(%stage2_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][40]) /* ty=Tensor[(128), float32] */;
  %71 = annotation.compiler_begin(%stage2_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][41]) /* ty=Tensor[(128), float32] */;
  %72 = nn.batch_norm(%67, %68, %69, %70, %71, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %73 = %72.0;
  %74 = nn.relu(%73) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %75 = annotation.compiler_begin(%stage2_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][42]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %76 = annotation.compiler_begin(%stage2_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][43]) /* ty=Tensor[(128, 64, 1, 1), float32] */;
  %77 = nn.conv2d(%74, %75, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %78 = nn.conv2d(%65, %76, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %79 = add(%77, %78) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %80 = annotation.compiler_begin(%stage2_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][44]) /* ty=Tensor[(128), float32] */;
  %81 = annotation.compiler_begin(%stage2_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][45]) /* ty=Tensor[(128), float32] */;
  %82 = annotation.compiler_begin(%stage2_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][46]) /* ty=Tensor[(128), float32] */;
  %83 = annotation.compiler_begin(%stage2_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][47]) /* ty=Tensor[(128), float32] */;
  %84 = nn.batch_norm(%79, %80, %81, %82, %83, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %85 = %84.0;
  %86 = nn.relu(%85) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %87 = annotation.compiler_begin(%stage2_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][48]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %88 = nn.conv2d(%86, %87, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %89 = annotation.compiler_begin(%stage2_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][49]) /* ty=Tensor[(128), float32] */;
  %90 = annotation.compiler_begin(%stage2_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][50]) /* ty=Tensor[(128), float32] */;
  %91 = annotation.compiler_begin(%stage2_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][51]) /* ty=Tensor[(128), float32] */;
  %92 = annotation.compiler_begin(%stage2_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][52]) /* ty=Tensor[(128), float32] */;
  %93 = nn.batch_norm(%88, %89, %90, %91, %92, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %94 = %93.0;
  %95 = nn.relu(%94) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %96 = annotation.compiler_begin(%stage2_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][53]) /* ty=Tensor[(128, 128, 3, 3), float32] */;
  %97 = nn.conv2d(%95, %96, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %98 = add(%97, %79) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %99 = annotation.compiler_begin(%stage3_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][54]) /* ty=Tensor[(128), float32] */;
  %100 = annotation.compiler_begin(%stage3_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][55]) /* ty=Tensor[(128), float32] */;
  %101 = annotation.compiler_begin(%stage3_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][56]) /* ty=Tensor[(128), float32] */;
  %102 = annotation.compiler_begin(%stage3_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][57]) /* ty=Tensor[(128), float32] */;
  %103 = nn.batch_norm(%98, %99, %100, %101, %102, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %104 = %103.0;
  %105 = nn.relu(%104) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %106 = annotation.compiler_begin(%stage3_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][58]) /* ty=Tensor[(256, 128, 3, 3), float32] */;
  %107 = nn.conv2d(%105, %106, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %108 = annotation.compiler_begin(%stage3_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][59]) /* ty=Tensor[(256), float32] */;
  %109 = annotation.compiler_begin(%stage3_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][60]) /* ty=Tensor[(256), float32] */;
  %110 = annotation.compiler_begin(%stage3_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][61]) /* ty=Tensor[(256), float32] */;
  %111 = annotation.compiler_begin(%stage3_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][62]) /* ty=Tensor[(256), float32] */;
  %112 = nn.batch_norm(%107, %108, %109, %110, %111, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %113 = %112.0;
  %114 = nn.relu(%113) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %115 = annotation.compiler_begin(%stage3_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][63]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %116 = annotation.compiler_begin(%stage3_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][64]) /* ty=Tensor[(256, 128, 1, 1), float32] */;
  %117 = nn.conv2d(%114, %115, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %118 = nn.conv2d(%105, %116, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %119 = add(%117, %118) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %120 = annotation.compiler_begin(%stage3_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][65]) /* ty=Tensor[(256), float32] */;
  %121 = annotation.compiler_begin(%stage3_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][66]) /* ty=Tensor[(256), float32] */;
  %122 = annotation.compiler_begin(%stage3_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][67]) /* ty=Tensor[(256), float32] */;
  %123 = annotation.compiler_begin(%stage3_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][68]) /* ty=Tensor[(256), float32] */;
  %124 = nn.batch_norm(%119, %120, %121, %122, %123, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %125 = %124.0;
  %126 = nn.relu(%125) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %127 = annotation.compiler_begin(%stage3_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][69]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %128 = nn.conv2d(%126, %127, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %129 = annotation.compiler_begin(%stage3_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][70]) /* ty=Tensor[(256), float32] */;
  %130 = annotation.compiler_begin(%stage3_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][71]) /* ty=Tensor[(256), float32] */;
  %131 = annotation.compiler_begin(%stage3_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][72]) /* ty=Tensor[(256), float32] */;
  %132 = annotation.compiler_begin(%stage3_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][73]) /* ty=Tensor[(256), float32] */;
  %133 = nn.batch_norm(%128, %129, %130, %131, %132, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %134 = %133.0;
  %135 = nn.relu(%134) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %136 = annotation.compiler_begin(%stage3_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][74]) /* ty=Tensor[(256, 256, 3, 3), float32] */;
  %137 = nn.conv2d(%135, %136, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %138 = add(%137, %119) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %139 = annotation.compiler_begin(%stage4_unit1_bn1_gamma, meta[relay.attrs.CompilerAttrs][75]) /* ty=Tensor[(256), float32] */;
  %140 = annotation.compiler_begin(%stage4_unit1_bn1_beta, meta[relay.attrs.CompilerAttrs][76]) /* ty=Tensor[(256), float32] */;
  %141 = annotation.compiler_begin(%stage4_unit1_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][77]) /* ty=Tensor[(256), float32] */;
  %142 = annotation.compiler_begin(%stage4_unit1_bn1_moving_var, meta[relay.attrs.CompilerAttrs][78]) /* ty=Tensor[(256), float32] */;
  %143 = nn.batch_norm(%138, %139, %140, %141, %142, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %144 = %143.0;
  %145 = nn.relu(%144) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %146 = annotation.compiler_begin(%stage4_unit1_conv1_weight, meta[relay.attrs.CompilerAttrs][79]) /* ty=Tensor[(512, 256, 3, 3), float32] */;
  %147 = nn.conv2d(%145, %146, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %148 = annotation.compiler_begin(%stage4_unit1_bn2_gamma, meta[relay.attrs.CompilerAttrs][80]) /* ty=Tensor[(512), float32] */;
  %149 = annotation.compiler_begin(%stage4_unit1_bn2_beta, meta[relay.attrs.CompilerAttrs][81]) /* ty=Tensor[(512), float32] */;
  %150 = annotation.compiler_begin(%stage4_unit1_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][82]) /* ty=Tensor[(512), float32] */;
  %151 = annotation.compiler_begin(%stage4_unit1_bn2_moving_var, meta[relay.attrs.CompilerAttrs][83]) /* ty=Tensor[(512), float32] */;
  %152 = nn.batch_norm(%147, %148, %149, %150, %151, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %153 = %152.0;
  %154 = nn.relu(%153) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %155 = annotation.compiler_begin(%stage4_unit1_conv2_weight, meta[relay.attrs.CompilerAttrs][84]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %156 = annotation.compiler_begin(%stage4_unit1_sc_weight, meta[relay.attrs.CompilerAttrs][85]) /* ty=Tensor[(512, 256, 1, 1), float32] */;
  %157 = nn.conv2d(%154, %155, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %158 = nn.conv2d(%145, %156, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %159 = add(%157, %158) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %160 = annotation.compiler_begin(%stage4_unit2_bn1_gamma, meta[relay.attrs.CompilerAttrs][86]) /* ty=Tensor[(512), float32] */;
  %161 = annotation.compiler_begin(%stage4_unit2_bn1_beta, meta[relay.attrs.CompilerAttrs][87]) /* ty=Tensor[(512), float32] */;
  %162 = annotation.compiler_begin(%stage4_unit2_bn1_moving_mean, meta[relay.attrs.CompilerAttrs][88]) /* ty=Tensor[(512), float32] */;
  %163 = annotation.compiler_begin(%stage4_unit2_bn1_moving_var, meta[relay.attrs.CompilerAttrs][89]) /* ty=Tensor[(512), float32] */;
  %164 = nn.batch_norm(%159, %160, %161, %162, %163, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %165 = %164.0;
  %166 = nn.relu(%165) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %167 = annotation.compiler_begin(%stage4_unit2_conv1_weight, meta[relay.attrs.CompilerAttrs][90]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %168 = nn.conv2d(%166, %167, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %169 = annotation.compiler_begin(%stage4_unit2_bn2_gamma, meta[relay.attrs.CompilerAttrs][91]) /* ty=Tensor[(512), float32] */;
  %170 = annotation.compiler_begin(%stage4_unit2_bn2_beta, meta[relay.attrs.CompilerAttrs][92]) /* ty=Tensor[(512), float32] */;
  %171 = annotation.compiler_begin(%stage4_unit2_bn2_moving_mean, meta[relay.attrs.CompilerAttrs][93]) /* ty=Tensor[(512), float32] */;
  %172 = annotation.compiler_begin(%stage4_unit2_bn2_moving_var, meta[relay.attrs.CompilerAttrs][94]) /* ty=Tensor[(512), float32] */;
  %173 = nn.batch_norm(%168, %169, %170, %171, %172, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %174 = %173.0;
  %175 = nn.relu(%174) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %176 = annotation.compiler_begin(%stage4_unit2_conv2_weight, meta[relay.attrs.CompilerAttrs][95]) /* ty=Tensor[(512, 512, 3, 3), float32] */;
  %177 = nn.conv2d(%175, %176, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %178 = add(%177, %159) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %179 = annotation.compiler_begin(%bn1_gamma, meta[relay.attrs.CompilerAttrs][96]) /* ty=Tensor[(512), float32] */;
  %180 = annotation.compiler_begin(%bn1_beta, meta[relay.attrs.CompilerAttrs][97]) /* ty=Tensor[(512), float32] */;
  %181 = annotation.compiler_begin(%bn1_moving_mean, meta[relay.attrs.CompilerAttrs][98]) /* ty=Tensor[(512), float32] */;
  %182 = annotation.compiler_begin(%bn1_moving_var, meta[relay.attrs.CompilerAttrs][99]) /* ty=Tensor[(512), float32] */;
  %183 = nn.batch_norm(%178, %179, %180, %181, %182, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %184 = %183.0;
  %185 = nn.relu(%184) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %186 = annotation.compiler_end(%185, meta[relay.attrs.CompilerAttrs][100]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %187 = nn.global_avg_pool2d(%186) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %188 = nn.batch_flatten(%187) /* ty=Tensor[(1, 512), float32] */;
  %189 = annotation.compiler_begin(%188, meta[relay.attrs.CompilerAttrs][101]) /* ty=Tensor[(1, 512), float32] */;
  %190 = annotation.compiler_begin(%fc1_weight, meta[relay.attrs.CompilerAttrs][102]) /* ty=Tensor[(1000, 512), float32] */;
  %191 = nn.dense(%189, %190, units=1000) /* ty=Tensor[(1, 1000), float32] */;
  %192 = annotation.compiler_end(%191, meta[relay.attrs.CompilerAttrs][103]) /* ty=Tensor[(1, 1000), float32] */;
  %193 = nn.bias_add(%192, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%193) /* ty=Tensor[(1, 1000), float32] */
}

def @dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], global_symbol="dnnl_0", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 64, 112, 112), float32] {
  %194 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %195 = %194.0;
  %196 = nn.conv2d(%195, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %197 = nn.batch_norm(%196, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %198 = %197.0;
  nn.relu(%198) /* ty=Tensor[(1, 64, 112, 112), float32] */
}

def @dnnl_10(%dnnl_10_i0: Tensor[(1, 64, 56, 56), float32], %dnnl_10_i1: Tensor[(64), float32], %dnnl_10_i2: Tensor[(64), float32], %dnnl_10_i3: Tensor[(64), float32], %dnnl_10_i4: Tensor[(64), float32], %dnnl_10_i5: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i6: Tensor[(64), float32], %dnnl_10_i7: Tensor[(64), float32], %dnnl_10_i8: Tensor[(64), float32], %dnnl_10_i9: Tensor[(64), float32], %dnnl_10_i10: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i11: Tensor[(64, 64, 1, 1), float32], %dnnl_10_i12: Tensor[(64), float32], %dnnl_10_i13: Tensor[(64), float32], %dnnl_10_i14: Tensor[(64), float32], %dnnl_10_i15: Tensor[(64), float32], %dnnl_10_i16: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i17: Tensor[(64), float32], %dnnl_10_i18: Tensor[(64), float32], %dnnl_10_i19: Tensor[(64), float32], %dnnl_10_i20: Tensor[(64), float32], %dnnl_10_i21: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i22: Tensor[(64), float32], %dnnl_10_i23: Tensor[(64), float32], %dnnl_10_i24: Tensor[(64), float32], %dnnl_10_i25: Tensor[(64), float32], %dnnl_10_i26: Tensor[(128, 64, 3, 3), float32], %dnnl_10_i27: Tensor[(128), float32], %dnnl_10_i28: Tensor[(128), float32], %dnnl_10_i29: Tensor[(128), float32], %dnnl_10_i30: Tensor[(128), float32], %dnnl_10_i31: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i32: Tensor[(128, 64, 1, 1), float32], %dnnl_10_i33: Tensor[(128), float32], %dnnl_10_i34: Tensor[(128), float32], %dnnl_10_i35: Tensor[(128), float32], %dnnl_10_i36: Tensor[(128), float32], %dnnl_10_i37: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i38: Tensor[(128), float32], %dnnl_10_i39: Tensor[(128), float32], %dnnl_10_i40: Tensor[(128), float32], %dnnl_10_i41: Tensor[(128), float32], %dnnl_10_i42: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i43: Tensor[(128), float32], %dnnl_10_i44: Tensor[(128), float32], %dnnl_10_i45: Tensor[(128), float32], %dnnl_10_i46: Tensor[(128), float32], %dnnl_10_i47: Tensor[(256, 128, 3, 3), float32], %dnnl_10_i48: Tensor[(256), float32], %dnnl_10_i49: Tensor[(256), float32], %dnnl_10_i50: Tensor[(256), float32], %dnnl_10_i51: Tensor[(256), float32], %dnnl_10_i52: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i53: Tensor[(256, 128, 1, 1), float32], %dnnl_10_i54: Tensor[(256), float32], %dnnl_10_i55: Tensor[(256), float32], %dnnl_10_i56: Tensor[(256), float32], %dnnl_10_i57: Tensor[(256), float32], %dnnl_10_i58: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i59: Tensor[(256), float32], %dnnl_10_i60: Tensor[(256), float32], %dnnl_10_i61: Tensor[(256), float32], %dnnl_10_i62: Tensor[(256), float32], %dnnl_10_i63: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i64: Tensor[(256), float32], %dnnl_10_i65: Tensor[(256), float32], %dnnl_10_i66: Tensor[(256), float32], %dnnl_10_i67: Tensor[(256), float32], %dnnl_10_i68: Tensor[(512, 256, 3, 3), float32], %dnnl_10_i69: Tensor[(512), float32], %dnnl_10_i70: Tensor[(512), float32], %dnnl_10_i71: Tensor[(512), float32], %dnnl_10_i72: Tensor[(512), float32], %dnnl_10_i73: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i74: Tensor[(512, 256, 1, 1), float32], %dnnl_10_i75: Tensor[(512), float32], %dnnl_10_i76: Tensor[(512), float32], %dnnl_10_i77: Tensor[(512), float32], %dnnl_10_i78: Tensor[(512), float32], %dnnl_10_i79: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i80: Tensor[(512), float32], %dnnl_10_i81: Tensor[(512), float32], %dnnl_10_i82: Tensor[(512), float32], %dnnl_10_i83: Tensor[(512), float32], %dnnl_10_i84: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i85: Tensor[(512), float32], %dnnl_10_i86: Tensor[(512), float32], %dnnl_10_i87: Tensor[(512), float32], %dnnl_10_i88: Tensor[(512), float32], global_symbol="dnnl_10", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 512, 7, 7), float32] {
  %199 = nn.batch_norm(%dnnl_10_i0, %dnnl_10_i1, %dnnl_10_i2, %dnnl_10_i3, %dnnl_10_i4, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %200 = %199.0;
  %201 = nn.relu(%200) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %202 = nn.conv2d(%201, %dnnl_10_i5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %203 = nn.batch_norm(%202, %dnnl_10_i6, %dnnl_10_i7, %dnnl_10_i8, %dnnl_10_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %204 = %203.0;
  %205 = nn.relu(%204) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %206 = nn.conv2d(%205, %dnnl_10_i10, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %207 = nn.conv2d(%201, %dnnl_10_i11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %208 = add(%206, %207) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %209 = nn.batch_norm(%208, %dnnl_10_i12, %dnnl_10_i13, %dnnl_10_i14, %dnnl_10_i15, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %210 = %209.0;
  %211 = nn.relu(%210) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %212 = nn.conv2d(%211, %dnnl_10_i16, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %213 = nn.batch_norm(%212, %dnnl_10_i17, %dnnl_10_i18, %dnnl_10_i19, %dnnl_10_i20, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %214 = %213.0;
  %215 = nn.relu(%214) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %216 = nn.conv2d(%215, %dnnl_10_i21, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %217 = add(%216, %208) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %218 = nn.batch_norm(%217, %dnnl_10_i22, %dnnl_10_i23, %dnnl_10_i24, %dnnl_10_i25, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %219 = %218.0;
  %220 = nn.relu(%219) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %221 = nn.conv2d(%220, %dnnl_10_i26, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %222 = nn.batch_norm(%221, %dnnl_10_i27, %dnnl_10_i28, %dnnl_10_i29, %dnnl_10_i30, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %223 = %222.0;
  %224 = nn.relu(%223) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %225 = nn.conv2d(%224, %dnnl_10_i31, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %226 = nn.conv2d(%220, %dnnl_10_i32, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %227 = add(%225, %226) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %228 = nn.batch_norm(%227, %dnnl_10_i33, %dnnl_10_i34, %dnnl_10_i35, %dnnl_10_i36, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %229 = %228.0;
  %230 = nn.relu(%229) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %231 = nn.conv2d(%230, %dnnl_10_i37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %232 = nn.batch_norm(%231, %dnnl_10_i38, %dnnl_10_i39, %dnnl_10_i40, %dnnl_10_i41, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %233 = %232.0;
  %234 = nn.relu(%233) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %235 = nn.conv2d(%234, %dnnl_10_i42, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %236 = add(%235, %227) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %237 = nn.batch_norm(%236, %dnnl_10_i43, %dnnl_10_i44, %dnnl_10_i45, %dnnl_10_i46, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %238 = %237.0;
  %239 = nn.relu(%238) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %240 = nn.conv2d(%239, %dnnl_10_i47, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %241 = nn.batch_norm(%240, %dnnl_10_i48, %dnnl_10_i49, %dnnl_10_i50, %dnnl_10_i51, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %242 = %241.0;
  %243 = nn.relu(%242) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %244 = nn.conv2d(%243, %dnnl_10_i52, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %245 = nn.conv2d(%239, %dnnl_10_i53, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %246 = add(%244, %245) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %247 = nn.batch_norm(%246, %dnnl_10_i54, %dnnl_10_i55, %dnnl_10_i56, %dnnl_10_i57, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %248 = %247.0;
  %249 = nn.relu(%248) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %250 = nn.conv2d(%249, %dnnl_10_i58, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %251 = nn.batch_norm(%250, %dnnl_10_i59, %dnnl_10_i60, %dnnl_10_i61, %dnnl_10_i62, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %252 = %251.0;
  %253 = nn.relu(%252) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %254 = nn.conv2d(%253, %dnnl_10_i63, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %255 = add(%254, %246) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %256 = nn.batch_norm(%255, %dnnl_10_i64, %dnnl_10_i65, %dnnl_10_i66, %dnnl_10_i67, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %257 = %256.0;
  %258 = nn.relu(%257) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %259 = nn.conv2d(%258, %dnnl_10_i68, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %260 = nn.batch_norm(%259, %dnnl_10_i69, %dnnl_10_i70, %dnnl_10_i71, %dnnl_10_i72, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %261 = %260.0;
  %262 = nn.relu(%261) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %263 = nn.conv2d(%262, %dnnl_10_i73, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %264 = nn.conv2d(%258, %dnnl_10_i74, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %265 = add(%263, %264) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %266 = nn.batch_norm(%265, %dnnl_10_i75, %dnnl_10_i76, %dnnl_10_i77, %dnnl_10_i78, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %267 = %266.0;
  %268 = nn.relu(%267) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %269 = nn.conv2d(%268, %dnnl_10_i79, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %270 = nn.batch_norm(%269, %dnnl_10_i80, %dnnl_10_i81, %dnnl_10_i82, %dnnl_10_i83, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %271 = %270.0;
  %272 = nn.relu(%271) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %273 = nn.conv2d(%272, %dnnl_10_i84, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %274 = add(%273, %265) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %275 = nn.batch_norm(%274, %dnnl_10_i85, %dnnl_10_i86, %dnnl_10_i87, %dnnl_10_i88, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %276 = %275.0;
  nn.relu(%276) /* ty=Tensor[(1, 512, 7, 7), float32] */
}

def @dnnl_99(%dnnl_99_i0: Tensor[(1, 512), float32], %dnnl_99_i1: Tensor[(1000, 512), float32], global_symbol="dnnl_99", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 1000), float32] {
  nn.dense(%dnnl_99_i0, %dnnl_99_i1, units=1000)
}


Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @dnnl_10(%dnnl_10_i0: Tensor[(1, 64, 56, 56), float32], %dnnl_10_i1: Tensor[(64), float32], %dnnl_10_i2: Tensor[(64), float32], %dnnl_10_i3: Tensor[(64), float32], %dnnl_10_i4: Tensor[(64), float32], %dnnl_10_i5: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i6: Tensor[(64), float32], %dnnl_10_i7: Tensor[(64), float32], %dnnl_10_i8: Tensor[(64), float32], %dnnl_10_i9: Tensor[(64), float32], %dnnl_10_i10: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i11: Tensor[(64, 64, 1, 1), float32], %dnnl_10_i12: Tensor[(64), float32], %dnnl_10_i13: Tensor[(64), float32], %dnnl_10_i14: Tensor[(64), float32], %dnnl_10_i15: Tensor[(64), float32], %dnnl_10_i16: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i17: Tensor[(64), float32], %dnnl_10_i18: Tensor[(64), float32], %dnnl_10_i19: Tensor[(64), float32], %dnnl_10_i20: Tensor[(64), float32], %dnnl_10_i21: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i22: Tensor[(64), float32], %dnnl_10_i23: Tensor[(64), float32], %dnnl_10_i24: Tensor[(64), float32], %dnnl_10_i25: Tensor[(64), float32], %dnnl_10_i26: Tensor[(128, 64, 3, 3), float32], %dnnl_10_i27: Tensor[(128), float32], %dnnl_10_i28: Tensor[(128), float32], %dnnl_10_i29: Tensor[(128), float32], %dnnl_10_i30: Tensor[(128), float32], %dnnl_10_i31: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i32: Tensor[(128, 64, 1, 1), float32], %dnnl_10_i33: Tensor[(128), float32], %dnnl_10_i34: Tensor[(128), float32], %dnnl_10_i35: Tensor[(128), float32], %dnnl_10_i36: Tensor[(128), float32], %dnnl_10_i37: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i38: Tensor[(128), float32], %dnnl_10_i39: Tensor[(128), float32], %dnnl_10_i40: Tensor[(128), float32], %dnnl_10_i41: Tensor[(128), float32], %dnnl_10_i42: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i43: Tensor[(128), float32], %dnnl_10_i44: Tensor[(128), float32], %dnnl_10_i45: Tensor[(128), float32], %dnnl_10_i46: Tensor[(128), float32], %dnnl_10_i47: Tensor[(256, 128, 3, 3), float32], %dnnl_10_i48: Tensor[(256), float32], %dnnl_10_i49: Tensor[(256), float32], %dnnl_10_i50: Tensor[(256), float32], %dnnl_10_i51: Tensor[(256), float32], %dnnl_10_i52: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i53: Tensor[(256, 128, 1, 1), float32], %dnnl_10_i54: Tensor[(256), float32], %dnnl_10_i55: Tensor[(256), float32], %dnnl_10_i56: Tensor[(256), float32], %dnnl_10_i57: Tensor[(256), float32], %dnnl_10_i58: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i59: Tensor[(256), float32], %dnnl_10_i60: Tensor[(256), float32], %dnnl_10_i61: Tensor[(256), float32], %dnnl_10_i62: Tensor[(256), float32], %dnnl_10_i63: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i64: Tensor[(256), float32], %dnnl_10_i65: Tensor[(256), float32], %dnnl_10_i66: Tensor[(256), float32], %dnnl_10_i67: Tensor[(256), float32], %dnnl_10_i68: Tensor[(512, 256, 3, 3), float32], %dnnl_10_i69: Tensor[(512), float32], %dnnl_10_i70: Tensor[(512), float32], %dnnl_10_i71: Tensor[(512), float32], %dnnl_10_i72: Tensor[(512), float32], %dnnl_10_i73: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i74: Tensor[(512, 256, 1, 1), float32], %dnnl_10_i75: Tensor[(512), float32], %dnnl_10_i76: Tensor[(512), float32], %dnnl_10_i77: Tensor[(512), float32], %dnnl_10_i78: Tensor[(512), float32], %dnnl_10_i79: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i80: Tensor[(512), float32], %dnnl_10_i81: Tensor[(512), float32], %dnnl_10_i82: Tensor[(512), float32], %dnnl_10_i83: Tensor[(512), float32], %dnnl_10_i84: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i85: Tensor[(512), float32], %dnnl_10_i86: Tensor[(512), float32], %dnnl_10_i87: Tensor[(512), float32], %dnnl_10_i88: Tensor[(512), float32], global_symbol="dnnl_10", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 512, 7, 7), float32] {
  %0 = nn.batch_norm(%dnnl_10_i0, %dnnl_10_i1, %dnnl_10_i2, %dnnl_10_i3, %dnnl_10_i4, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %1 = %0.0;
  %2 = nn.relu(%1) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %3 = nn.conv2d(%2, %dnnl_10_i5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %4 = nn.batch_norm(%3, %dnnl_10_i6, %dnnl_10_i7, %dnnl_10_i8, %dnnl_10_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %5 = %4.0;
  %6 = nn.relu(%5) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.conv2d(%6, %dnnl_10_i10, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%2, %dnnl_10_i11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %9 = add(%7, %8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.batch_norm(%9, %dnnl_10_i12, %dnnl_10_i13, %dnnl_10_i14, %dnnl_10_i15, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %11 = %10.0;
  %12 = nn.relu(%11) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %13 = nn.conv2d(%12, %dnnl_10_i16, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.batch_norm(%13, %dnnl_10_i17, %dnnl_10_i18, %dnnl_10_i19, %dnnl_10_i20, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %15 = %14.0;
  %16 = nn.relu(%15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.conv2d(%16, %dnnl_10_i21, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = add(%17, %9) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = nn.batch_norm(%18, %dnnl_10_i22, %dnnl_10_i23, %dnnl_10_i24, %dnnl_10_i25, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = %19.0;
  %21 = nn.relu(%20) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, %dnnl_10_i26, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %23 = nn.batch_norm(%22, %dnnl_10_i27, %dnnl_10_i28, %dnnl_10_i29, %dnnl_10_i30, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %24 = %23.0;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %26 = nn.conv2d(%25, %dnnl_10_i31, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %27 = nn.conv2d(%21, %dnnl_10_i32, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %28 = add(%26, %27) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %29 = nn.batch_norm(%28, %dnnl_10_i33, %dnnl_10_i34, %dnnl_10_i35, %dnnl_10_i36, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %30 = %29.0;
  %31 = nn.relu(%30) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %32 = nn.conv2d(%31, %dnnl_10_i37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.batch_norm(%32, %dnnl_10_i38, %dnnl_10_i39, %dnnl_10_i40, %dnnl_10_i41, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %34 = %33.0;
  %35 = nn.relu(%34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.conv2d(%35, %dnnl_10_i42, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %37 = add(%36, %28) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %38 = nn.batch_norm(%37, %dnnl_10_i43, %dnnl_10_i44, %dnnl_10_i45, %dnnl_10_i46, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %39 = %38.0;
  %40 = nn.relu(%39) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %41 = nn.conv2d(%40, %dnnl_10_i47, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %42 = nn.batch_norm(%41, %dnnl_10_i48, %dnnl_10_i49, %dnnl_10_i50, %dnnl_10_i51, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %43 = %42.0;
  %44 = nn.relu(%43) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %45 = nn.conv2d(%44, %dnnl_10_i52, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %46 = nn.conv2d(%40, %dnnl_10_i53, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %47 = add(%45, %46) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %48 = nn.batch_norm(%47, %dnnl_10_i54, %dnnl_10_i55, %dnnl_10_i56, %dnnl_10_i57, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %49 = %48.0;
  %50 = nn.relu(%49) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %51 = nn.conv2d(%50, %dnnl_10_i58, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.batch_norm(%51, %dnnl_10_i59, %dnnl_10_i60, %dnnl_10_i61, %dnnl_10_i62, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %53 = %52.0;
  %54 = nn.relu(%53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.conv2d(%54, %dnnl_10_i63, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %56 = add(%55, %47) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %57 = nn.batch_norm(%56, %dnnl_10_i64, %dnnl_10_i65, %dnnl_10_i66, %dnnl_10_i67, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %58 = %57.0;
  %59 = nn.relu(%58) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %60 = nn.conv2d(%59, %dnnl_10_i68, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %61 = nn.batch_norm(%60, %dnnl_10_i69, %dnnl_10_i70, %dnnl_10_i71, %dnnl_10_i72, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %62 = %61.0;
  %63 = nn.relu(%62) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %64 = nn.conv2d(%63, %dnnl_10_i73, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %65 = nn.conv2d(%59, %dnnl_10_i74, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %66 = add(%64, %65) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %67 = nn.batch_norm(%66, %dnnl_10_i75, %dnnl_10_i76, %dnnl_10_i77, %dnnl_10_i78, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %68 = %67.0;
  %69 = nn.relu(%68) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %70 = nn.conv2d(%69, %dnnl_10_i79, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.batch_norm(%70, %dnnl_10_i80, %dnnl_10_i81, %dnnl_10_i82, %dnnl_10_i83, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %72 = %71.0;
  %73 = nn.relu(%72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.conv2d(%73, %dnnl_10_i84, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %75 = add(%74, %66) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %76 = nn.batch_norm(%75, %dnnl_10_i85, %dnnl_10_i86, %dnnl_10_i87, %dnnl_10_i88, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %77 = %76.0;
  nn.relu(%77) /* ty=Tensor[(1, 512, 7, 7), float32] */
}

def @dnnl_99(%dnnl_99_i0: Tensor[(1, 512), float32], %dnnl_99_i1: Tensor[(1000, 512), float32], global_symbol="dnnl_99", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 1000), float32] {
  nn.dense(%dnnl_99_i0, %dnnl_99_i1, units=1000) /* ty=Tensor[(1, 1000), float32] */
}

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %78 = @dnnl_0(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, %conv0_weight, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var);
  %79 = nn.max_pool2d(%78, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]);
  %80 = @dnnl_10(%79, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, %stage1_unit1_conv1_weight, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, %stage1_unit1_conv2_weight, %stage1_unit1_sc_weight, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, %stage1_unit2_conv1_weight, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, %stage1_unit2_conv2_weight, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, %stage2_unit1_conv1_weight, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, %stage2_unit1_conv2_weight, %stage2_unit1_sc_weight, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, %stage2_unit2_conv1_weight, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, %stage2_unit2_conv2_weight, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, %stage3_unit1_conv1_weight, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, %stage3_unit1_conv2_weight, %stage3_unit1_sc_weight, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, %stage3_unit2_conv1_weight, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, %stage3_unit2_conv2_weight, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, %stage4_unit1_conv1_weight, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, %stage4_unit1_conv2_weight, %stage4_unit1_sc_weight, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, %stage4_unit2_conv1_weight, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, %stage4_unit2_conv2_weight, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var);
  %81 = nn.global_avg_pool2d(%80);
  %82 = nn.batch_flatten(%81);
  %83 = @dnnl_99(%82, %fc1_weight);
  %84 = nn.bias_add(%83, %fc1_bias, axis=-1);
  nn.softmax(%84)
}

def @dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], global_symbol="dnnl_0", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 64, 112, 112), float32] {
  %85 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %86 = %85.0;
  %87 = nn.conv2d(%86, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %88 = nn.batch_norm(%87, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %89 = %88.0;
  nn.relu(%89) /* ty=Tensor[(1, 64, 112, 112), float32] */
}

Running pass: {} The meta data of the pass: pass name: NameMangleExtFuncsopt_level: 0required passes: [
]

def @dnnl_10(%dnnl_10_i0: Tensor[(1, 64, 56, 56), float32], %dnnl_10_i1: Tensor[(64), float32], %dnnl_10_i2: Tensor[(64), float32], %dnnl_10_i3: Tensor[(64), float32], %dnnl_10_i4: Tensor[(64), float32], %dnnl_10_i5: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i6: Tensor[(64), float32], %dnnl_10_i7: Tensor[(64), float32], %dnnl_10_i8: Tensor[(64), float32], %dnnl_10_i9: Tensor[(64), float32], %dnnl_10_i10: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i11: Tensor[(64, 64, 1, 1), float32], %dnnl_10_i12: Tensor[(64), float32], %dnnl_10_i13: Tensor[(64), float32], %dnnl_10_i14: Tensor[(64), float32], %dnnl_10_i15: Tensor[(64), float32], %dnnl_10_i16: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i17: Tensor[(64), float32], %dnnl_10_i18: Tensor[(64), float32], %dnnl_10_i19: Tensor[(64), float32], %dnnl_10_i20: Tensor[(64), float32], %dnnl_10_i21: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i22: Tensor[(64), float32], %dnnl_10_i23: Tensor[(64), float32], %dnnl_10_i24: Tensor[(64), float32], %dnnl_10_i25: Tensor[(64), float32], %dnnl_10_i26: Tensor[(128, 64, 3, 3), float32], %dnnl_10_i27: Tensor[(128), float32], %dnnl_10_i28: Tensor[(128), float32], %dnnl_10_i29: Tensor[(128), float32], %dnnl_10_i30: Tensor[(128), float32], %dnnl_10_i31: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i32: Tensor[(128, 64, 1, 1), float32], %dnnl_10_i33: Tensor[(128), float32], %dnnl_10_i34: Tensor[(128), float32], %dnnl_10_i35: Tensor[(128), float32], %dnnl_10_i36: Tensor[(128), float32], %dnnl_10_i37: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i38: Tensor[(128), float32], %dnnl_10_i39: Tensor[(128), float32], %dnnl_10_i40: Tensor[(128), float32], %dnnl_10_i41: Tensor[(128), float32], %dnnl_10_i42: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i43: Tensor[(128), float32], %dnnl_10_i44: Tensor[(128), float32], %dnnl_10_i45: Tensor[(128), float32], %dnnl_10_i46: Tensor[(128), float32], %dnnl_10_i47: Tensor[(256, 128, 3, 3), float32], %dnnl_10_i48: Tensor[(256), float32], %dnnl_10_i49: Tensor[(256), float32], %dnnl_10_i50: Tensor[(256), float32], %dnnl_10_i51: Tensor[(256), float32], %dnnl_10_i52: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i53: Tensor[(256, 128, 1, 1), float32], %dnnl_10_i54: Tensor[(256), float32], %dnnl_10_i55: Tensor[(256), float32], %dnnl_10_i56: Tensor[(256), float32], %dnnl_10_i57: Tensor[(256), float32], %dnnl_10_i58: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i59: Tensor[(256), float32], %dnnl_10_i60: Tensor[(256), float32], %dnnl_10_i61: Tensor[(256), float32], %dnnl_10_i62: Tensor[(256), float32], %dnnl_10_i63: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i64: Tensor[(256), float32], %dnnl_10_i65: Tensor[(256), float32], %dnnl_10_i66: Tensor[(256), float32], %dnnl_10_i67: Tensor[(256), float32], %dnnl_10_i68: Tensor[(512, 256, 3, 3), float32], %dnnl_10_i69: Tensor[(512), float32], %dnnl_10_i70: Tensor[(512), float32], %dnnl_10_i71: Tensor[(512), float32], %dnnl_10_i72: Tensor[(512), float32], %dnnl_10_i73: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i74: Tensor[(512, 256, 1, 1), float32], %dnnl_10_i75: Tensor[(512), float32], %dnnl_10_i76: Tensor[(512), float32], %dnnl_10_i77: Tensor[(512), float32], %dnnl_10_i78: Tensor[(512), float32], %dnnl_10_i79: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i80: Tensor[(512), float32], %dnnl_10_i81: Tensor[(512), float32], %dnnl_10_i82: Tensor[(512), float32], %dnnl_10_i83: Tensor[(512), float32], %dnnl_10_i84: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i85: Tensor[(512), float32], %dnnl_10_i86: Tensor[(512), float32], %dnnl_10_i87: Tensor[(512), float32], %dnnl_10_i88: Tensor[(512), float32], global_symbol="dnnl_10", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 512, 7, 7), float32] {
  %0 = nn.batch_norm(%dnnl_10_i0, %dnnl_10_i1, %dnnl_10_i2, %dnnl_10_i3, %dnnl_10_i4, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %1 = %0.0;
  %2 = nn.relu(%1) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %3 = nn.conv2d(%2, %dnnl_10_i5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %4 = nn.batch_norm(%3, %dnnl_10_i6, %dnnl_10_i7, %dnnl_10_i8, %dnnl_10_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %5 = %4.0;
  %6 = nn.relu(%5) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.conv2d(%6, %dnnl_10_i10, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%2, %dnnl_10_i11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %9 = add(%7, %8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.batch_norm(%9, %dnnl_10_i12, %dnnl_10_i13, %dnnl_10_i14, %dnnl_10_i15, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %11 = %10.0;
  %12 = nn.relu(%11) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %13 = nn.conv2d(%12, %dnnl_10_i16, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.batch_norm(%13, %dnnl_10_i17, %dnnl_10_i18, %dnnl_10_i19, %dnnl_10_i20, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %15 = %14.0;
  %16 = nn.relu(%15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.conv2d(%16, %dnnl_10_i21, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = add(%17, %9) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = nn.batch_norm(%18, %dnnl_10_i22, %dnnl_10_i23, %dnnl_10_i24, %dnnl_10_i25, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = %19.0;
  %21 = nn.relu(%20) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, %dnnl_10_i26, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %23 = nn.batch_norm(%22, %dnnl_10_i27, %dnnl_10_i28, %dnnl_10_i29, %dnnl_10_i30, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %24 = %23.0;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %26 = nn.conv2d(%25, %dnnl_10_i31, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %27 = nn.conv2d(%21, %dnnl_10_i32, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %28 = add(%26, %27) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %29 = nn.batch_norm(%28, %dnnl_10_i33, %dnnl_10_i34, %dnnl_10_i35, %dnnl_10_i36, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %30 = %29.0;
  %31 = nn.relu(%30) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %32 = nn.conv2d(%31, %dnnl_10_i37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.batch_norm(%32, %dnnl_10_i38, %dnnl_10_i39, %dnnl_10_i40, %dnnl_10_i41, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %34 = %33.0;
  %35 = nn.relu(%34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.conv2d(%35, %dnnl_10_i42, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %37 = add(%36, %28) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %38 = nn.batch_norm(%37, %dnnl_10_i43, %dnnl_10_i44, %dnnl_10_i45, %dnnl_10_i46, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %39 = %38.0;
  %40 = nn.relu(%39) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %41 = nn.conv2d(%40, %dnnl_10_i47, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %42 = nn.batch_norm(%41, %dnnl_10_i48, %dnnl_10_i49, %dnnl_10_i50, %dnnl_10_i51, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %43 = %42.0;
  %44 = nn.relu(%43) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %45 = nn.conv2d(%44, %dnnl_10_i52, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %46 = nn.conv2d(%40, %dnnl_10_i53, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %47 = add(%45, %46) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %48 = nn.batch_norm(%47, %dnnl_10_i54, %dnnl_10_i55, %dnnl_10_i56, %dnnl_10_i57, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %49 = %48.0;
  %50 = nn.relu(%49) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %51 = nn.conv2d(%50, %dnnl_10_i58, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.batch_norm(%51, %dnnl_10_i59, %dnnl_10_i60, %dnnl_10_i61, %dnnl_10_i62, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %53 = %52.0;
  %54 = nn.relu(%53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.conv2d(%54, %dnnl_10_i63, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %56 = add(%55, %47) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %57 = nn.batch_norm(%56, %dnnl_10_i64, %dnnl_10_i65, %dnnl_10_i66, %dnnl_10_i67, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %58 = %57.0;
  %59 = nn.relu(%58) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %60 = nn.conv2d(%59, %dnnl_10_i68, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %61 = nn.batch_norm(%60, %dnnl_10_i69, %dnnl_10_i70, %dnnl_10_i71, %dnnl_10_i72, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %62 = %61.0;
  %63 = nn.relu(%62) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %64 = nn.conv2d(%63, %dnnl_10_i73, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %65 = nn.conv2d(%59, %dnnl_10_i74, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %66 = add(%64, %65) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %67 = nn.batch_norm(%66, %dnnl_10_i75, %dnnl_10_i76, %dnnl_10_i77, %dnnl_10_i78, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %68 = %67.0;
  %69 = nn.relu(%68) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %70 = nn.conv2d(%69, %dnnl_10_i79, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.batch_norm(%70, %dnnl_10_i80, %dnnl_10_i81, %dnnl_10_i82, %dnnl_10_i83, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %72 = %71.0;
  %73 = nn.relu(%72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.conv2d(%73, %dnnl_10_i84, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %75 = add(%74, %66) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %76 = nn.batch_norm(%75, %dnnl_10_i85, %dnnl_10_i86, %dnnl_10_i87, %dnnl_10_i88, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %77 = %76.0;
  nn.relu(%77) /* ty=Tensor[(1, 512, 7, 7), float32] */
}

def @dnnl_99(%dnnl_99_i0: Tensor[(1, 512), float32], %dnnl_99_i1: Tensor[(1000, 512), float32], global_symbol="dnnl_99", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 1000), float32] {
  nn.dense(%dnnl_99_i0, %dnnl_99_i1, units=1000) /* ty=Tensor[(1, 1000), float32] */
}

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %78 = @dnnl_0(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, %conv0_weight, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %79 = nn.max_pool2d(%78, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %80 = @dnnl_10(%79, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, %stage1_unit1_conv1_weight, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, %stage1_unit1_conv2_weight, %stage1_unit1_sc_weight, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, %stage1_unit2_conv1_weight, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, %stage1_unit2_conv2_weight, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, %stage2_unit1_conv1_weight, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, %stage2_unit1_conv2_weight, %stage2_unit1_sc_weight, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, %stage2_unit2_conv1_weight, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, %stage2_unit2_conv2_weight, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, %stage3_unit1_conv1_weight, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, %stage3_unit1_conv2_weight, %stage3_unit1_sc_weight, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, %stage3_unit2_conv1_weight, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, %stage3_unit2_conv2_weight, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, %stage4_unit1_conv1_weight, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, %stage4_unit1_conv2_weight, %stage4_unit1_sc_weight, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, %stage4_unit2_conv1_weight, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, %stage4_unit2_conv2_weight, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.global_avg_pool2d(%80) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %82 = nn.batch_flatten(%81) /* ty=Tensor[(1, 512), float32] */;
  %83 = @dnnl_99(%82, %fc1_weight) /* ty=Tensor[(1, 1000), float32] */;
  %84 = nn.bias_add(%83, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%84) /* ty=Tensor[(1, 1000), float32] */
}

def @dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], global_symbol="dnnl_0", Primitive=1, Compiler="dnnl", Inline=1) -> Tensor[(1, 64, 112, 112), float32] {
  %85 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %86 = %85.0;
  %87 = nn.conv2d(%86, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %88 = nn.batch_norm(%87, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %89 = %88.0;
  nn.relu(%89) /* ty=Tensor[(1, 64, 112, 112), float32] */
}

Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @tvmgen_default_dnnl_10(%dnnl_10_i0: Tensor[(1, 64, 56, 56), float32], %dnnl_10_i1: Tensor[(64), float32], %dnnl_10_i2: Tensor[(64), float32], %dnnl_10_i3: Tensor[(64), float32], %dnnl_10_i4: Tensor[(64), float32], %dnnl_10_i5: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i6: Tensor[(64), float32], %dnnl_10_i7: Tensor[(64), float32], %dnnl_10_i8: Tensor[(64), float32], %dnnl_10_i9: Tensor[(64), float32], %dnnl_10_i10: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i11: Tensor[(64, 64, 1, 1), float32], %dnnl_10_i12: Tensor[(64), float32], %dnnl_10_i13: Tensor[(64), float32], %dnnl_10_i14: Tensor[(64), float32], %dnnl_10_i15: Tensor[(64), float32], %dnnl_10_i16: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i17: Tensor[(64), float32], %dnnl_10_i18: Tensor[(64), float32], %dnnl_10_i19: Tensor[(64), float32], %dnnl_10_i20: Tensor[(64), float32], %dnnl_10_i21: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i22: Tensor[(64), float32], %dnnl_10_i23: Tensor[(64), float32], %dnnl_10_i24: Tensor[(64), float32], %dnnl_10_i25: Tensor[(64), float32], %dnnl_10_i26: Tensor[(128, 64, 3, 3), float32], %dnnl_10_i27: Tensor[(128), float32], %dnnl_10_i28: Tensor[(128), float32], %dnnl_10_i29: Tensor[(128), float32], %dnnl_10_i30: Tensor[(128), float32], %dnnl_10_i31: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i32: Tensor[(128, 64, 1, 1), float32], %dnnl_10_i33: Tensor[(128), float32], %dnnl_10_i34: Tensor[(128), float32], %dnnl_10_i35: Tensor[(128), float32], %dnnl_10_i36: Tensor[(128), float32], %dnnl_10_i37: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i38: Tensor[(128), float32], %dnnl_10_i39: Tensor[(128), float32], %dnnl_10_i40: Tensor[(128), float32], %dnnl_10_i41: Tensor[(128), float32], %dnnl_10_i42: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i43: Tensor[(128), float32], %dnnl_10_i44: Tensor[(128), float32], %dnnl_10_i45: Tensor[(128), float32], %dnnl_10_i46: Tensor[(128), float32], %dnnl_10_i47: Tensor[(256, 128, 3, 3), float32], %dnnl_10_i48: Tensor[(256), float32], %dnnl_10_i49: Tensor[(256), float32], %dnnl_10_i50: Tensor[(256), float32], %dnnl_10_i51: Tensor[(256), float32], %dnnl_10_i52: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i53: Tensor[(256, 128, 1, 1), float32], %dnnl_10_i54: Tensor[(256), float32], %dnnl_10_i55: Tensor[(256), float32], %dnnl_10_i56: Tensor[(256), float32], %dnnl_10_i57: Tensor[(256), float32], %dnnl_10_i58: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i59: Tensor[(256), float32], %dnnl_10_i60: Tensor[(256), float32], %dnnl_10_i61: Tensor[(256), float32], %dnnl_10_i62: Tensor[(256), float32], %dnnl_10_i63: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i64: Tensor[(256), float32], %dnnl_10_i65: Tensor[(256), float32], %dnnl_10_i66: Tensor[(256), float32], %dnnl_10_i67: Tensor[(256), float32], %dnnl_10_i68: Tensor[(512, 256, 3, 3), float32], %dnnl_10_i69: Tensor[(512), float32], %dnnl_10_i70: Tensor[(512), float32], %dnnl_10_i71: Tensor[(512), float32], %dnnl_10_i72: Tensor[(512), float32], %dnnl_10_i73: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i74: Tensor[(512, 256, 1, 1), float32], %dnnl_10_i75: Tensor[(512), float32], %dnnl_10_i76: Tensor[(512), float32], %dnnl_10_i77: Tensor[(512), float32], %dnnl_10_i78: Tensor[(512), float32], %dnnl_10_i79: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i80: Tensor[(512), float32], %dnnl_10_i81: Tensor[(512), float32], %dnnl_10_i82: Tensor[(512), float32], %dnnl_10_i83: Tensor[(512), float32], %dnnl_10_i84: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i85: Tensor[(512), float32], %dnnl_10_i86: Tensor[(512), float32], %dnnl_10_i87: Tensor[(512), float32], %dnnl_10_i88: Tensor[(512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_10", Primitive=1) -> Tensor[(1, 512, 7, 7), float32] {
  %0 = nn.batch_norm(%dnnl_10_i0, %dnnl_10_i1, %dnnl_10_i2, %dnnl_10_i3, %dnnl_10_i4, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %1 = %0.0;
  %2 = nn.relu(%1) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %3 = nn.conv2d(%2, %dnnl_10_i5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %4 = nn.batch_norm(%3, %dnnl_10_i6, %dnnl_10_i7, %dnnl_10_i8, %dnnl_10_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %5 = %4.0;
  %6 = nn.relu(%5) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %7 = nn.conv2d(%6, %dnnl_10_i10, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%2, %dnnl_10_i11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %9 = add(%7, %8) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %10 = nn.batch_norm(%9, %dnnl_10_i12, %dnnl_10_i13, %dnnl_10_i14, %dnnl_10_i15, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %11 = %10.0;
  %12 = nn.relu(%11) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %13 = nn.conv2d(%12, %dnnl_10_i16, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = nn.batch_norm(%13, %dnnl_10_i17, %dnnl_10_i18, %dnnl_10_i19, %dnnl_10_i20, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %15 = %14.0;
  %16 = nn.relu(%15) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %17 = nn.conv2d(%16, %dnnl_10_i21, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = add(%17, %9) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = nn.batch_norm(%18, %dnnl_10_i22, %dnnl_10_i23, %dnnl_10_i24, %dnnl_10_i25, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = %19.0;
  %21 = nn.relu(%20) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, %dnnl_10_i26, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %23 = nn.batch_norm(%22, %dnnl_10_i27, %dnnl_10_i28, %dnnl_10_i29, %dnnl_10_i30, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %24 = %23.0;
  %25 = nn.relu(%24) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %26 = nn.conv2d(%25, %dnnl_10_i31, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %27 = nn.conv2d(%21, %dnnl_10_i32, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %28 = add(%26, %27) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %29 = nn.batch_norm(%28, %dnnl_10_i33, %dnnl_10_i34, %dnnl_10_i35, %dnnl_10_i36, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %30 = %29.0;
  %31 = nn.relu(%30) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %32 = nn.conv2d(%31, %dnnl_10_i37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = nn.batch_norm(%32, %dnnl_10_i38, %dnnl_10_i39, %dnnl_10_i40, %dnnl_10_i41, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %34 = %33.0;
  %35 = nn.relu(%34) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %36 = nn.conv2d(%35, %dnnl_10_i42, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %37 = add(%36, %28) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %38 = nn.batch_norm(%37, %dnnl_10_i43, %dnnl_10_i44, %dnnl_10_i45, %dnnl_10_i46, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %39 = %38.0;
  %40 = nn.relu(%39) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %41 = nn.conv2d(%40, %dnnl_10_i47, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %42 = nn.batch_norm(%41, %dnnl_10_i48, %dnnl_10_i49, %dnnl_10_i50, %dnnl_10_i51, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %43 = %42.0;
  %44 = nn.relu(%43) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %45 = nn.conv2d(%44, %dnnl_10_i52, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %46 = nn.conv2d(%40, %dnnl_10_i53, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %47 = add(%45, %46) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %48 = nn.batch_norm(%47, %dnnl_10_i54, %dnnl_10_i55, %dnnl_10_i56, %dnnl_10_i57, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %49 = %48.0;
  %50 = nn.relu(%49) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %51 = nn.conv2d(%50, %dnnl_10_i58, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = nn.batch_norm(%51, %dnnl_10_i59, %dnnl_10_i60, %dnnl_10_i61, %dnnl_10_i62, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %53 = %52.0;
  %54 = nn.relu(%53) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %55 = nn.conv2d(%54, %dnnl_10_i63, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %56 = add(%55, %47) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %57 = nn.batch_norm(%56, %dnnl_10_i64, %dnnl_10_i65, %dnnl_10_i66, %dnnl_10_i67, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %58 = %57.0;
  %59 = nn.relu(%58) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %60 = nn.conv2d(%59, %dnnl_10_i68, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %61 = nn.batch_norm(%60, %dnnl_10_i69, %dnnl_10_i70, %dnnl_10_i71, %dnnl_10_i72, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %62 = %61.0;
  %63 = nn.relu(%62) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %64 = nn.conv2d(%63, %dnnl_10_i73, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %65 = nn.conv2d(%59, %dnnl_10_i74, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %66 = add(%64, %65) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %67 = nn.batch_norm(%66, %dnnl_10_i75, %dnnl_10_i76, %dnnl_10_i77, %dnnl_10_i78, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %68 = %67.0;
  %69 = nn.relu(%68) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %70 = nn.conv2d(%69, %dnnl_10_i79, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = nn.batch_norm(%70, %dnnl_10_i80, %dnnl_10_i81, %dnnl_10_i82, %dnnl_10_i83, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %72 = %71.0;
  %73 = nn.relu(%72) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %74 = nn.conv2d(%73, %dnnl_10_i84, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %75 = add(%74, %66) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %76 = nn.batch_norm(%75, %dnnl_10_i85, %dnnl_10_i86, %dnnl_10_i87, %dnnl_10_i88, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %77 = %76.0;
  nn.relu(%77) /* ty=Tensor[(1, 512, 7, 7), float32] */
}

def @tvmgen_default_dnnl_99(%dnnl_99_i0: Tensor[(1, 512), float32], %dnnl_99_i1: Tensor[(1000, 512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_99", Primitive=1) -> Tensor[(1, 1000), float32] {
  nn.dense(%dnnl_99_i0, %dnnl_99_i1, units=1000) /* ty=Tensor[(1, 1000), float32] */
}

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %78 = @tvmgen_default_dnnl_0(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, %conv0_weight, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var);
  %79 = nn.max_pool2d(%78, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]);
  %80 = @tvmgen_default_dnnl_10(%79, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, %stage1_unit1_conv1_weight, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, %stage1_unit1_conv2_weight, %stage1_unit1_sc_weight, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, %stage1_unit2_conv1_weight, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, %stage1_unit2_conv2_weight, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, %stage2_unit1_conv1_weight, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, %stage2_unit1_conv2_weight, %stage2_unit1_sc_weight, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, %stage2_unit2_conv1_weight, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, %stage2_unit2_conv2_weight, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, %stage3_unit1_conv1_weight, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, %stage3_unit1_conv2_weight, %stage3_unit1_sc_weight, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, %stage3_unit2_conv1_weight, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, %stage3_unit2_conv2_weight, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, %stage4_unit1_conv1_weight, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, %stage4_unit1_conv2_weight, %stage4_unit1_sc_weight, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, %stage4_unit2_conv1_weight, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, %stage4_unit2_conv2_weight, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var);
  %81 = nn.global_avg_pool2d(%80);
  %82 = nn.batch_flatten(%81);
  %83 = @tvmgen_default_dnnl_99(%82, %fc1_weight);
  %84 = nn.bias_add(%83, %fc1_bias, axis=-1);
  nn.softmax(%84)
}

def @tvmgen_default_dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_0", Primitive=1) -> Tensor[(1, 64, 112, 112), float32] {
  %85 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %86 = %85.0;
  %87 = nn.conv2d(%86, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %88 = nn.batch_norm(%87, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %89 = %88.0;
  nn.relu(%89) /* ty=Tensor[(1, 64, 112, 112), float32] */
}

Running pass: {} The meta data of the pass: pass name: RemoveUnusedFunctionsopt_level: 1required passes: [
]

def @tvmgen_default_dnnl_99(%dnnl_99_i0: Tensor[(1, 512), float32], %dnnl_99_i1: Tensor[(1000, 512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_99", Primitive=1) -> Tensor[(1, 1000), float32] {
  nn.dense(%dnnl_99_i0, %dnnl_99_i1, units=1000) /* ty=Tensor[(1, 1000), float32] */
}

def @tvmgen_default_dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_0", Primitive=1) -> Tensor[(1, 64, 112, 112), float32] {
  %0 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */
}

def @tvmgen_default_dnnl_10(%dnnl_10_i0: Tensor[(1, 64, 56, 56), float32], %dnnl_10_i1: Tensor[(64), float32], %dnnl_10_i2: Tensor[(64), float32], %dnnl_10_i3: Tensor[(64), float32], %dnnl_10_i4: Tensor[(64), float32], %dnnl_10_i5: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i6: Tensor[(64), float32], %dnnl_10_i7: Tensor[(64), float32], %dnnl_10_i8: Tensor[(64), float32], %dnnl_10_i9: Tensor[(64), float32], %dnnl_10_i10: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i11: Tensor[(64, 64, 1, 1), float32], %dnnl_10_i12: Tensor[(64), float32], %dnnl_10_i13: Tensor[(64), float32], %dnnl_10_i14: Tensor[(64), float32], %dnnl_10_i15: Tensor[(64), float32], %dnnl_10_i16: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i17: Tensor[(64), float32], %dnnl_10_i18: Tensor[(64), float32], %dnnl_10_i19: Tensor[(64), float32], %dnnl_10_i20: Tensor[(64), float32], %dnnl_10_i21: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i22: Tensor[(64), float32], %dnnl_10_i23: Tensor[(64), float32], %dnnl_10_i24: Tensor[(64), float32], %dnnl_10_i25: Tensor[(64), float32], %dnnl_10_i26: Tensor[(128, 64, 3, 3), float32], %dnnl_10_i27: Tensor[(128), float32], %dnnl_10_i28: Tensor[(128), float32], %dnnl_10_i29: Tensor[(128), float32], %dnnl_10_i30: Tensor[(128), float32], %dnnl_10_i31: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i32: Tensor[(128, 64, 1, 1), float32], %dnnl_10_i33: Tensor[(128), float32], %dnnl_10_i34: Tensor[(128), float32], %dnnl_10_i35: Tensor[(128), float32], %dnnl_10_i36: Tensor[(128), float32], %dnnl_10_i37: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i38: Tensor[(128), float32], %dnnl_10_i39: Tensor[(128), float32], %dnnl_10_i40: Tensor[(128), float32], %dnnl_10_i41: Tensor[(128), float32], %dnnl_10_i42: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i43: Tensor[(128), float32], %dnnl_10_i44: Tensor[(128), float32], %dnnl_10_i45: Tensor[(128), float32], %dnnl_10_i46: Tensor[(128), float32], %dnnl_10_i47: Tensor[(256, 128, 3, 3), float32], %dnnl_10_i48: Tensor[(256), float32], %dnnl_10_i49: Tensor[(256), float32], %dnnl_10_i50: Tensor[(256), float32], %dnnl_10_i51: Tensor[(256), float32], %dnnl_10_i52: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i53: Tensor[(256, 128, 1, 1), float32], %dnnl_10_i54: Tensor[(256), float32], %dnnl_10_i55: Tensor[(256), float32], %dnnl_10_i56: Tensor[(256), float32], %dnnl_10_i57: Tensor[(256), float32], %dnnl_10_i58: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i59: Tensor[(256), float32], %dnnl_10_i60: Tensor[(256), float32], %dnnl_10_i61: Tensor[(256), float32], %dnnl_10_i62: Tensor[(256), float32], %dnnl_10_i63: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i64: Tensor[(256), float32], %dnnl_10_i65: Tensor[(256), float32], %dnnl_10_i66: Tensor[(256), float32], %dnnl_10_i67: Tensor[(256), float32], %dnnl_10_i68: Tensor[(512, 256, 3, 3), float32], %dnnl_10_i69: Tensor[(512), float32], %dnnl_10_i70: Tensor[(512), float32], %dnnl_10_i71: Tensor[(512), float32], %dnnl_10_i72: Tensor[(512), float32], %dnnl_10_i73: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i74: Tensor[(512, 256, 1, 1), float32], %dnnl_10_i75: Tensor[(512), float32], %dnnl_10_i76: Tensor[(512), float32], %dnnl_10_i77: Tensor[(512), float32], %dnnl_10_i78: Tensor[(512), float32], %dnnl_10_i79: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i80: Tensor[(512), float32], %dnnl_10_i81: Tensor[(512), float32], %dnnl_10_i82: Tensor[(512), float32], %dnnl_10_i83: Tensor[(512), float32], %dnnl_10_i84: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i85: Tensor[(512), float32], %dnnl_10_i86: Tensor[(512), float32], %dnnl_10_i87: Tensor[(512), float32], %dnnl_10_i88: Tensor[(512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_10", Primitive=1) -> Tensor[(1, 512, 7, 7), float32] {
  %5 = nn.batch_norm(%dnnl_10_i0, %dnnl_10_i1, %dnnl_10_i2, %dnnl_10_i3, %dnnl_10_i4, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %6 = %5.0;
  %7 = nn.relu(%6) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%7, %dnnl_10_i5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %9 = nn.batch_norm(%8, %dnnl_10_i6, %dnnl_10_i7, %dnnl_10_i8, %dnnl_10_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %10 = %9.0;
  %11 = nn.relu(%10) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %12 = nn.conv2d(%11, %dnnl_10_i10, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %13 = nn.conv2d(%7, %dnnl_10_i11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = add(%12, %13) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.batch_norm(%14, %dnnl_10_i12, %dnnl_10_i13, %dnnl_10_i14, %dnnl_10_i15, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %16 = %15.0;
  %17 = nn.relu(%16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = nn.conv2d(%17, %dnnl_10_i16, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = nn.batch_norm(%18, %dnnl_10_i17, %dnnl_10_i18, %dnnl_10_i19, %dnnl_10_i20, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = %19.0;
  %21 = nn.relu(%20) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, %dnnl_10_i21, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %23 = add(%22, %14) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.batch_norm(%23, %dnnl_10_i22, %dnnl_10_i23, %dnnl_10_i24, %dnnl_10_i25, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %25 = %24.0;
  %26 = nn.relu(%25) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %27 = nn.conv2d(%26, %dnnl_10_i26, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %28 = nn.batch_norm(%27, %dnnl_10_i27, %dnnl_10_i28, %dnnl_10_i29, %dnnl_10_i30, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %29 = %28.0;
  %30 = nn.relu(%29) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %31 = nn.conv2d(%30, %dnnl_10_i31, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %32 = nn.conv2d(%26, %dnnl_10_i32, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = add(%31, %32) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.batch_norm(%33, %dnnl_10_i33, %dnnl_10_i34, %dnnl_10_i35, %dnnl_10_i36, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %35 = %34.0;
  %36 = nn.relu(%35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %37 = nn.conv2d(%36, %dnnl_10_i37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %38 = nn.batch_norm(%37, %dnnl_10_i38, %dnnl_10_i39, %dnnl_10_i40, %dnnl_10_i41, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %39 = %38.0;
  %40 = nn.relu(%39) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %41 = nn.conv2d(%40, %dnnl_10_i42, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %42 = add(%41, %33) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.batch_norm(%42, %dnnl_10_i43, %dnnl_10_i44, %dnnl_10_i45, %dnnl_10_i46, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %44 = %43.0;
  %45 = nn.relu(%44) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %46 = nn.conv2d(%45, %dnnl_10_i47, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %47 = nn.batch_norm(%46, %dnnl_10_i48, %dnnl_10_i49, %dnnl_10_i50, %dnnl_10_i51, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %48 = %47.0;
  %49 = nn.relu(%48) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %50 = nn.conv2d(%49, %dnnl_10_i52, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %51 = nn.conv2d(%45, %dnnl_10_i53, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = add(%50, %51) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.batch_norm(%52, %dnnl_10_i54, %dnnl_10_i55, %dnnl_10_i56, %dnnl_10_i57, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %54 = %53.0;
  %55 = nn.relu(%54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %56 = nn.conv2d(%55, %dnnl_10_i58, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %57 = nn.batch_norm(%56, %dnnl_10_i59, %dnnl_10_i60, %dnnl_10_i61, %dnnl_10_i62, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %58 = %57.0;
  %59 = nn.relu(%58) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %60 = nn.conv2d(%59, %dnnl_10_i63, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %61 = add(%60, %52) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.batch_norm(%61, %dnnl_10_i64, %dnnl_10_i65, %dnnl_10_i66, %dnnl_10_i67, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %63 = %62.0;
  %64 = nn.relu(%63) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %65 = nn.conv2d(%64, %dnnl_10_i68, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %66 = nn.batch_norm(%65, %dnnl_10_i69, %dnnl_10_i70, %dnnl_10_i71, %dnnl_10_i72, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %67 = %66.0;
  %68 = nn.relu(%67) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %69 = nn.conv2d(%68, %dnnl_10_i73, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %70 = nn.conv2d(%64, %dnnl_10_i74, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = add(%69, %70) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.batch_norm(%71, %dnnl_10_i75, %dnnl_10_i76, %dnnl_10_i77, %dnnl_10_i78, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %73 = %72.0;
  %74 = nn.relu(%73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %75 = nn.conv2d(%74, %dnnl_10_i79, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %76 = nn.batch_norm(%75, %dnnl_10_i80, %dnnl_10_i81, %dnnl_10_i82, %dnnl_10_i83, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %77 = %76.0;
  %78 = nn.relu(%77) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %79 = nn.conv2d(%78, %dnnl_10_i84, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %80 = add(%79, %71) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.batch_norm(%80, %dnnl_10_i85, %dnnl_10_i86, %dnnl_10_i87, %dnnl_10_i88, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %82 = %81.0;
  nn.relu(%82) /* ty=Tensor[(1, 512, 7, 7), float32] */
}

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %83 = @tvmgen_default_dnnl_0(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, %conv0_weight, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %84 = nn.max_pool2d(%83, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %85 = @tvmgen_default_dnnl_10(%84, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, %stage1_unit1_conv1_weight, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, %stage1_unit1_conv2_weight, %stage1_unit1_sc_weight, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, %stage1_unit2_conv1_weight, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, %stage1_unit2_conv2_weight, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, %stage2_unit1_conv1_weight, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, %stage2_unit1_conv2_weight, %stage2_unit1_sc_weight, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, %stage2_unit2_conv1_weight, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, %stage2_unit2_conv2_weight, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, %stage3_unit1_conv1_weight, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, %stage3_unit1_conv2_weight, %stage3_unit1_sc_weight, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, %stage3_unit2_conv1_weight, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, %stage3_unit2_conv2_weight, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, %stage4_unit1_conv1_weight, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, %stage4_unit1_conv2_weight, %stage4_unit1_sc_weight, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, %stage4_unit2_conv1_weight, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, %stage4_unit2_conv2_weight, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = @tvmgen_default_dnnl_99(%87, %fc1_weight) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @tvmgen_default_dnnl_99(%dnnl_99_i0: Tensor[(1, 512), float32], %dnnl_99_i1: Tensor[(1000, 512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_99", Primitive=1) -> Tensor[(1, 1000), float32] {
  nn.dense(%dnnl_99_i0, %dnnl_99_i1, units=1000) /* ty=Tensor[(1, 1000), float32] */
}

def @tvmgen_default_dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_0", Primitive=1) -> Tensor[(1, 64, 112, 112), float32] {
  %0 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */
}

def @tvmgen_default_dnnl_10(%dnnl_10_i0: Tensor[(1, 64, 56, 56), float32], %dnnl_10_i1: Tensor[(64), float32], %dnnl_10_i2: Tensor[(64), float32], %dnnl_10_i3: Tensor[(64), float32], %dnnl_10_i4: Tensor[(64), float32], %dnnl_10_i5: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i6: Tensor[(64), float32], %dnnl_10_i7: Tensor[(64), float32], %dnnl_10_i8: Tensor[(64), float32], %dnnl_10_i9: Tensor[(64), float32], %dnnl_10_i10: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i11: Tensor[(64, 64, 1, 1), float32], %dnnl_10_i12: Tensor[(64), float32], %dnnl_10_i13: Tensor[(64), float32], %dnnl_10_i14: Tensor[(64), float32], %dnnl_10_i15: Tensor[(64), float32], %dnnl_10_i16: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i17: Tensor[(64), float32], %dnnl_10_i18: Tensor[(64), float32], %dnnl_10_i19: Tensor[(64), float32], %dnnl_10_i20: Tensor[(64), float32], %dnnl_10_i21: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i22: Tensor[(64), float32], %dnnl_10_i23: Tensor[(64), float32], %dnnl_10_i24: Tensor[(64), float32], %dnnl_10_i25: Tensor[(64), float32], %dnnl_10_i26: Tensor[(128, 64, 3, 3), float32], %dnnl_10_i27: Tensor[(128), float32], %dnnl_10_i28: Tensor[(128), float32], %dnnl_10_i29: Tensor[(128), float32], %dnnl_10_i30: Tensor[(128), float32], %dnnl_10_i31: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i32: Tensor[(128, 64, 1, 1), float32], %dnnl_10_i33: Tensor[(128), float32], %dnnl_10_i34: Tensor[(128), float32], %dnnl_10_i35: Tensor[(128), float32], %dnnl_10_i36: Tensor[(128), float32], %dnnl_10_i37: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i38: Tensor[(128), float32], %dnnl_10_i39: Tensor[(128), float32], %dnnl_10_i40: Tensor[(128), float32], %dnnl_10_i41: Tensor[(128), float32], %dnnl_10_i42: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i43: Tensor[(128), float32], %dnnl_10_i44: Tensor[(128), float32], %dnnl_10_i45: Tensor[(128), float32], %dnnl_10_i46: Tensor[(128), float32], %dnnl_10_i47: Tensor[(256, 128, 3, 3), float32], %dnnl_10_i48: Tensor[(256), float32], %dnnl_10_i49: Tensor[(256), float32], %dnnl_10_i50: Tensor[(256), float32], %dnnl_10_i51: Tensor[(256), float32], %dnnl_10_i52: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i53: Tensor[(256, 128, 1, 1), float32], %dnnl_10_i54: Tensor[(256), float32], %dnnl_10_i55: Tensor[(256), float32], %dnnl_10_i56: Tensor[(256), float32], %dnnl_10_i57: Tensor[(256), float32], %dnnl_10_i58: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i59: Tensor[(256), float32], %dnnl_10_i60: Tensor[(256), float32], %dnnl_10_i61: Tensor[(256), float32], %dnnl_10_i62: Tensor[(256), float32], %dnnl_10_i63: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i64: Tensor[(256), float32], %dnnl_10_i65: Tensor[(256), float32], %dnnl_10_i66: Tensor[(256), float32], %dnnl_10_i67: Tensor[(256), float32], %dnnl_10_i68: Tensor[(512, 256, 3, 3), float32], %dnnl_10_i69: Tensor[(512), float32], %dnnl_10_i70: Tensor[(512), float32], %dnnl_10_i71: Tensor[(512), float32], %dnnl_10_i72: Tensor[(512), float32], %dnnl_10_i73: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i74: Tensor[(512, 256, 1, 1), float32], %dnnl_10_i75: Tensor[(512), float32], %dnnl_10_i76: Tensor[(512), float32], %dnnl_10_i77: Tensor[(512), float32], %dnnl_10_i78: Tensor[(512), float32], %dnnl_10_i79: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i80: Tensor[(512), float32], %dnnl_10_i81: Tensor[(512), float32], %dnnl_10_i82: Tensor[(512), float32], %dnnl_10_i83: Tensor[(512), float32], %dnnl_10_i84: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i85: Tensor[(512), float32], %dnnl_10_i86: Tensor[(512), float32], %dnnl_10_i87: Tensor[(512), float32], %dnnl_10_i88: Tensor[(512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_10", Primitive=1) -> Tensor[(1, 512, 7, 7), float32] {
  %5 = nn.batch_norm(%dnnl_10_i0, %dnnl_10_i1, %dnnl_10_i2, %dnnl_10_i3, %dnnl_10_i4, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %6 = %5.0;
  %7 = nn.relu(%6) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%7, %dnnl_10_i5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %9 = nn.batch_norm(%8, %dnnl_10_i6, %dnnl_10_i7, %dnnl_10_i8, %dnnl_10_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %10 = %9.0;
  %11 = nn.relu(%10) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %12 = nn.conv2d(%11, %dnnl_10_i10, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %13 = nn.conv2d(%7, %dnnl_10_i11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = add(%12, %13) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.batch_norm(%14, %dnnl_10_i12, %dnnl_10_i13, %dnnl_10_i14, %dnnl_10_i15, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %16 = %15.0;
  %17 = nn.relu(%16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = nn.conv2d(%17, %dnnl_10_i16, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = nn.batch_norm(%18, %dnnl_10_i17, %dnnl_10_i18, %dnnl_10_i19, %dnnl_10_i20, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = %19.0;
  %21 = nn.relu(%20) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, %dnnl_10_i21, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %23 = add(%22, %14) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.batch_norm(%23, %dnnl_10_i22, %dnnl_10_i23, %dnnl_10_i24, %dnnl_10_i25, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %25 = %24.0;
  %26 = nn.relu(%25) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %27 = nn.conv2d(%26, %dnnl_10_i26, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %28 = nn.batch_norm(%27, %dnnl_10_i27, %dnnl_10_i28, %dnnl_10_i29, %dnnl_10_i30, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %29 = %28.0;
  %30 = nn.relu(%29) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %31 = nn.conv2d(%30, %dnnl_10_i31, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %32 = nn.conv2d(%26, %dnnl_10_i32, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = add(%31, %32) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.batch_norm(%33, %dnnl_10_i33, %dnnl_10_i34, %dnnl_10_i35, %dnnl_10_i36, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %35 = %34.0;
  %36 = nn.relu(%35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %37 = nn.conv2d(%36, %dnnl_10_i37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %38 = nn.batch_norm(%37, %dnnl_10_i38, %dnnl_10_i39, %dnnl_10_i40, %dnnl_10_i41, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %39 = %38.0;
  %40 = nn.relu(%39) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %41 = nn.conv2d(%40, %dnnl_10_i42, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %42 = add(%41, %33) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.batch_norm(%42, %dnnl_10_i43, %dnnl_10_i44, %dnnl_10_i45, %dnnl_10_i46, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %44 = %43.0;
  %45 = nn.relu(%44) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %46 = nn.conv2d(%45, %dnnl_10_i47, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %47 = nn.batch_norm(%46, %dnnl_10_i48, %dnnl_10_i49, %dnnl_10_i50, %dnnl_10_i51, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %48 = %47.0;
  %49 = nn.relu(%48) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %50 = nn.conv2d(%49, %dnnl_10_i52, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %51 = nn.conv2d(%45, %dnnl_10_i53, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = add(%50, %51) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.batch_norm(%52, %dnnl_10_i54, %dnnl_10_i55, %dnnl_10_i56, %dnnl_10_i57, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %54 = %53.0;
  %55 = nn.relu(%54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %56 = nn.conv2d(%55, %dnnl_10_i58, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %57 = nn.batch_norm(%56, %dnnl_10_i59, %dnnl_10_i60, %dnnl_10_i61, %dnnl_10_i62, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %58 = %57.0;
  %59 = nn.relu(%58) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %60 = nn.conv2d(%59, %dnnl_10_i63, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %61 = add(%60, %52) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.batch_norm(%61, %dnnl_10_i64, %dnnl_10_i65, %dnnl_10_i66, %dnnl_10_i67, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %63 = %62.0;
  %64 = nn.relu(%63) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %65 = nn.conv2d(%64, %dnnl_10_i68, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %66 = nn.batch_norm(%65, %dnnl_10_i69, %dnnl_10_i70, %dnnl_10_i71, %dnnl_10_i72, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %67 = %66.0;
  %68 = nn.relu(%67) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %69 = nn.conv2d(%68, %dnnl_10_i73, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %70 = nn.conv2d(%64, %dnnl_10_i74, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = add(%69, %70) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.batch_norm(%71, %dnnl_10_i75, %dnnl_10_i76, %dnnl_10_i77, %dnnl_10_i78, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %73 = %72.0;
  %74 = nn.relu(%73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %75 = nn.conv2d(%74, %dnnl_10_i79, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %76 = nn.batch_norm(%75, %dnnl_10_i80, %dnnl_10_i81, %dnnl_10_i82, %dnnl_10_i83, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %77 = %76.0;
  %78 = nn.relu(%77) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %79 = nn.conv2d(%78, %dnnl_10_i84, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %80 = add(%79, %71) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.batch_norm(%80, %dnnl_10_i85, %dnnl_10_i86, %dnnl_10_i87, %dnnl_10_i88, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %82 = %81.0;
  nn.relu(%82) /* ty=Tensor[(1, 512, 7, 7), float32] */
}

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %83 = @tvmgen_default_dnnl_0(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, %conv0_weight, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %84 = nn.max_pool2d(%83, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %85 = @tvmgen_default_dnnl_10(%84, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, %stage1_unit1_conv1_weight, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, %stage1_unit1_conv2_weight, %stage1_unit1_sc_weight, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, %stage1_unit2_conv1_weight, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, %stage1_unit2_conv2_weight, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, %stage2_unit1_conv1_weight, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, %stage2_unit1_conv2_weight, %stage2_unit1_sc_weight, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, %stage2_unit2_conv1_weight, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, %stage2_unit2_conv2_weight, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, %stage3_unit1_conv1_weight, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, %stage3_unit1_conv2_weight, %stage3_unit1_sc_weight, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, %stage3_unit2_conv1_weight, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, %stage3_unit2_conv2_weight, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, %stage4_unit1_conv1_weight, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, %stage4_unit1_conv2_weight, %stage4_unit1_sc_weight, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, %stage4_unit2_conv1_weight, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, %stage4_unit2_conv2_weight, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = @tvmgen_default_dnnl_99(%87, %fc1_weight) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: AlterOpLayoutopt_level: 3required passes: [
InferType, ]

def @tvmgen_default_dnnl_99(%dnnl_99_i0: Tensor[(1, 512), float32], %dnnl_99_i1: Tensor[(1000, 512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_99", Primitive=1) -> Tensor[(1, 1000), float32] {
  nn.dense(%dnnl_99_i0, %dnnl_99_i1, units=1000) /* ty=Tensor[(1, 1000), float32] */
}

def @tvmgen_default_dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_0", Primitive=1) -> Tensor[(1, 64, 112, 112), float32] {
  %0 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */
}

def @tvmgen_default_dnnl_10(%dnnl_10_i0: Tensor[(1, 64, 56, 56), float32], %dnnl_10_i1: Tensor[(64), float32], %dnnl_10_i2: Tensor[(64), float32], %dnnl_10_i3: Tensor[(64), float32], %dnnl_10_i4: Tensor[(64), float32], %dnnl_10_i5: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i6: Tensor[(64), float32], %dnnl_10_i7: Tensor[(64), float32], %dnnl_10_i8: Tensor[(64), float32], %dnnl_10_i9: Tensor[(64), float32], %dnnl_10_i10: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i11: Tensor[(64, 64, 1, 1), float32], %dnnl_10_i12: Tensor[(64), float32], %dnnl_10_i13: Tensor[(64), float32], %dnnl_10_i14: Tensor[(64), float32], %dnnl_10_i15: Tensor[(64), float32], %dnnl_10_i16: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i17: Tensor[(64), float32], %dnnl_10_i18: Tensor[(64), float32], %dnnl_10_i19: Tensor[(64), float32], %dnnl_10_i20: Tensor[(64), float32], %dnnl_10_i21: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i22: Tensor[(64), float32], %dnnl_10_i23: Tensor[(64), float32], %dnnl_10_i24: Tensor[(64), float32], %dnnl_10_i25: Tensor[(64), float32], %dnnl_10_i26: Tensor[(128, 64, 3, 3), float32], %dnnl_10_i27: Tensor[(128), float32], %dnnl_10_i28: Tensor[(128), float32], %dnnl_10_i29: Tensor[(128), float32], %dnnl_10_i30: Tensor[(128), float32], %dnnl_10_i31: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i32: Tensor[(128, 64, 1, 1), float32], %dnnl_10_i33: Tensor[(128), float32], %dnnl_10_i34: Tensor[(128), float32], %dnnl_10_i35: Tensor[(128), float32], %dnnl_10_i36: Tensor[(128), float32], %dnnl_10_i37: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i38: Tensor[(128), float32], %dnnl_10_i39: Tensor[(128), float32], %dnnl_10_i40: Tensor[(128), float32], %dnnl_10_i41: Tensor[(128), float32], %dnnl_10_i42: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i43: Tensor[(128), float32], %dnnl_10_i44: Tensor[(128), float32], %dnnl_10_i45: Tensor[(128), float32], %dnnl_10_i46: Tensor[(128), float32], %dnnl_10_i47: Tensor[(256, 128, 3, 3), float32], %dnnl_10_i48: Tensor[(256), float32], %dnnl_10_i49: Tensor[(256), float32], %dnnl_10_i50: Tensor[(256), float32], %dnnl_10_i51: Tensor[(256), float32], %dnnl_10_i52: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i53: Tensor[(256, 128, 1, 1), float32], %dnnl_10_i54: Tensor[(256), float32], %dnnl_10_i55: Tensor[(256), float32], %dnnl_10_i56: Tensor[(256), float32], %dnnl_10_i57: Tensor[(256), float32], %dnnl_10_i58: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i59: Tensor[(256), float32], %dnnl_10_i60: Tensor[(256), float32], %dnnl_10_i61: Tensor[(256), float32], %dnnl_10_i62: Tensor[(256), float32], %dnnl_10_i63: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i64: Tensor[(256), float32], %dnnl_10_i65: Tensor[(256), float32], %dnnl_10_i66: Tensor[(256), float32], %dnnl_10_i67: Tensor[(256), float32], %dnnl_10_i68: Tensor[(512, 256, 3, 3), float32], %dnnl_10_i69: Tensor[(512), float32], %dnnl_10_i70: Tensor[(512), float32], %dnnl_10_i71: Tensor[(512), float32], %dnnl_10_i72: Tensor[(512), float32], %dnnl_10_i73: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i74: Tensor[(512, 256, 1, 1), float32], %dnnl_10_i75: Tensor[(512), float32], %dnnl_10_i76: Tensor[(512), float32], %dnnl_10_i77: Tensor[(512), float32], %dnnl_10_i78: Tensor[(512), float32], %dnnl_10_i79: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i80: Tensor[(512), float32], %dnnl_10_i81: Tensor[(512), float32], %dnnl_10_i82: Tensor[(512), float32], %dnnl_10_i83: Tensor[(512), float32], %dnnl_10_i84: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i85: Tensor[(512), float32], %dnnl_10_i86: Tensor[(512), float32], %dnnl_10_i87: Tensor[(512), float32], %dnnl_10_i88: Tensor[(512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_10", Primitive=1) -> Tensor[(1, 512, 7, 7), float32] {
  %5 = nn.batch_norm(%dnnl_10_i0, %dnnl_10_i1, %dnnl_10_i2, %dnnl_10_i3, %dnnl_10_i4, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %6 = %5.0;
  %7 = nn.relu(%6) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%7, %dnnl_10_i5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %9 = nn.batch_norm(%8, %dnnl_10_i6, %dnnl_10_i7, %dnnl_10_i8, %dnnl_10_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %10 = %9.0;
  %11 = nn.relu(%10) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %12 = nn.conv2d(%11, %dnnl_10_i10, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %13 = nn.conv2d(%7, %dnnl_10_i11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = add(%12, %13) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.batch_norm(%14, %dnnl_10_i12, %dnnl_10_i13, %dnnl_10_i14, %dnnl_10_i15, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %16 = %15.0;
  %17 = nn.relu(%16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = nn.conv2d(%17, %dnnl_10_i16, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = nn.batch_norm(%18, %dnnl_10_i17, %dnnl_10_i18, %dnnl_10_i19, %dnnl_10_i20, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = %19.0;
  %21 = nn.relu(%20) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, %dnnl_10_i21, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %23 = add(%22, %14) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.batch_norm(%23, %dnnl_10_i22, %dnnl_10_i23, %dnnl_10_i24, %dnnl_10_i25, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %25 = %24.0;
  %26 = nn.relu(%25) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %27 = nn.conv2d(%26, %dnnl_10_i26, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %28 = nn.batch_norm(%27, %dnnl_10_i27, %dnnl_10_i28, %dnnl_10_i29, %dnnl_10_i30, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %29 = %28.0;
  %30 = nn.relu(%29) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %31 = nn.conv2d(%30, %dnnl_10_i31, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %32 = nn.conv2d(%26, %dnnl_10_i32, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = add(%31, %32) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.batch_norm(%33, %dnnl_10_i33, %dnnl_10_i34, %dnnl_10_i35, %dnnl_10_i36, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %35 = %34.0;
  %36 = nn.relu(%35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %37 = nn.conv2d(%36, %dnnl_10_i37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %38 = nn.batch_norm(%37, %dnnl_10_i38, %dnnl_10_i39, %dnnl_10_i40, %dnnl_10_i41, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %39 = %38.0;
  %40 = nn.relu(%39) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %41 = nn.conv2d(%40, %dnnl_10_i42, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %42 = add(%41, %33) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.batch_norm(%42, %dnnl_10_i43, %dnnl_10_i44, %dnnl_10_i45, %dnnl_10_i46, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %44 = %43.0;
  %45 = nn.relu(%44) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %46 = nn.conv2d(%45, %dnnl_10_i47, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %47 = nn.batch_norm(%46, %dnnl_10_i48, %dnnl_10_i49, %dnnl_10_i50, %dnnl_10_i51, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %48 = %47.0;
  %49 = nn.relu(%48) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %50 = nn.conv2d(%49, %dnnl_10_i52, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %51 = nn.conv2d(%45, %dnnl_10_i53, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = add(%50, %51) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.batch_norm(%52, %dnnl_10_i54, %dnnl_10_i55, %dnnl_10_i56, %dnnl_10_i57, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %54 = %53.0;
  %55 = nn.relu(%54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %56 = nn.conv2d(%55, %dnnl_10_i58, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %57 = nn.batch_norm(%56, %dnnl_10_i59, %dnnl_10_i60, %dnnl_10_i61, %dnnl_10_i62, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %58 = %57.0;
  %59 = nn.relu(%58) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %60 = nn.conv2d(%59, %dnnl_10_i63, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %61 = add(%60, %52) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.batch_norm(%61, %dnnl_10_i64, %dnnl_10_i65, %dnnl_10_i66, %dnnl_10_i67, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %63 = %62.0;
  %64 = nn.relu(%63) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %65 = nn.conv2d(%64, %dnnl_10_i68, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %66 = nn.batch_norm(%65, %dnnl_10_i69, %dnnl_10_i70, %dnnl_10_i71, %dnnl_10_i72, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %67 = %66.0;
  %68 = nn.relu(%67) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %69 = nn.conv2d(%68, %dnnl_10_i73, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %70 = nn.conv2d(%64, %dnnl_10_i74, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = add(%69, %70) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.batch_norm(%71, %dnnl_10_i75, %dnnl_10_i76, %dnnl_10_i77, %dnnl_10_i78, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %73 = %72.0;
  %74 = nn.relu(%73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %75 = nn.conv2d(%74, %dnnl_10_i79, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %76 = nn.batch_norm(%75, %dnnl_10_i80, %dnnl_10_i81, %dnnl_10_i82, %dnnl_10_i83, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %77 = %76.0;
  %78 = nn.relu(%77) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %79 = nn.conv2d(%78, %dnnl_10_i84, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %80 = add(%79, %71) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.batch_norm(%80, %dnnl_10_i85, %dnnl_10_i86, %dnnl_10_i87, %dnnl_10_i88, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %82 = %81.0;
  nn.relu(%82) /* ty=Tensor[(1, 512, 7, 7), float32] */
}

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %83 = @tvmgen_default_dnnl_0(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, %conv0_weight, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %84 = nn.max_pool2d(%83, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %85 = @tvmgen_default_dnnl_10(%84, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, %stage1_unit1_conv1_weight, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, %stage1_unit1_conv2_weight, %stage1_unit1_sc_weight, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, %stage1_unit2_conv1_weight, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, %stage1_unit2_conv2_weight, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, %stage2_unit1_conv1_weight, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, %stage2_unit1_conv2_weight, %stage2_unit1_sc_weight, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, %stage2_unit2_conv1_weight, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, %stage2_unit2_conv2_weight, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, %stage3_unit1_conv1_weight, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, %stage3_unit1_conv2_weight, %stage3_unit1_sc_weight, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, %stage3_unit2_conv1_weight, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, %stage3_unit2_conv2_weight, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, %stage4_unit1_conv1_weight, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, %stage4_unit1_conv2_weight, %stage4_unit1_sc_weight, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, %stage4_unit2_conv1_weight, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, %stage4_unit2_conv2_weight, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %86 = nn.global_avg_pool2d(%85) /* ty=Tensor[(1, 512, 1, 1), float32] */;
  %87 = nn.batch_flatten(%86) /* ty=Tensor[(1, 512), float32] */;
  %88 = @tvmgen_default_dnnl_99(%87, %fc1_weight) /* ty=Tensor[(1, 1000), float32] */;
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;
  nn.softmax(%89) /* ty=Tensor[(1, 1000), float32] */
}

Running pass: {} The meta data of the pass: pass name: InferTypeopt_level: 0required passes: [
]

def @tvmgen_default_dnnl_99(%dnnl_99_i0: Tensor[(1, 512), float32], %dnnl_99_i1: Tensor[(1000, 512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_99", Primitive=1) -> Tensor[(1, 1000), float32] {
  nn.dense(%dnnl_99_i0, %dnnl_99_i1, units=1000) /* ty=Tensor[(1, 1000), float32] */
}

def @tvmgen_default_dnnl_0(%dnnl_0_i0: Tensor[(1, 3, 224, 224), float32], %dnnl_0_i1: Tensor[(3), float32], %dnnl_0_i2: Tensor[(3), float32], %dnnl_0_i3: Tensor[(3), float32], %dnnl_0_i4: Tensor[(3), float32], %dnnl_0_i5: Tensor[(64, 3, 7, 7), float32], %dnnl_0_i6: Tensor[(64), float32], %dnnl_0_i7: Tensor[(64), float32], %dnnl_0_i8: Tensor[(64), float32], %dnnl_0_i9: Tensor[(64), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_0", Primitive=1) -> Tensor[(1, 64, 112, 112), float32] {
  %0 = nn.batch_norm(%dnnl_0_i0, %dnnl_0_i1, %dnnl_0_i2, %dnnl_0_i3, %dnnl_0_i4, epsilon=2e-05f, scale=False) /* ty=(Tensor[(1, 3, 224, 224), float32], Tensor[(3), float32], Tensor[(3), float32]) */;
  %1 = %0.0;
  %2 = nn.conv2d(%1, %dnnl_0_i5, strides=[2, 2], padding=[3, 3, 3, 3], channels=64, kernel_size=[7, 7]) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %3 = nn.batch_norm(%2, %dnnl_0_i6, %dnnl_0_i7, %dnnl_0_i8, %dnnl_0_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 112, 112), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %4 = %3.0;
  nn.relu(%4) /* ty=Tensor[(1, 64, 112, 112), float32] */
}

def @tvmgen_default_dnnl_10(%dnnl_10_i0: Tensor[(1, 64, 56, 56), float32], %dnnl_10_i1: Tensor[(64), float32], %dnnl_10_i2: Tensor[(64), float32], %dnnl_10_i3: Tensor[(64), float32], %dnnl_10_i4: Tensor[(64), float32], %dnnl_10_i5: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i6: Tensor[(64), float32], %dnnl_10_i7: Tensor[(64), float32], %dnnl_10_i8: Tensor[(64), float32], %dnnl_10_i9: Tensor[(64), float32], %dnnl_10_i10: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i11: Tensor[(64, 64, 1, 1), float32], %dnnl_10_i12: Tensor[(64), float32], %dnnl_10_i13: Tensor[(64), float32], %dnnl_10_i14: Tensor[(64), float32], %dnnl_10_i15: Tensor[(64), float32], %dnnl_10_i16: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i17: Tensor[(64), float32], %dnnl_10_i18: Tensor[(64), float32], %dnnl_10_i19: Tensor[(64), float32], %dnnl_10_i20: Tensor[(64), float32], %dnnl_10_i21: Tensor[(64, 64, 3, 3), float32], %dnnl_10_i22: Tensor[(64), float32], %dnnl_10_i23: Tensor[(64), float32], %dnnl_10_i24: Tensor[(64), float32], %dnnl_10_i25: Tensor[(64), float32], %dnnl_10_i26: Tensor[(128, 64, 3, 3), float32], %dnnl_10_i27: Tensor[(128), float32], %dnnl_10_i28: Tensor[(128), float32], %dnnl_10_i29: Tensor[(128), float32], %dnnl_10_i30: Tensor[(128), float32], %dnnl_10_i31: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i32: Tensor[(128, 64, 1, 1), float32], %dnnl_10_i33: Tensor[(128), float32], %dnnl_10_i34: Tensor[(128), float32], %dnnl_10_i35: Tensor[(128), float32], %dnnl_10_i36: Tensor[(128), float32], %dnnl_10_i37: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i38: Tensor[(128), float32], %dnnl_10_i39: Tensor[(128), float32], %dnnl_10_i40: Tensor[(128), float32], %dnnl_10_i41: Tensor[(128), float32], %dnnl_10_i42: Tensor[(128, 128, 3, 3), float32], %dnnl_10_i43: Tensor[(128), float32], %dnnl_10_i44: Tensor[(128), float32], %dnnl_10_i45: Tensor[(128), float32], %dnnl_10_i46: Tensor[(128), float32], %dnnl_10_i47: Tensor[(256, 128, 3, 3), float32], %dnnl_10_i48: Tensor[(256), float32], %dnnl_10_i49: Tensor[(256), float32], %dnnl_10_i50: Tensor[(256), float32], %dnnl_10_i51: Tensor[(256), float32], %dnnl_10_i52: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i53: Tensor[(256, 128, 1, 1), float32], %dnnl_10_i54: Tensor[(256), float32], %dnnl_10_i55: Tensor[(256), float32], %dnnl_10_i56: Tensor[(256), float32], %dnnl_10_i57: Tensor[(256), float32], %dnnl_10_i58: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i59: Tensor[(256), float32], %dnnl_10_i60: Tensor[(256), float32], %dnnl_10_i61: Tensor[(256), float32], %dnnl_10_i62: Tensor[(256), float32], %dnnl_10_i63: Tensor[(256, 256, 3, 3), float32], %dnnl_10_i64: Tensor[(256), float32], %dnnl_10_i65: Tensor[(256), float32], %dnnl_10_i66: Tensor[(256), float32], %dnnl_10_i67: Tensor[(256), float32], %dnnl_10_i68: Tensor[(512, 256, 3, 3), float32], %dnnl_10_i69: Tensor[(512), float32], %dnnl_10_i70: Tensor[(512), float32], %dnnl_10_i71: Tensor[(512), float32], %dnnl_10_i72: Tensor[(512), float32], %dnnl_10_i73: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i74: Tensor[(512, 256, 1, 1), float32], %dnnl_10_i75: Tensor[(512), float32], %dnnl_10_i76: Tensor[(512), float32], %dnnl_10_i77: Tensor[(512), float32], %dnnl_10_i78: Tensor[(512), float32], %dnnl_10_i79: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i80: Tensor[(512), float32], %dnnl_10_i81: Tensor[(512), float32], %dnnl_10_i82: Tensor[(512), float32], %dnnl_10_i83: Tensor[(512), float32], %dnnl_10_i84: Tensor[(512, 512, 3, 3), float32], %dnnl_10_i85: Tensor[(512), float32], %dnnl_10_i86: Tensor[(512), float32], %dnnl_10_i87: Tensor[(512), float32], %dnnl_10_i88: Tensor[(512), float32], Inline=1, Compiler="dnnl", global_symbol="tvmgen_default_dnnl_10", Primitive=1) -> Tensor[(1, 512, 7, 7), float32] {
  %5 = nn.batch_norm(%dnnl_10_i0, %dnnl_10_i1, %dnnl_10_i2, %dnnl_10_i3, %dnnl_10_i4, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %6 = %5.0;
  %7 = nn.relu(%6) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %8 = nn.conv2d(%7, %dnnl_10_i5, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %9 = nn.batch_norm(%8, %dnnl_10_i6, %dnnl_10_i7, %dnnl_10_i8, %dnnl_10_i9, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %10 = %9.0;
  %11 = nn.relu(%10) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %12 = nn.conv2d(%11, %dnnl_10_i10, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %13 = nn.conv2d(%7, %dnnl_10_i11, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %14 = add(%12, %13) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %15 = nn.batch_norm(%14, %dnnl_10_i12, %dnnl_10_i13, %dnnl_10_i14, %dnnl_10_i15, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %16 = %15.0;
  %17 = nn.relu(%16) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %18 = nn.conv2d(%17, %dnnl_10_i16, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %19 = nn.batch_norm(%18, %dnnl_10_i17, %dnnl_10_i18, %dnnl_10_i19, %dnnl_10_i20, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %20 = %19.0;
  %21 = nn.relu(%20) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %22 = nn.conv2d(%21, %dnnl_10_i21, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %23 = add(%22, %14) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %24 = nn.batch_norm(%23, %dnnl_10_i22, %dnnl_10_i23, %dnnl_10_i24, %dnnl_10_i25, epsilon=2e-05f) /* ty=(Tensor[(1, 64, 56, 56), float32], Tensor[(64), float32], Tensor[(64), float32]) */;
  %25 = %24.0;
  %26 = nn.relu(%25) /* ty=Tensor[(1, 64, 56, 56), float32] */;
  %27 = nn.conv2d(%26, %dnnl_10_i26, strides=[2, 2], padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %28 = nn.batch_norm(%27, %dnnl_10_i27, %dnnl_10_i28, %dnnl_10_i29, %dnnl_10_i30, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %29 = %28.0;
  %30 = nn.relu(%29) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %31 = nn.conv2d(%30, %dnnl_10_i31, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %32 = nn.conv2d(%26, %dnnl_10_i32, strides=[2, 2], padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %33 = add(%31, %32) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %34 = nn.batch_norm(%33, %dnnl_10_i33, %dnnl_10_i34, %dnnl_10_i35, %dnnl_10_i36, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %35 = %34.0;
  %36 = nn.relu(%35) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %37 = nn.conv2d(%36, %dnnl_10_i37, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %38 = nn.batch_norm(%37, %dnnl_10_i38, %dnnl_10_i39, %dnnl_10_i40, %dnnl_10_i41, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %39 = %38.0;
  %40 = nn.relu(%39) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %41 = nn.conv2d(%40, %dnnl_10_i42, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %42 = add(%41, %33) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %43 = nn.batch_norm(%42, %dnnl_10_i43, %dnnl_10_i44, %dnnl_10_i45, %dnnl_10_i46, epsilon=2e-05f) /* ty=(Tensor[(1, 128, 28, 28), float32], Tensor[(128), float32], Tensor[(128), float32]) */;
  %44 = %43.0;
  %45 = nn.relu(%44) /* ty=Tensor[(1, 128, 28, 28), float32] */;
  %46 = nn.conv2d(%45, %dnnl_10_i47, strides=[2, 2], padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %47 = nn.batch_norm(%46, %dnnl_10_i48, %dnnl_10_i49, %dnnl_10_i50, %dnnl_10_i51, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %48 = %47.0;
  %49 = nn.relu(%48) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %50 = nn.conv2d(%49, %dnnl_10_i52, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %51 = nn.conv2d(%45, %dnnl_10_i53, strides=[2, 2], padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %52 = add(%50, %51) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %53 = nn.batch_norm(%52, %dnnl_10_i54, %dnnl_10_i55, %dnnl_10_i56, %dnnl_10_i57, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %54 = %53.0;
  %55 = nn.relu(%54) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %56 = nn.conv2d(%55, %dnnl_10_i58, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %57 = nn.batch_norm(%56, %dnnl_10_i59, %dnnl_10_i60, %dnnl_10_i61, %dnnl_10_i62, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %58 = %57.0;
  %59 = nn.relu(%58) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %60 = nn.conv2d(%59, %dnnl_10_i63, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %61 = add(%60, %52) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %62 = nn.batch_norm(%61, %dnnl_10_i64, %dnnl_10_i65, %dnnl_10_i66, %dnnl_10_i67, epsilon=2e-05f) /* ty=(Tensor[(1, 256, 14, 14), float32], Tensor[(256), float32], Tensor[(256), float32]) */;
  %63 = %62.0;
  %64 = nn.relu(%63) /* ty=Tensor[(1, 256, 14, 14), float32] */;
  %65 = nn.conv2d(%64, %dnnl_10_i68, strides=[2, 2], padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %66 = nn.batch_norm(%65, %dnnl_10_i69, %dnnl_10_i70, %dnnl_10_i71, %dnnl_10_i72, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %67 = %66.0;
  %68 = nn.relu(%67) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %69 = nn.conv2d(%68, %dnnl_10_i73, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %70 = nn.conv2d(%64, %dnnl_10_i74, strides=[2, 2], padding=[0, 0, 0, 0], channels=512, kernel_size=[1, 1]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %71 = add(%69, %70) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %72 = nn.batch_norm(%71, %dnnl_10_i75, %dnnl_10_i76, %dnnl_10_i77, %dnnl_10_i78, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %73 = %72.0;
  %74 = nn.relu(%73) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %75 = nn.conv2d(%74, %dnnl_10_i79, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %76 = nn.batch_norm(%75, %dnnl_10_i80, %dnnl_10_i81, %dnnl_10_i82, %dnnl_10_i83, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %77 = %76.0;
  %78 = nn.relu(%77) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %79 = nn.conv2d(%78, %dnnl_10_i84, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %80 = add(%79, %71) /* ty=Tensor[(1, 512, 7, 7), float32] */;
  %81 = nn.batch_norm(%80, %dnnl_10_i85, %dnnl_10_i86, %dnnl_10_i87, %dnnl_10_i88, epsilon=2e-05f) /* ty=(Tensor[(1, 512, 7, 7), float32], Tensor[(512), float32], Tensor[(512), float32]) */;
  %82 = %81.0;
  nn.relu(%82) /* ty=Tensor[(1, 512, 7, 7), float32] */
}

def @main(%data: Tensor[(1, 3, 224, 224), float32], %bn_data_gamma: Tensor[(3), float32], %bn_data_beta: Tensor[(3), float32], %bn_data_moving_mean: Tensor[(3), float32], %bn_data_moving_var: Tensor[(3), float32], %conv0_weight: Tensor[(64, 3, 7, 7), float32], %bn0_gamma: Tensor[(64), float32], %bn0_beta: Tensor[(64), float32], %bn0_moving_mean: Tensor[(64), float32], %bn0_moving_var: Tensor[(64), float32], %stage1_unit1_bn1_gamma: Tensor[(64), float32], %stage1_unit1_bn1_beta: Tensor[(64), float32], %stage1_unit1_bn1_moving_mean: Tensor[(64), float32], %stage1_unit1_bn1_moving_var: Tensor[(64), float32], %stage1_unit1_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_bn2_gamma: Tensor[(64), float32], %stage1_unit1_bn2_beta: Tensor[(64), float32], %stage1_unit1_bn2_moving_mean: Tensor[(64), float32], %stage1_unit1_bn2_moving_var: Tensor[(64), float32], %stage1_unit1_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit1_sc_weight: Tensor[(64, 64, 1, 1), float32], %stage1_unit2_bn1_gamma: Tensor[(64), float32], %stage1_unit2_bn1_beta: Tensor[(64), float32], %stage1_unit2_bn1_moving_mean: Tensor[(64), float32], %stage1_unit2_bn1_moving_var: Tensor[(64), float32], %stage1_unit2_conv1_weight: Tensor[(64, 64, 3, 3), float32], %stage1_unit2_bn2_gamma: Tensor[(64), float32], %stage1_unit2_bn2_beta: Tensor[(64), float32], %stage1_unit2_bn2_moving_mean: Tensor[(64), float32], %stage1_unit2_bn2_moving_var: Tensor[(64), float32], %stage1_unit2_conv2_weight: Tensor[(64, 64, 3, 3), float32], %stage2_unit1_bn1_gamma: Tensor[(64), float32], %stage2_unit1_bn1_beta: Tensor[(64), float32], %stage2_unit1_bn1_moving_mean: Tensor[(64), float32], %stage2_unit1_bn1_moving_var: Tensor[(64), float32], %stage2_unit1_conv1_weight: Tensor[(128, 64, 3, 3), float32], %stage2_unit1_bn2_gamma: Tensor[(128), float32], %stage2_unit1_bn2_beta: Tensor[(128), float32], %stage2_unit1_bn2_moving_mean: Tensor[(128), float32], %stage2_unit1_bn2_moving_var: Tensor[(128), float32], %stage2_unit1_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit1_sc_weight: Tensor[(128, 64, 1, 1), float32], %stage2_unit2_bn1_gamma: Tensor[(128), float32], %stage2_unit2_bn1_beta: Tensor[(128), float32], %stage2_unit2_bn1_moving_mean: Tensor[(128), float32], %stage2_unit2_bn1_moving_var: Tensor[(128), float32], %stage2_unit2_conv1_weight: Tensor[(128, 128, 3, 3), float32], %stage2_unit2_bn2_gamma: Tensor[(128), float32], %stage2_unit2_bn2_beta: Tensor[(128), float32], %stage2_unit2_bn2_moving_mean: Tensor[(128), float32], %stage2_unit2_bn2_moving_var: Tensor[(128), float32], %stage2_unit2_conv2_weight: Tensor[(128, 128, 3, 3), float32], %stage3_unit1_bn1_gamma: Tensor[(128), float32], %stage3_unit1_bn1_beta: Tensor[(128), float32], %stage3_unit1_bn1_moving_mean: Tensor[(128), float32], %stage3_unit1_bn1_moving_var: Tensor[(128), float32], %stage3_unit1_conv1_weight: Tensor[(256, 128, 3, 3), float32], %stage3_unit1_bn2_gamma: Tensor[(256), float32], %stage3_unit1_bn2_beta: Tensor[(256), float32], %stage3_unit1_bn2_moving_mean: Tensor[(256), float32], %stage3_unit1_bn2_moving_var: Tensor[(256), float32], %stage3_unit1_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit1_sc_weight: Tensor[(256, 128, 1, 1), float32], %stage3_unit2_bn1_gamma: Tensor[(256), float32], %stage3_unit2_bn1_beta: Tensor[(256), float32], %stage3_unit2_bn1_moving_mean: Tensor[(256), float32], %stage3_unit2_bn1_moving_var: Tensor[(256), float32], %stage3_unit2_conv1_weight: Tensor[(256, 256, 3, 3), float32], %stage3_unit2_bn2_gamma: Tensor[(256), float32], %stage3_unit2_bn2_beta: Tensor[(256), float32], %stage3_unit2_bn2_moving_mean: Tensor[(256), float32], %stage3_unit2_bn2_moving_var: Tensor[(256), float32], %stage3_unit2_conv2_weight: Tensor[(256, 256, 3, 3), float32], %stage4_unit1_bn1_gamma: Tensor[(256), float32], %stage4_unit1_bn1_beta: Tensor[(256), float32], %stage4_unit1_bn1_moving_mean: Tensor[(256), float32], %stage4_unit1_bn1_moving_var: Tensor[(256), float32], %stage4_unit1_conv1_weight: Tensor[(512, 256, 3, 3), float32], %stage4_unit1_bn2_gamma: Tensor[(512), float32], %stage4_unit1_bn2_beta: Tensor[(512), float32], %stage4_unit1_bn2_moving_mean: Tensor[(512), float32], %stage4_unit1_bn2_moving_var: Tensor[(512), float32], %stage4_unit1_conv2_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit1_sc_weight: Tensor[(512, 256, 1, 1), float32], %stage4_unit2_bn1_gamma: Tensor[(512), float32], %stage4_unit2_bn1_beta: Tensor[(512), float32], %stage4_unit2_bn1_moving_mean: Tensor[(512), float32], %stage4_unit2_bn1_moving_var: Tensor[(512), float32], %stage4_unit2_conv1_weight: Tensor[(512, 512, 3, 3), float32], %stage4_unit2_bn2_gamma: Tensor[(512), float32], %stage4_unit2_bn2_beta: Tensor[(512), float32], %stage4_unit2_bn2_moving_mean: Tensor[(512), float32], %stage4_unit2_bn2_moving_var: Tensor[(512), float32], %stage4_unit2_conv2_weight: Tensor[(512, 512, 3, 3), float32], %bn1_gamma: Tensor[(512), float32], %bn1_beta: Tensor[(512), float32], %bn1_moving_mean: Tensor[(512), float32], %bn1_moving_var: Tensor[(512), float32], %fc1_weight: Tensor[(1000, 512), float32], %fc1_bias: Tensor[(1000), float32]) -> Tensor[(1, 1000), float32] {
  %83 = @tvmgen_default_dnnl_0(%data, %bn_data_gamma, %bn_data_beta, %bn_data_moving_mean, %bn_data_moving_var, %conv0_weight, %bn0_gamma, %bn0_beta, %bn0_moving_mean, %bn0_moving_var) /* ty=Tensor[(1, 64, 112, 112), float32] */;
  %84 = nn.max_pool2d(%83, pool_size=[3, 3], strides=[2, 2], padding=[1, 1, 1, 1]);
  %85 = @tvmgen_default_dnnl_10(%84, %stage1_unit1_bn1_gamma, %stage1_unit1_bn1_beta, %stage1_unit1_bn1_moving_mean, %stage1_unit1_bn1_moving_var, %stage1_unit1_conv1_weight, %stage1_unit1_bn2_gamma, %stage1_unit1_bn2_beta, %stage1_unit1_bn2_moving_mean, %stage1_unit1_bn2_moving_var, %stage1_unit1_conv2_weight, %stage1_unit1_sc_weight, %stage1_unit2_bn1_gamma, %stage1_unit2_bn1_beta, %stage1_unit2_bn1_moving_mean, %stage1_unit2_bn1_moving_var, %stage1_unit2_conv1_weight, %stage1_unit2_bn2_gamma, %stage1_unit2_bn2_beta, %stage1_unit2_bn2_moving_mean, %stage1_unit2_bn2_moving_var, %stage1_unit2_conv2_weight, %stage2_unit1_bn1_gamma, %stage2_unit1_bn1_beta, %stage2_unit1_bn1_moving_mean, %stage2_unit1_bn1_moving_var, %stage2_unit1_conv1_weight, %stage2_unit1_bn2_gamma, %stage2_unit1_bn2_beta, %stage2_unit1_bn2_moving_mean, %stage2_unit1_bn2_moving_var, %stage2_unit1_conv2_weight, %stage2_unit1_sc_weight, %stage2_unit2_bn1_gamma, %stage2_unit2_bn1_beta, %stage2_unit2_bn1_moving_mean, %stage2_unit2_bn1_moving_var, %stage2_unit2_conv1_weight, %stage2_unit2_bn2_gamma, %stage2_unit2_bn2_beta, %stage2_unit2_bn2_moving_mean, %stage2_unit2_bn2_moving_var, %stage2_unit2_conv2_weight, %stage3_unit1_bn1_gamma, %stage3_unit1_bn1_beta, %stage3_unit1_bn1_moving_mean, %stage3_unit1_bn1_moving_var, %stage3_unit1_conv1_weight, %stage3_unit1_bn2_gamma, %stage3_unit1_bn2_beta, %stage3_unit1_bn2_moving_mean, %stage3_unit1_bn2_moving_var, %stage3_unit1_conv2_weight, %stage3_unit1_sc_weight, %stage3_unit2_bn1_gamma, %stage3_unit2_bn1_beta, %stage3_unit2_bn1_moving_mean, %stage3_unit2_bn1_moving_var, %stage3_unit2_conv1_weight, %stage3_unit2_bn2_gamma, %stage3_unit2_bn2_beta, %stage3_unit2_bn2_moving_mean, %stage3_unit2_bn2_moving_var, %stage3_unit2_conv2_weight, %stage4_unit1_bn1_gamma, %stage4_unit1_bn1_beta, %stage4_unit1_bn1_moving_mean, %stage4_unit1_bn1_moving_var, %stage4_unit1_conv1_weight, %stage4_unit1_bn2_gamma, %stage4_unit1_bn2_beta, %stage4_unit1_bn2_moving_mean, %stage4_unit1_bn2_moving_var, %stage4_unit1_conv2_weight, %stage4_unit1_sc_weight, %stage4_unit2_bn1_gamma, %stage4_unit2_bn1_beta, %stage4_unit2_bn1_moving_mean, %stage4_unit2_bn1_moving_var, %stage4_unit2_conv1_weight, %stage4_unit2_bn2_gamma, %stage4_unit2_bn2_beta, %stage4_unit2_bn2_moving_mean, %stage4_unit2_bn2_moving_var, %stage4_unit2_conv2_weight, %bn1_gamma, %bn1_beta, %bn1_moving_mean, %bn1_moving_var);
  %86 = nn.global_avg_pool2d(%85);
  %87 = nn.batch_flatten(%86);
  %88 = @tvmgen_default_dnnl_99(%87, %fc1_weight);
  %89 = nn.bias_add(%88, %fc1_bias, axis=-1);
  nn.softmax(%89)
}
dnnl_verbose,info,oneDNN v2.3.0 (commit 3b28c5246290d97107bf7f8e8f40618825717ef6)
dnnl_verbose,info,cpu,runtime:OpenMP
dnnl_verbose,info,cpu,isa:Intel AVX-512 with AVX512BW, AVX512VL, and AVX512DQ extensions
dnnl_verbose,info,gpu,runtime:none
dnnl_verbose,info,prim_template:operation,engine,primitive,implementation,prop_kind,memory_descriptors,attributes,auxiliary,problem_desc,exec_time
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.692871
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,154.654
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,5.17017
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.364014
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.425049
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,16.0659
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.186035
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0651855
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,11.312
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.450928
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.101807
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.269043
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0629883
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,1.00195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.188965
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0598145
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.81201
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.098877
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.260986
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.102051
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.581787
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.097168
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0439453
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.30786
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.468994
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0549316
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0991211
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0500488
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.65405
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0991211
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.045166
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.77417
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0571289
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.184814
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0480957
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.562012
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0998535
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0270996
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,6.07886
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.125977
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0358887
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.072998
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0251465
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.54517
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.12793
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0249023
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.39795
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0410156
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0668945
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0239258
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.488037
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0358887
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.0229492
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.681152
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0820312
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0268555
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0361328
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.0170898
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.660889
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0339355
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.0170898
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.636963
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.027832
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0332031
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.0180664
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0620117
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.114014
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.86499
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.125
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.100098
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.246094
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0300293
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.521
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.15918
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0319824
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,8.98413
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.172852
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0661621
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.240967
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0290527
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.998047
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.162842
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0310059
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.80103
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0620117
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.23999
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0268555
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.553955
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0900879
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0151367
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.302
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.103027
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0380859
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0788574
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0141602
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.66406
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.134033
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0151367
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.88989
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0361328
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.163086
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0141602
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.527832
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0898438
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,6.61597
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.11792
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0249023
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0681152
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.55493
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.118164
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.41309
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0251465
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0600586
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.48291
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0300293
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.0078125
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.670166
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0791016
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0231934
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0280762
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.0078125
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.638184
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0290527
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.648926
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0229492
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0290527
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0629883
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.123047
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.86499
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.521
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.112061
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.250977
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0349121
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.66016
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.175049
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0371094
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,8.927
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.171875
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0710449
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.240967
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0358887
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.979004
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.161865
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0349121
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.72095
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0480957
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.255859
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0310059
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.550049
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0881348
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0151367
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.29297
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.101074
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0300293
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.078125
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.013916
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.65991
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.128906
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0310059
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.77515
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0300293
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.186035
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.528809
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0888672
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,6.00098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.108887
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0629883
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.552
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.11792
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.38403
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0229492
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0600586
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.48291
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0297852
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00878906
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.677979
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0778809
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0270996
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.646973
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0290527
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.645996
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0212402
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0270996
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00585938
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0581055
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.11499
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.86499
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.12891
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.109863
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.244873
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0341797
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.479
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.176025
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0339355
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,8.86084
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.168945
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0661621
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.239014
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0358887
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.981934
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.161865
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0349121
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.72803
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0429688
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.246094
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0300293
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.551025
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0808105
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0161133
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.30688
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.100098
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0449219
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0800781
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.013916
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.66187
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.13501
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.013916
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,7.27222
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0290527
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.163086
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0239258
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.72583
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.163818
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,6.198
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.11499
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.119141
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.74902
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.245117
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.57983
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0219727
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0891113
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.5
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0290527
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.680908
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0849609
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0161133
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0251465
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00610352
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.661865
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0280762
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.657959
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.027832
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0688477
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.117188
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,4.74707
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.13013
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.11499
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.25
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0390625
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,16.293
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.151855
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0371094
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,11.5508
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.177979
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.072998
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.244141
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0368652
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.706055
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.146973
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0268555
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.23608
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0510254
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.23999
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0290527
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.457031
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0800781
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0131836
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.80688
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.0859375
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0297852
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0749512
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.28491
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.118896
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.21387
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.027832
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.161865
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.515869
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0849609
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0109863
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,5.92603
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.0881348
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0458984
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.55005
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.104004
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.37695
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0219727
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0507812
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.00878906
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.476074
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0290527
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.674072
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0761719
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.026123
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.0078125
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.669189
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0258789
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.6521
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.026123
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0610352
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.11206
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.12988
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.0969238
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.25
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0351562
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.38086
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.147949
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0349121
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.69214
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.137939
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0751953
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.240967
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0368652
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.763916
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.151123
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0307617
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.67896
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0490723
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.23999
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0310059
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.454102
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0810547
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0148926
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.80005
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.0871582
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0310059
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0761719
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.32104
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.118896
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.51587
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0332031
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.162109
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.568115
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0861816
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,7.02393
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.115967
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0219727
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0458984
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.76587
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.106934
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.5022
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0219727
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0649414
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.00878906
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.495117
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0290527
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.704102
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.078125
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.026123
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.679199
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0270996
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.674072
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0219727
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0268555
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0639648
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.108887
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.96118
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.12793
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.11499
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.256104
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0390625
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.39502
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.149902
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0368652
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.73999
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.140137
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0800781
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.23999
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0361328
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.737061
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.146973
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0258789
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.64404
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.052002
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.239014
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0280762
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.432129
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.079834
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0141602
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.83398
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.0949707
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0310059
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0761719
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.22705
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.126953
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.24194
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0290527
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.161133
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.52002
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0861816
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,5.92603
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.0888672
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0490723
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.51514
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.110107
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.37402
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0222168
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0510254
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.467041
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0268555
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.65918
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0820312
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0187988
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0268555
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.641846
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0258789
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.677002
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0270996
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0629883
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.101074
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.78711
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.12207
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.104004
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.248047
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0378418
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.40112
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.147949
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0419922
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.7019
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.139893
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0788574
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.239014
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0349121
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.742188
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.147949
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0268555
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.64697
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0439453
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.240967
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.027832
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.439941
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0800781
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.013916
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.78809
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.0881348
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0280762
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0761719
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.21582
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.119873
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.21802
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0290527
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.162109
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.518066
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0849609
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0109863
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,5.96802
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.0900879
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0300293
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0500488
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.51196
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.103027
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.37012
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0219727
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.052002
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.462158
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.026123
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.674072
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0817871
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0249023
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0258789
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.642822
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0268555
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.640137
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0270996
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0620117
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.101807
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.09106
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.12891
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.0979004
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.249023
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0400391
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.41821
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.14917
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0339355
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.7229
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.137939
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0759277
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.240234
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0410156
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.76416
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.14502
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0339355
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.68799
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.045166
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.240967
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0290527
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.455078
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0810547
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0141602
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.80786
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.0888672
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0288086
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0769043
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.22803
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.119141
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.23218
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0290527
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.161133
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.513916
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0859375
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0109863
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,5.93701
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.0878906
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0471191
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.573
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.103027
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.38403
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0219727
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0529785
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.469971
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0290527
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.682861
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0800781
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.026123
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.662109
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0268555
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.640137
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0270996
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0690918
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.109131
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.09692
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.12305
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.103027
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.253174
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0410156
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.88892
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.1521
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0349121
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,8.54907
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.156006
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0620117
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.23999
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0268555
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.937012
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.146973
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0219727
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.99512
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0510254
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.240234
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.026123
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.459961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0842285
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.013916
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.78613
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.0891113
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.032959
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0759277
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.25903
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.120117
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.012207
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.22192
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0280762
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.162109
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.50708
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0849609
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,5.96094
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.0942383
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.046875
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.49585
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.10498
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.51807
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0219727
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0529785
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.471924
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0349121
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.655029
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0749512
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.026123
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.644043
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0300293
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.648193
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0270996
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.0078125
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0620117
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.101807
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.11499
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.13086
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.098877
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.24585
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0349121
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.396
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.150146
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0368652
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.72021
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.13916
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0761719
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.240967
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0358887
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.71582
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.147949
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.026123
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.68408
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0419922
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.238037
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0280762
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.452881
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0810547
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.013916
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.80103
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.0859375
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.027832
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0759277
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.21997
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.120117
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.012207
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.23096
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0280762
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.161133
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.52002
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0852051
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,5.9231
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.0881348
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0498047
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.52588
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.103027
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.39697
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0529785
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.474854
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0288086
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.708984
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.0749512
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.019043
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.026123
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.670898
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0280762
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.670898
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0270996
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.064209
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic3ih224iw224,0.103027
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic3oc64_ih224oh112kh7sh2dh0ph3_iw224ow112kw7sw2dw0pw3,3.09912
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih112iw112,2.12109
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x112x112,0.103027
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.249023
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0339355
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.40601
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.147949
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0349121
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,7.72998
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0,0.137939
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0771484
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.239014
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0349121
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,0.716064
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.147949
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0268555
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc64_ih56oh56kh3sh1dh0ph1_iw56ow56kw3sw1dw0pw1,6.66089
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x64x56x56:1x64x56x56,0.0441895
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic64ih56iw56,0.239014
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x64x56x56,0.0290527
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh3sh2dh0ph1_iw56ow28kw3sw2dw0pw1,0.449951
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0810547
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.013916
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.79712
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic64oc128_ih56oh28kh1sh2dh0ph0_iw56ow28kw1sw2dw0pw0,0.0859375
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0280762
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.0769043
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0129395
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,2.21606
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.127197
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.013916
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc128_ih28oh28kh3sh1dh0ph1_iw28ow28kw3sw1dw0pw1,3.22412
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x128x28x28:1x128x28x28,0.0280762
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic128ih28iw28,0.162109
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x128x28x28,0.0119629
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh3sh2dh0ph1_iw28ow14kw3sw2dw0pw1,0.52417
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0849609
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0100098
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,5.92212
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic128oc256_ih28oh14kh1sh2dh0ph0_iw28ow14kw1sw2dw0pw0,0.0891113
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.0458984
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.50586
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.105957
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc256_ih14oh14kh3sh1dh0ph1_iw14ow14kw3sw1dw0pw1,1.36816
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x256x14x14:1x256x14x14,0.0200195
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic256ih14iw14,0.052002
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x256x14x14,0.0090332
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh3sh2dh0ph1_iw14ow7kw3sw2dw0pw1,0.477051
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0280762
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00805664
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.677002
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic256oc512_ih14oh7kh1sh2dh0ph0_iw14ow7kw1sw2dw0pw0,0.072998
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.019043
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0270996
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00683594
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.614014
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.0258789
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,convolution,gemm:jit,forward_inference,src_f32::blocked:abcd:f0 wei_f32::blocked:abcd:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,,alg:convolution_direct,mb1_ic512oc512_ih7oh7kh3sh1dh0ph1_iw7ow7kw3sw1dw0pw1,0.654053
dnnl_verbose,exec,cpu,binary,jit:uni,undef,src_f32::blocked:abcd:f0 src_f32::blocked:abcd:f0 dst_f32::blocked:abcd:f0,,alg:binary_add,1x512x7x7:1x512x7x7,0.0209961
dnnl_verbose,exec,cpu,batch_normalization,ncsp_bnorm:any,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,flags:GS,mb1ic512ih7iw7,0.026123
dnnl_verbose,exec,cpu,eltwise,jit:avx512_common,forward_inference,data_f32::blocked:abcd:f0 diff_undef::undef::f0,,alg:eltwise_relu alpha:0 beta:0,1x512x7x7,0.00708008
dnnl_verbose,exec,cpu,inner_product,gemm:jit,forward_inference,src_f32::blocked:ab:f0 wei_f32::blocked:ab:f0 bia_f32::blocked:a:f0 dst_f32::blocked:ab:f0,,,mb1ic512oc1000,0.0629883

resnet50_v1: with_fuse_ms: 61.0203 ms
